\chapter{Introduction}

The ability to read DNA, to understand it or even to modify it, is certainly one of the ways that many people thing will define the future. But in order to work with DNA there needs to be a mathematical model that can actually compute with such structures and that is prepared to be run on computers. And working with this model must be efficient enough becasue genetic code has huge number of digits.

This works follows the work of Zulkufli et al. \cite{WK_GRAMMARS_1}, \cite{WK_GRAMMARS_2}, \cite{WK_CYK} who have studied models for working with Watson-Crick languages and introduced the WK-CYK algorithm, a modification of the CYK algorithm, which works with Watson-Crick context-free grammars and is able to decide membership problem for these languages. The stated complexity of this algorithm is $\mathcal{O}(n^6)$ with respect to the input length. However, with this complexity the algorithm still does not seem to be useful for practical DNA computatons considering how long DNA code is.

Therefore this thesis introduces State space search algorithm. While its theoretical complexity is not as good as in case of WK-CYK, it takes more more practical approach. In practice, thanks to various heuristics, it is very often able to decide the membership problem with Watson-Crick context-free grammars of inputs far longer then what WK-CYK can handle.

Chapter \ref{chapter:models} contains an overview of most common models for working with Watson-Crick languages. Chapter \ref{chapter:WK_CYK} discusses ways of deciding membership problem of those languages with the focus on the WK-CYK algorithm. Chapter \ref{chapter:parse_tree} introduces the State space search algorithm and the heuristics and optimizations that make it more efficient. Chapter \ref{chapter:implementation} focuses on the implementation of the State space search and is probably going to be useful to someone who wants to delve into the code and use it or build on it. Finally, chapter \ref{chapter:testing} containes twenty grammars that were used for testing both State space search and WK-CYK algorithms in practice. And present results of these tests.

A integral component of this thesis is an implementation of the State space search algorithm, the WK-CYK algorithm and a number tests used to analyse the state and space complexities and compare the algorithms.

\chapter{Watson-Crick models} \label{chapter:models}
Number of models working with double stranded sequences has been proposed. The purpuse of this chapter is to present motivation for using them and to summarize these models and some of their key attributes that will be used in a later chapters.

\section{DNA as an inspiration for Watson-Crick languages}
The study of Watson-Crick models is motivated by DNA (deoxyribonucleic acid) computing. In order to study the DNA mathematically, i.e. to perform mathematical operations, it is necessary to work with a suitable abstraction --- a model which captures its key characteristics. Specifically, there are two characterictics that the Watson-Crick models capture --- the fact that DNA is a double stranded chain and the Watson-Crick relation between DNA nucleotides.
The two fundamental models that are used to define a language in computer theory are grammars and automata. Several versions of both have been proposed but all of them work with these two characterictics in a very similar manner.

DNA consists of two chains of nucleotides that are connected by covalent bonds and together form a double helix (figure \ref{fig:dna}). These two chains are represented in the Watson-Crick automata by two reading heads which read two inputs independetly but controlled by the same states. Similarly, Watson-Crick grammars produce by their rules not just a chain of symbols, but two chains.

\begin{figure}[ht]
  \includegraphics[height=8cm]{DNA.png}
  \centering
  \label{fig:dna}
  \caption{The DNA double helix}
\end{figure}

Each nucleotide contains one of the four nucleobases - cytosine (C), guanine (G), adenine (A), thymine (T). These bases are always connected with their counterpart: cytosine with guanine and adenine with thymine. That means that whenever one of the four appears in a chain, its counterpart appears in the other chain in the corresponding place being bound together by the covalent bond. The Watson-Crick models therefore introduce a complementary relation -- a relation between symbols which must be kept in the whole input for it to be valid. Typically, this relation is symmetric ($a R b \Leftrightarrow b R a$) and covers the whole alphabet (every symbol must have at least one counterpart). Often every symbol has exactly one counterpart -- just like in case of DNA (the relation is frequently defined as an identity which is still somewhat similar to the DNA pairing).


\section{Watson-Crick automata}
Watson-Crick automata have been first proposed in \cite{WK_FIN_AUT} as an enhancement of standard Finite Automata. Watson-Crick finite automaton is a 6-tuple $M = (V, \rho, Q, q_0, F, P)$ with the following meaning.
\begin{itemize}
  \item{$V$ -- finite input alphabet}
  \item{$\rho \subseteq V \times V$ -- complementarity relation}
  \item{$Q$ -- finite set of states}
  \item{$q_0 \in Q$ -- starting symbol}
  \item{$F \subseteq Q$ -- set of finite states}
  \item{$P$ -- finite set of transition rules in a form $q({w_1 \atop w_2}) \rightarrow q'$ where $q, q' \in Q, w_1, w_2 \in V^*$}
\end{itemize}

Compared to Finite automata, Watson-Crick automata have different form of transition rules which read two strings at the same time. These represent the two independent reading heads -- one reading the upper strand ($w_1$) and the other reading the lower strand ($w_2$). They also add the complementarity relation which is usually required to be symmetric. The symbols in the upper and lower strands with the same indices need to adhere to it.

A Watson-Crick domain is a set $WK_{\rho}(V)$ which denotes all valid double strands associated with a given $V$ and $\rho$. Formally:
\begin{align}
	WK_{\rho}(V) = \wkdomain{V}{V}_{\rho}^{*} && \textnormal{where} && \wkdomain{V}{V}_{\rho} = \Big\{\wkdomain{a}{b} | a, b \in V, (a, b) \in \rho \Big\}
\end{align}
This implies that both strands have the same length.

A configuraion of a Watson-Crick automaton is a pair $(q, ({w_1 \atop w_2}))$ where $q \in Q$ is a current state and $w_1, w_2 \in V^*$ are the parts of the upper and lower strands yet to be read.

If $q\big({u_1 \atop u_2}\big) \rightarrow q' \in P$ and $\big({u_1 v_1 \atop u_2 v_2}\big) \in \big({V^* \atop V^*}\big)$ then $q\big({u_1 v_1 \atop u_2 v_2}\big) \Rightarrow q'\big({v_1 \atop v_2}\big)$ is a transition of the Watson-Crick automaton. $\Rightarrow^*$ denotes the transitive and reflexive closure of the relation $\Rightarrow$.

A Watson-Crick automaton accepts the language $L(M)$:

$$L(M) = \Big\{w_1 \in V^* | q_0 \wkdomain{w_1}{w_2} \Rightarrow^* f \genfrac{(}{)}{0pt}{1}{\lambda}{\lambda} \textnormal{ where } f \in F, w_2 \in V^*, \wkdomain{w_1}{w_2} \in WK_{\rho}(V)\Big\}$$

This means that only the upper strand is accepted by this automaton to the language $L$. The lower strand has just an auxiliary purpose.

\section{Special versions of Watson-Crick automata}
Four special versions of Watson-Crick automata are often used. These are:
\begin{itemize}
  \item{Stateless WKA -- The WKA has only one state: $Q = F = {q_0}$}
  \item{All final WKA -- All the states are final: $Q = F$}
  \item{Simple WKA -- each rule reads only one head: $(q({w_1 \atop w_2}) \rightarrow q' \in P) \Rightarrow (w_1 = \lambda \vee w_2 = \lambda)$}
  \item{1-limited WKA -- similar to Simple WKA but also reads only one symbol at a time: $(q({w_1 \atop w_2}) \rightarrow q' \in P) \Rightarrow |w_1 w_2| = 1$}
\end{itemize}

It has been shown that three of these four special types of WKAs have the same power as the actual WKA, namely all final WKA, simple WKA and 1-limited WKA (stateless WKA is weaker). Therefore one possible approach to decide membership would be to limit the decision algorithm to one of these three types without any loss in expressing power.

There are three different variants of deterministic WKA proposed in \cite{DETERM_WKA}. These are:
\begin{itemize}
  \item{Weakly deterministic WKA -- WKA where in each reachable configuration, there is at most one possible continuation.}
  \item{Deterministic WKA -- for any two rules which lead from the same state, either their upper strands or their lower strands must not be prefix comparable, meaning one is not the prefix of the other. Formally: $(q({u \atop v}) \rightarrow q_1 \in P \wedge q({u' \atop v'}) \rightarrow q_2 \in P) \Rightarrow u \nsim_p u' \vee v \nsim_p v'$ where $\sim_p$ is the relation of prefix comparability}
  \item{Stronly deterministic WKA}
\end{itemize}

It is not specified how to actually achieve weak determinism. In fact, \cite{DETERM_WKA} shows that this property is undecidable. Informally, for a WKA to be weakly deterministic but not deterministic, there must be at least two rules which could both be used in certain configuration (otherwise it would be deterministic). But such a configuration must not be reachable (otherwise it would not be weakly deterministic). The configuration may be unreachable trivially -- by such rules using an unreachable state or a symbols that have no related symbols in the complementarity relation. But a configuration may be unreachable non-trivially, if it is possible to tell how many symbols will be read from each strand before reaching certain state.

Both weakly deterministic and deterministic WKA are in reality not deterministic (in an intuitive sense). Their determinism relies on the fact, that the configuration is known. But that is probably not a typical way how to work with WKA, since WKA decides the membership in a language for the upper strand only. That means that a compatible lower strand has to be found in the process of running the WKA. Theoretically, it is possible to approach this problem by first generating all possible lower strands for the given upper strand based solely on the complementarity relation and afterwards use all these pairs as inputs for the WKA. In such a case the weakly deterministic and deterministic automata would be truly deterministic, however this is clearly not feasible for non-trivial complementarity relations. Therefore, the strongly deterministic WKA is the only one witch is truly deterministic under all circumstances, because the identity relation required leaves no space these types of non-determinism.

\section{Watson-Crick grammars}
There are several WK grammars.


\section{Watson-Crick pushdown automata}
The Watson-Crick Pushdown automata (WCPDA) have been introduced in \cite{WK_PUSHDOWN_AUT}. It is basically a two-head pushdown automaton with the complementarity relation added on top. Formally a WCPDA $P$ is a 10-tuple $P = (Q, \#, \$, V, \Gamma, \delta, q_0, Z_0, F, \rho)$ with most symbols having the same standard meaning as in conventional Pushdown automaton -- $Q$ is a finite set of states, $V$ is an input alphabet, $\Gamma$ is a stack alphabet, $q_0 \in Q$ is a starting state, $Z_0 \in \Gamma$ is a starting stack symbol and $F \subseteq Q$ is the set of final states. Symbols $\#, \$ \notin V$ are left and right input markers on the two strands. $\rho$ is the complementarity relation similar to standard WKA.

$\delta$ is a set of rules in the following form: $(q, ({w_1 \atop w_2}), x) \rightarrow (q', \gamma) \textnormal{ where } q, q' \in Q, w_1, w_2 \in V^* \cup \#V^* \cup V^*\$ \cup \#V^*\$, x \in \Gamma, \delta \in \Gamma^*$. That means the automaton can transition from state $q$ to $q'$ reading the input $w_1$ with the first head and $w_2$ with the second and go to state $q'$ while putting a string (i.e. 0-n symbols) of the stack symbols onto the stack. The two strands on the input are enclosed in the beginning symbol $\#$ and the closing symbol $\$$, therefore the symbol $\#$ may appear in the begginning of $w_1$ or $w_2$ and similarly the closing symbol $\$$ at the end.




\section{Watson-Crick context-free systems}

\section{Role of the complementarity relation}

\section{Comparison of expressing power of various models}

\chapter{Existing ways of testing membership in Watson-Crick languages} \label{chapter:WK_CYK}

\section{Using deterministic automata}

\section{WK-CYK algorithm}
The WK-CYK algorithm was introduced in \cite{WK_CYK} and it is an enhancement of the CYK algorithm modified for WK languages.

\subsection{The CYK algorithm}
The CYK algorithm, is used to decide the membership in a language defined by a context-free grammar, which must be in the Chomsky normal form (CNF).

On the input there is a string and a grammar and the algorithm decides whether the string belongs to the language defined by the grammar. There are two kinds of rules in a grammer in the CNF (disregarding the $S \rightarrow \epsilon$ rule which is used only to include empty string in the language): $A \rightarrow a$ and $A \rightarrow BC$ where $A, B, C$ are non-terminals and $a$ is a terminal. In the first stage, it analyses the first kind of rules --- each of the symbols from the input string has to be generated by a rule or several rules of this form. Thus it gets a set of candidate non-terminals for each symbol.

In the next stage it uses the second kind of rules. Every non-terminal (except the starting one) has to be generated by such a rule.
The algorithm is looking for rules which can generate the candidate non-terminals which have been found in the previous stage. All possible combinations need to be considered, for instance the sequence of non-terminals $ABC$ may be generated by rules $X \rightarrow AB$ and $Y \rightarrow XC$ or by rules $X \rightarrow BC$ and $Y \rightarrow AX$. In this way, the algorithm needs to find all possible ways to generate words of increasing length (all parse trees). Finally, it needs to find a non-terminal that can generate the whole word and it must be the starting non-terminal in the given grammar. If it succeeds, the word given on the input is in the language, otherwise it is not.

The complexity of the CYK algorithm is $O(n^3 \times R)$ where n is the input string length and R is the number of rules in the grammar.

\todo{possibly show an example and the triangular visualization}

\subsection{Watson-Crick Chomsky normal form}
Just like the CYK algorithm works with grammars in the Chomsky normal form, the WK-CYK algorithm requires grammars to be in the Watcon-Crick Chomsky normal form. A Watcon-Crick Chomsky normal form (WK-CNF) is a modification of CNF for Watson-Crick context-free grammars. A grammar in the WK-CNF has only rules of one of the following forms:

\begin{itemize}
  \item{$A \rightarrow \wkpair{a}{\lambda}$}
  \item{$A \rightarrow \wkpair{\lambda}{a}$}
  \item{$A \rightarrow B C$}
  \item{$S \rightarrow \wkpair{\lambda}{\lambda}$ (this rule is used only to include an empty word in the language)}
\end{itemize}

where $A$, $B$ and $C$ are non-terminals, $S$ is the starting non-terminal and $a$ is a terminal of the grammar. It is possible to transform any WK context-free grammar to the WK-CNF. The steps are mostly analogous to the transformation of the standard context-free grammar to CNF. This process includes:

\begin{enumerate}
  \item{removing $\lambda$-rules}
  \item{removing unit rules (rules of the form $A \rightarrow B$)}
  \item{removing useless rules and symbols (symbols that are unreachable from the starting symbol or cannot lead to a terminal string, and rules which use such symbols)}
  \item{replacing every terminal on the left-hand side of each rule with a new nonterminal and adding a new corresponding rule}
  \item{breaking down the non-terminals producing rules, so that they produce only two at a time}
\end{enumerate}

Detailed description of the transformation can be found in \cite{WK_CYK}.

\subsection{Description of the WK-CYK algorithm}
A complication compared to CYK that WK-CYK has to deal with is the ambiguity in the order of generating terminals. In case of standart context-free grammar in the CNF, the order of non-terminals that generate a word, for instance $abcd$, is clear --- if the rules are $A \rightarrow a$, $B \rightarrow b$, $C \rightarrow c$ and $D \rightarrow d$, the word with non-terminals that generates that terminal string must be $ABCD$. The order cannot change.

But in case of WK grammars, the order is not clear. For the terminal string $\wkpair{ab}{cd}$, the only given order is $a$ before $b$ and $c$ before $d$, anything else is uncertain. If the rules are $A \rightarrow \wkpair{a}{\lambda}$, $B \rightarrow \wkpair{b}{\lambda}$, $C \rightarrow \wkpair{\lambda}{c}$ and $D \rightarrow \wkpair{\lambda}{d}$, that terminal word can be produced by six different orderings of the non-terminals:
$ABCD$, $ACBD$, $ACDB$, $CABD$, $CADB$ and $CDAB$.

WK-CYK algorithm expects a grammar in the WK-CNF and a double stranded string on the input. Since one of the algorithm's requirements is that the complementarity relation must be identity, the upper and lower strands are always the same.

WK-CYK uses sets $X_{a:b,c:d}$ --- sets of non-terminals, that can generate a part of the input double stranded string specified by the indexes $a$, $b$, $c$ and $d$. $a$ and $b$ are indexes of a terminals in the upper strand thus specifying an interval (starting index is 1 and borders are included). The lower strand interval is specified by indexes $c$ and $d$. If a pair of indexes is 0, no symbols for the corresponding strand are included. For instance, for the input $\wkpair{abcd}{abcd}$, $X_{2:2,0:0}$ will contain a set of non-terminals generating $\wkpair{b}{\lambda}$, $X_{2:4,1:3}$ non-teminals generating $\wkpair{bcd}{abc}$.

\subsubsection{The main procedure of WK-CYK}
In the first step, WK-CYK finds sets $X_{i:i,0:0}$ and $X_{0:0,i:i}$ for $0 < i \leq |input|$. These are non-terminals that directly generate single terminals. Then, it searches for ways to generate segments of the input of increasing lengths, beginning with length of 2 and up to the length of $2n$ where $n$ is the length of the input. It is beacuse in the input of length 2 there are actually $2n$ of terminals --- $n$ in the upper and $n$ in the lower strand. For each length of $n$ it takes all possible combinations of number of symbols from the upper and the lower strands. For instance, if the length of the current segment is 3, that could be 3 terminals from the upper strand and 0 from the lower, 2 and 2, 1 and 3 or 0 and 4.

For each of these segments, it calls procedure \textit{compute set} (described below) which finds all non-terminals, that could generate the given segment.
For computation of a set of non-terminals $X_{i:j,k:l}$ WK-CYK uses a procedure called \textit{compute set}. When WK-CYK uses this procedure to compute set $X$ of a segment of length $n$, it is necessary to have already computed sets $X$ for all segments of length $m < n$. Therefore it proceeds from the length 1 upward.

Let us consider an example with input $\wkpair{abcd}{abcd}$. The first step looks for way of generating the individual terminals, in other words non-terminals that generate $\wkpair{a}{\lambda}$, $\wkpair{\lambda}{a}$, $\wkpair{b}{\lambda}$ etc. Then it looks for non-terminals that could generate two terminals, meaning either two terminals from the upper strand, two terminals from the lower strand or one from each. In each of these cases, all possible comninations need to be considered. If the two terminals are from the upper strand, the combinations are either $\wkpair{ab}{\lambda}$, $\wkpair{bc}{\lambda}$ or $\wkpair{cd}{\lambda}$ (or using the $X$ sets: $X_{1:2,0:0}$, $X_{2:3,0:0}$ or $X{3:4,0:0}$). It the two retminals are one from each strand, there are 16 combinations $\wkpair{x}{y}$ where $x, y \in \{a, b, c, d\}$ ($X_{1:1,1:1}$, $X_{2:2,1:1}$, $X_{1:1,2:2}$, ...). And for terminals from the lower strand, the combinations are $\wkpair{\lambda}{ab}$, $\wkpair{\lambda}{bc}$ or $\wkpair{\lambda}{cd}$ ($X_{0:0,1:2}$, $X_{0:0,2:3}$ and $X_{0:0,3:4}$).

When the segment length of $2n$ has been computed, WK-CYK is finished. It succeeded if the starting symbol $S$ can generate the whole input, in other words: $S \in X_{1:n,1:n}$.

\subsubsection{The compute set procedure}
The \textit{compute set} procedure has as a parameter a segment of input specified by the four indexes. It searches all pairs of sets $X$ which could together produce the given segment. If the segment consists of symbols from one strand only ($X_{i:j,0:0}$ or $X_{0:0,i:j}$), the situation is siplier --- it needs to consider the pairs of sets $X$ that produce the segment split in any two parts. If the segment contains symbols from both strands, there are more ways to split it:

\begin{itemize}
  \item{one set could produce the entire upper strand and the second set could produce the entire lower strand, or the other way around}
  \item{one set could produce and entire upper (or lower) strand and any part the lower (upper), the other the rest of the incomplete strand. In this casec all possible divisions of the divided strand need to be considered}
  \item{both set could produce parts of both strands, again, all possible combinations of divisions of both strands need to be considered}
\end{itemize}

When all combinations of two $X$ sets potentially producing the input segment have been found, the procedure then needs to check the grammar rules to find those rules which actually generate non-terminal from these sets. For such a rule, the non-terminal on its left-hand side is going to be included in the procedure's result. The final result is then a set of all such non-terminals.

Lets us consider an example, where the procedure computes $X_{1:2,1:2}$, in other words, the segment $\wkpair{ab}{ab}$. All possible division of this segment are:

\begin{enumerate}
  \item{$\wkpair{ab}{\lambda}$, $\wkpair{\lambda}{ab}$ --- $X_{1:2,0:0}$, $X_{0:0,1:2}$}
  \item{$\wkpair{\lambda}{ab}$, $\wkpair{ab}{\lambda}$ --- $X_{0:0,1:2}$, $X_{1:2,0:0}$}
  \item{$\wkpair{ab}{a}$, $\wkpair{\lambda}{b}$ --- $X_{1:2,1:1}$, $X_{0:0,2:2}$}
  \item{$\wkpair{a}{ab}$, $\wkpair{b}{\lambda}$ --- $X_{1:1,1:2}$, $X_{2:2,0:0}$}
  \item{$\wkpair{a}{\lambda}$, $\wkpair{b}{ab}$ --- $X_{1:1,0:0}$, $X_{2:2,1:2}$}
  \item{$\wkpair{\lambda}{a}$, $\wkpair{ab}{b}$ --- $X_{0:0,1:1}$, $X_{1:2,2:2}$}
  \item{$\wkpair{a}{a}$, $\wkpair{b}{b}$ --- $X_{1:1,1:1}$, $X_{2:2,2:2}$}
\end{enumerate}

All these seven $X$ sets must already be computed, they are sets for segments of lengths 1, 2 and 3. Each of these steps contains zero or more non-terminals that can produce the given subsegment. In last step, the procedure checks all rules of the grammar of type $A \rightarrow BC$ to find rules where $B$ is in the first $X$ set and $C$ is in the second $X$ set of one of the divisions of the segment. If it finds such a rule, it means that the non-terminal on the left-hand side can produce the givan segment.


\subsubsection{Two remarks regarding WK-CYK}
1. Zulkufli et al. provides the WK-CYK algorithm in pseudo-code in \cite{WK_CYK}, section 6.1. The loop on the line 13 iterates $\beta$ from 0 to $n$. In this context, $\beta$ represents the length of the lower strand segment, while $\alpha$ represents the length of the upper strand segment while $\alpha = y - \beta$ where $y$ is the length of the whole segment. Part of the loop actually calculates with non-sensical values. When calculating with segment that are shorter then one strand (i.e. $y < n$), it includes case when $\beta > y$ and so $\alpha < 0$. In other words, the algorithm splits the segment of length, say, 2 to part of lengths 3 and -1.

And then, when calculating with segment that is longer than one strand, it includes case when $\beta$ is too short and so $\alpha$ is then longer then a strand length. In other words, it splits the segment of length, say, 8 when the length of the strands is 4, to lengths 7 and 1, but the maximum length can be the 4.

This does not affect the correctness of the computation, because the non-sensical values find no result. However, more precise and efficient solution would be to iterate $\beta$ in the interval: $\langle max(y-n, 0), min(n, y)\rangle$, instead of interval $\langle 0, n\rangle$.

\medskip

2. The time complexity of WK-CYK is $\mathcal{O}(n^6)$. As Zulkufli et al. \cite{WK_CYK} explain in section 6, the WK-CYK main procedure has complexity of $\mathcal{O}(n^4)$ and the nested procedure \textit{compute set} has complexity of $\mathcal{O}(n^2)$. This is true with respect to the input length. Possibly, more precise description of the complexity would be $\mathcal{O}(n^6 \times R)$ where $n$ is the input and $R$ is the number of rules in the grammar. The description of the procedure \textit{compute set} uses the operation of set union ($\cup$) as if it has constant time complexity, which, in reality, it does not --- it requires iterating over the rules of the grammar.

\section{Using WK Pushdown automata}

\chapter{Testing membership by searching the state space} \label{chapter:parse_tree}
This section will introduce the main algorithm of this thesis for testing memebership in WK context-free languages. In this thesis it is referred to as \textbf{Space state search} or \textbf{Tree search}. Its core is a standard Breadth-first search algorithm (BFS) and then various optimizations are added.

Standard BFS starts with a root node. In case of grammars, that is the starting non-terminal symbol. Then the tree is built by applying all possible rules to all possible non-terminals. Each rule application generates a new node. The node is a word which consists of some non-terminals, some terminals in the upper strand and terminals in the lower strand. The node that is the solution needs to meet the following criteria:
\begin{enumerate}
  \item{It contains no non-terminals.}
  \item{The upper and lower strands are of the same length.}
  \item{Each pair of symbols from the upper and lower strands with the same index must be related by the complementarity relation.}
  \item{The upper strand must be equal to the input string.}
\end{enumerate}
If the criteria are met, the algorithm has found the right node and that means the input string is part of the language defined by the grammar. It has been accepted by the state space search algorithm.

The BFS algorithm always finds solution if there is one. It finds the optimal solution, which in this case means the shortest sequence of rules that generate the input string from the starting non-terminal. However, whether the solution is optimal or not is irrelevant for the membership problem. If there is no solution, the algorithm will probably never stop, as the state tree is usually infinite. Also, such a tree would grow very rapidly and the solution would usually not be found in a reasonable time frame. Therefore, some optimizations need to be used. This work introduces two kinds of optimizations. Firstly, identifying dead ends in the search tree and removing them from the computation -- this is referred to as \textbf{pruning}. Secondly, choosing such nodes for the subsequent computation, which seem to be most promising in leading to the solution. This is referred to as \textbf{node precedence}.

Besides pruning and node precedence heuristics, the algorithm keeps a set of states which have been generated (added to the tree), in order to avoid analysing the same word repeatedly or even getting stuck in a loop. Also, it consideres leftmost derivation only. This means that a node which contains several non-terminals can generate new nodes only by applying rules to the first non-terminal in the word.

The figure \ref{fig:search_tree} shows an example of a tree search computation. The rules of its grammar are $S \rightarrow S S \:|\: A B C$, $A \rightarrow \wkpair{a}{a} \:|\: \wkpair{b}{b}$ and some rules $B \rightarrow ...$, $C \rightarrow ...$ which are not important. $S$ is the starting non-terminal, therefore $S$ is the first node and there are two possible rules that can be applied to $S$ -- this node has two successors. The node precedence heuristic will choose one of the successors to be analysed next -- perhaps the left one with word $A B C$. This node, too, only has two successors, which are made by the two rules that can be applied to the first non-terminal -- $A$. Even though there are some rules for $B$ and $C$, these rules are not used to produce successors, yet. The nodes created by rules applied on $B$ would be successors of the words $\wkpair{a}{a} B C$ and $\wkpair{b}{b} B C$ which have the symbol $B$ as the first non-terminal from the left.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{search_tree.png}
  \caption{Example of a search tree}
  \label{fig:search_tree}
\end{figure}


\section{Identifying a dead end in the state tree}
A blind BFS would stop seaching a branch only when all non-terminals have been used to generate all possible end words (words with terminals only). But sometimes it is possible to tell in advance that a specific word cannot lead to the desired solution. If that is the case the whole branch can be removed from the tree. The next sections describes various ways (heuristics) of recognizing the dead branches. These are called pruning heuristics, there are five of them and each one has an abbriviation which is used further on.

\begin{enumerate}
  \item{Detecting that one of the strands is already too long --- SL}
  \item{Detecting that the overall word is already too long --- TL}
  \item{Matching the starting terminals in the upper strand to the input --- WS}
  \item{Checking the complementarity relation --- RL}
  \item{Comparing the input to a regular expression generated from the word --- RE}
\end{enumerate}


\subsection{One of the strands is too long (SL)}
A terminal symbol which appears both in the upper or lower strand can never disappear further in the branch. That means that the count of all symbols in upper and in the lower strand must not be grater then the length of the input string. Otherwise the solution can never be reached from that branch.

\subsection{The word including non-terminals is too long (TL)}
Non-terminals present a more complex problem when dealing with the length of the word. First of all, the algorithm calculates in advance how many terminals each non-terminal produces at minimum. For instance, if the grammar contains rules: $A \rightarrow A A \:|\: \wkpair{ab}{cd} \:|\: B B$ and $B \rightarrow \wkpair{a}{\lambda}$, non-terminal $B$ produces always one terminal, that means one terminal at minimum. Non-terminal $A$ can produce various number of terminals, but two at minimum -- thanks to the rule $A \rightarrow B B$ and the fact that $B$ has the minimum of one. This value is than considered to be the length of the given non-terminal.
This length can be applied both to the upper or to the lower strand because, in general, it is not known which strand will absorb the symbols generated from the non-terminal.
This then leads to the following constraint on the word:

$$|upper| + |lower| + |nts| \geq 2 \times |input|$$

where $|upper|$ and $|lower|$ are the counts of terminals in the upper and lower strands, $|nts|$ is the length of all non-terminals in the word and $|input|$ is the length of the input string. If this constraint is broken, the word can not lead to the solution and the branch can be trimmed.


If the grammar contains no $\lambda$-rule (rule of the form $N \rightarrow ({\lambda \atop \lambda})$), This contraint guarentees that the algorithm will finish. Once all the words within the given length limit have been generated and solution not found, the search will end.

If the grammar does contain $\lambda$-rules, the previous constrain can still be applied -- the non-terminals that can be erased are assinged the length of zero. In this case, it is not possible to guarentee that the search will end, because the non-terminals of length zero can be combined infinitely many times. However, it is possible to utilize the algorithm for removing $\lambda$-rules (which is described in \cite{WK_CYK} and which is implemented in the WK\_CFG class described in the next chapter).

\subsection{The beginning of the word does not match the input (WS)}
If the current word begins with some terminal symbols in the upper strand, these symbols will always stay at the beginning further in the given branch. If these symbols do not match the prefix of the input string of the same length, the input string can never be generated from this branch.
If on the other hand, the word starts with a non-terminal, there is nothing to be said about what can be at the beginning of the word futher in the branch.

It is possible to check the end of the word in the same manner, but as it will be described futher, the generation is performed from the left to the right and so there is no benefit in checking the end.

\subsection{Checking the complementarity relation (RL)}
As previously described, the symbols in the upper and lower strands with the same indexes must be related by the complementarity relation. Unfortunately, this can be checked only at the beginning of the word. Indexes of the symbols in the middle part (anywhere after the first non-terminal) are not known. Thus this check can be understood as an extension of the previous one -- if the word begins with some terminal symbols and there are some symbols in both the upper and lower strands, these symbols can be checked if they are in the relation -- as long as its counterpart is already known.

\subsection{Checking the word correspondence to a regular expression --- RE}
It is possible to generate a regular expression that represents the current word. Each non-terminal serves as a wild card ($.*$). Each terminal in the upper strand stands for itself. Lower strand is ignored. This expression must be matchable to the input string, otherwise it is not possible to generate it from the current branch. For instance, if the word is this:
$$\wkpair{abc}{f}N_1\wkpair{d}{gh}N_2\wkpair{e}{i}N_3$$

The resulting regular expression will be: $\verb/^abc.*d.*e/$. The symbols $abc$ must be at the beginning (therefore the \verb/^/ denoting beginning of the expression); then it is not known what will be generated by the non-terminal $N_1$ -- therefore the wildcard; then there will have to be a symbol $d$; another wildcard for non-terminal $N_2$; symbol $e$; and then anything. Starting non-terminal can be represented by omitting the symbol \verb/^/ denoting the beginning of the string. Ending non-terminal can be represented by omitting the symbol \verb/$/ denoting the end of the string.

It is possible to come up with some more checks that could identify a dead end in the search tree. The disadvantage of any check is the computing power that has to be used for any node that is generated and analysed. If some checks are unlikely to significantly trim the tree and/or are complicated to compute, it is not clear if they will improve the actual performance of the algorithm.

\subsection{Examples of pruning}
Let us consider a grammar with no $\lambda$-rules with the complementarity relation being an identity, input string \verb/'abcd'/  and a following words:

\begin{enumerate}
\item{
$$\wkpair{a}{ab} N_1 \wkpair{\lambda}{cd} N_2 \wkpair{\lambda}{e}$$
The input string can never be generated from this word because the fragments of the lower strand are too long already -- it has five symbols and the input string only has four.
}
\item{
$$\wkpair{ab}{ab} N_1 N_2 \wkpair{\lambda}{d} N_3 N_4$$
This word would be promising if there had been some $\lambda$-rules. Since there are not, the word contains too many non-terminals. Each of them is going to generate at least one terminal symbol and only three symbols are missing ($c$ and $d$ in the upper strand and $c$ in the lower strand). Inevitably, there will be at least one symbol too many.
}
\item{
$$\wkpair{abd}{\lambda} N_1 \wkpair{\lambda}{ab} N_2$$
Regarless of what canb be generated from $N_1$ and $N_2$, the upper strand will always have to begin with symbols $abd$. There is no way how to insert $c$ between $b$ and $d$. Therefore the input string can never be generated from here.
}
\item{
$$\wkpair{abc}{ac} N_1 \wkpair{\lambda}{d} N_2$$
The upper strand looks promising as it starts with the same symbols $abc$ as the input string. But the lower strand starts with $ac$. The first symbol pair $(a/a)$ passes the check, the second one $(b/c)$ does not. The third symbol in the upper strand -- $c$ cannot be related to any symbol -- it has no counterpart, yet. The check was always going to end with the second symbol pair.
}
\item{
$$N_1 \wkpair{b}{\lambda} N_2 \wkpair{a}{\lambda} N_3$$
Whatever is generated from the non-terminals $N_1$, $N_2$, $N_3$ the upper strand will always keep the order of symbols -- first symbol $b$ and then symbol $a$ (with potentially some symbol before, in between and after). That can never result in the string $abcd$.
}

\section{Heuristics for node precedence} \label{heur_node_pref}

The aim of the node precedence heuristics is to choose a path in the search tree, which is likely to lead to the solution -- the more promising nodes are taken before the others and their successors are generated. The individual heuristic functions attept to answer the question -- which node is more promising than the rest? It assignes each node a number -- an evaluation of the node. The lower the node evaluation, the higher priority the node has.

Such heuristics can only be effective if the answer to the membership is positive. Unfortunately, if it is negative, it does not help that the algorithm eliminates the more promising branches of the tree first. Eventually, it will have to search through all possible states anyway in order to make sure that there is no solution.
Following node precedence heuristics have been implemented and tested.

\begin{itemize}
  \item{No heuristic -- the evaluation of the word is always 0. This is used for comparision to the other heuristics.}
  \item{Aversion to non-terminals (NTA) -- the evaluation is equal to the count of non-terminals in the word}
  \item{Weighted aversion to non-terminals (WNTA) -- each non-terminal has a precalculated weight, which is the minimum amount of rules that must be used in order to generate only terminals from it. The evaluation is equal to the sum of the weights of all non-terminals in the word.}
  \item{The terminal matching -- there are three variants that differ slightly (TM1, TM2, TM3). Each of them increases priority for each upper strand non-terminal (going from left to right), which matches the input string symbol on the same index.
  \begin{itemize}
    \item{TM1 examines terminals from the left side while ignoring non-terminals, decresing evaluation (i.e. increasing priority) for each match and finishing when it discoveres the first difference.}
    \item{TM2 is similar to TM1, but when it discovers a difference, does not finish, but increases evaluation and moves on}
    \item{TM3 evaluates the first item in the word only. If it is a non-terminal, it returns zero.}
  \end{itemize}
  }
  \item{Combinations of NTA/WNTA and TM1/TM2/TM3 -- There are six combinations, as it does not make sense to combine NTA and WNTA together.}
\end{itemize}

In summary, there are 12 node precedence heuristics considered in total (including the first, empty heuristic). Unlike in case of pruning, where all methods can be applied at the same time, there can be only one node precedence heuristic at one time. Therefore section \todo{LINK} contains the tests and comparison of the effectiveness of these heutistics.
\end{enumerate}

\section{Theoretical complexity of the State space search}
The State space search algorithm uses Breadth-first search (BFS) as its basis. Both the time and space complexity of BFS is $\mathcal{O}(b^d)$ where $b$ is the maximum number of successors of a node (branching factor) and $d$ is the depth of the tree. The branching factor is then equal to the maximum number of rules of the given grammar that have the same non-terminal on the left-hand side. This is because always only the first non-terminal in the word is used to generate sucessors in the tree. The depth of the tree is going to be different for different grammars and even for different inputs.

In general, the theoretical complexity of the State space search algorithm is not impressive, it is much worse then WK-CYK's $\mathcal{O}(n^6)$ or $\mathcal{O}(n^6 \times R)$. However, this is because it has been designed with a rather practical approach, it relies heavily on the heuristics and optimizations and thus has usually much better performance.

\section{Parallelizing the State space search algorithm}
The parallelization of State space search should be very much possible and straightforward. The algorithm uses a priority queue to store all the nodes which are to be analysed. Therefore, multiple processes could be taking nodes from the queue and analyse them independently. There are many variants how this could be done --- analysing one node at the time and returning result immediately to the queue shared by all process or analysing independetly whole segments of the tree with less need for synchronization but, perhaps, more redundant work --- these are questions of the efficiency of the actual implementation which is out of the scope of this work.

\chapter{Implementation of the state space search} \label{chapter:implementation}

The implementation has been done in the language Python3. The main components are the following:
\begin{itemize}
  \item{The class representing a Watson-Crick context-free grammar, including the WK-CYK algorithm and the State space search algorithm}
  \item{Classes representing a rule of a grammar and a tree node}
  \item{Set of grammar definitions and generators of the input strings}
  \item{Set of test scripts that run the various tests comparing heuristics, performance etc.}
  \item{Test runner class which is a sort of middle layer between the test scripts and the main grammar class.}
\end{itemize}

In the code and its description I use the term \textbf{word} to refer to the right hand side of the rules and, in general, a list of letters. The term \textbf{letter} is a one element of the word, which is either a non-terminal, or a segment with terminals. Such segments with terminals are stored as a pair (tuple) of two lists --- upper and lower strand. For instance, a word
$A \wkpair{abc}{\lambda} B$
has three letters: non-terminals $A$ and $B$ and a segment of non-terminals $\wkpair{abc}{\lambda}$, which contains two lists, first (upper strand) with three items --- terminals $a$, $b$ and $c$ and the second one (lower strand) is a empty list.

Any time two or more letters with terminals appear next to each other, they are merged. For instance, a three-letter word: $\wkpair{abc}{\lambda} A \wkpair{b}{b}$ after application of rule $A \rightarrow \wkpair{\lambda}{a}$, will result in a word with just one letter: $\wkpair{abcb}{ab}$.


\section{Implementation of the main class representing the grammar}
The class representing a grammar is called \textit{cWK\_CFG}, the source file is \textit{lib/ctf\_WK\_grammar.py}. It contains the following data:
\begin{itemize}
  \item{the items which define the grammar: \textit{nts} --- set of non-terminals, which are represented by alpha-numeric characters; \textit{ts} --- set of terminals, also represented by alpha-numeric characters; \textit{startSymbol} --- starting non-terminal; \textit{rules} --- set of grammar rules, which are objects of the class \textit{cRule}; \textit{relation} --- list representing a complementarity relation, it contains tuples of two terminal. These five items are parameters, which need to be passed to a constructor, which corresponds to the way how a context-free WK grammar is theoretically defined.}

  \item{\textit{nodePrecedenceList} --- a list of all implemented node precedence heuristics, it is stored as a list of pairs (tuples) of heuristic name and function; \textit{currentNodePrecedence} is the index of the active one}

  \item{\textit{pruningOptions} --- a dictionary with a pruning function as a key and boolean as a value indicating which pruning heuristics are active; \textit{pruneCnts} --- a dictionary of pruning functions as keys and count, how many times the given pruning has been used, as a key}

  \item{\textit{ruleDict} --- grammar rules stored in a dictionary with non-terminals as keys and list of letters as values. This is a more efficient way of accessing rules for a given non-terminal, then iterating over all rules and filtering them based on left-hend side non-terminal.}

  \item{\textit{relDict} --- complementarity relation stored in a more convinient way in a dictionary with first symbol as a key and string of symbols as a value. This is more efficient way of finding all symbols related to a given symbol.}

  \item{\textit{ntDistances} --- a precalculated dictionary, with all non-terminals as keys and distance to terminals as value. This distance is a minimum number of rules, which lead from the given non-terminal, to a word with terminals only. This is used for the node precedence heuristic WNTA.}

  \item{\textit{erasableNts} --- a precalculated set of non-terminals, which can be erased by appling certain sequnce of rules. It is used for removing the $\lambda$-rules.}

  \item{\textit{termsFromNts} --- a precalculated dictionary, which has all non-terminals as key and as a value minimum amount of terminals, which can be generated from this non-terminal. This is used in the pruning heuristic TL.}

  \item{timeLimit --- after how long should the tree search or WK-CYK timeout}
\end{itemize}

A constructor of the \textit{cWK\_CFG} class requires a the five items described in the first bullet-point (in that order). Example of an object construction would then be (for rules definition, see \ref{later}):
\begin{verbatim}
  g = cWK_CFG(['S', 'A'], ['a', 'b'], 'S', rules, [('a', 'a'), ('b', 'b')])
\end{verbatim}

\bigskip
The class \textit{cWK\_CFG} has these key functionalities:
\begin{itemize}
  \item{init, restore and backup}
  \item{run of the parse tree algorithm}
  \item{set of pruning heuristics}
  \item{set of node precedence heuristics}
  \item{transformation of grammar to CNF}
  \item{run of the WK-CYK algorithm}
\end{itemize}

\subsection{Init, restore and backup}
During the initialization of the class, several methods are run ensuring validity of the grammar and precalculating data.

\begin{itemize}
  \item{method \textit{is\_consistent} verifies that the construtor parameters that define the grammar are consistent: sets of terminals and non-terminals must be exclusive, the start symbols and all rule lefh-hand sides must be found among the non-terminals, rule right hand sides must contain only specified termianal and non-terminals and the complementarity relation list must refer only to the defined terminals. It the method fails, an exception is thrown and the class is not created.}
  \item{method \textit{generate\_rule\_dict} parses the set of rules and creates \textit{ruleDic}}
  \item{method \textit{generate\_relation\_dict} parses the complementarity relation and creates \textit{relDict}}
  \item{method \textit{find\_erasable\_nts} creates the set \textit{erasableNts}, a set of non-terminals that can be erased by applying a sequence of rules. The set is empty if the grammar contains no $\lambda$-rule.}
  \item{method calc\_nt\_distances creates the dictionary \textit{ntDistances} mapping non-terminals to the minimum number of rules needed to reach a terminal word}
  \item{method calc\_min\_terms\_from\_nt creates the dictionary \textit{termsFromNts} mapping non-terminals to the minimum number of terminals, that they can generate}
  \item{method calc\_rules\_nt\_lens calculates a rule length for each rule.}
\end{itemize}

A rule length is a value which indicates how the minimal length of a word will be changed after application of the rule. It is used by the TL (total length) pruning heuristic. The rule length is equal to the negative left-hand side non-terminal length plus lengths of all elements on the right-hand side.

For instance, the word is $A \wkpair{ab}{c} B$, the rule being applied is $A \rightarrow B \wkpair{d}{d}$ and the minimum number of terminals generated from $A$ is 2 and from $B$ is 3. The resulting word will be $B \wkpair{dab}{dc} B$. The word at the start had total length of 8 (1 for each terminal symbol, 2 for $A$, 3 for $B$). The word afterwards has a total length of 11 (3 for each $B$ and 1 for each terminal). Therefore, the rule $A \rightarrow B \wkpair{d}{d}$ must have length of 3. And it does: $-2$ for the $A$ on the left-hand side, 1 for each of the two terminals and 3 for the $B$.

Since the grammar is able to apply certain transformations (like to CNF or removing lambda rules), it is convinient to be able to save the state of the grammar and later restore it. This is what the methods \textit{backup} and \textit{restore} are for. The \textit{backup} method simply saves a copy of the set rules, non-terminals and terminals as \textit{rulesBackup}, \textit{ntsBackup}, \textit{tsBackup}. The \textit{restore} method then restores these backup sets and runs again the precalculating methods similarly to the class initialization phase.

\subsection{Run of the state space search algorithm}
The State space search algorithm is run by the \textit{can\_generate} method which has one parameter: the string to be tested for language membership. It uses a priority queue (python \textit{PriorityQueue} class from the \textit{queue} modul) to store the nodes if the search tree. When getting items from this queue, it returns the smallest value item, which is in the queue. The class representing a node is \textit{cTreeNode} (described in \ref{later}). The method also uses a set of hashes all states that have been put in the queue (whether they are still there or have been taken out), so that duplicate nodes are skipped.

At the beginning, the queue contains one node --- the starting non-terminal. If the queue is empty (and there is no node being parsed), it indicates that the whole state space has been searched and the methods ends with a negative response. Otherwise it gets next node from the queue, generates all possible successors of this node (method \textit{get\_all\_successors}). Each of the successor nodes is checked, if it is the desired solution (method \textit{is\_result}). If so, the search optionally prints the path to the solution and ends with the positive result. Otherwise it checks whether the node is new (has not been in the queue before) and is added to the queue. Also, it is checked during every iteraton if the time limit has been exceeded. If so, it returns an empty response (\textit{None} value).

The return value of the method is a tuple containing following items: maximum number of items in the queue, number of all nodes in the queue, list of pruning statistics, the actual result (\textit{True}, \textit{False}, or \textit{None}).

The \textit{get\_all\_successors} requires two parameters --- a node and the input string (which is then passed to the pruning methods). First, it finds the first non-terminal in the given word. It applies all the rules (method \textit{apply\_rule}) for this non-terminal that the grammar has, each time creating a new node. The node is then checked by the all the pruning algorithms (method \textit{is\_word\_feasible} described in \ref{later}) to see, if the node can lead to the solution. If so, the priority of the node is calculated (method \textit{calculate\_distance} described in \ref{later}) and the node is yielded as a result.

The \textit{is\_result} method needs to check all the conditions that the node has to meet in order to be a solution (word contains only one letter, the letter is a segment of termianals, its upper strand and lower strand have the same length, all the symbols from the two strands correspond to the complementarity relation and the upper strand is equel to the input string). It takes a word and the input string as parameters and returns a boolean value indicating whether the word is the solution.

The \textit{apply\_rule} method takes as parameter: a word, an index of non-terminal to be used, rule right-hand side. It removes the non-terminal specified by its index (because there can be more than one occurrence of this non-temrinal in the word) from the word, and replaces it with the word snippet specified by the rule left-hand side. It contains logic for merging letters containing terminals if they appear in the word next to each other. It returns the final word.

\subsection{Pruning heuristics}
In order to be able to work flexibly with the pruning methods the class contains a dictionary (called \textit{pruningOptions}) of the implemented pruning functions and indication whether they are active or not. The method \textit{is\_word\_feasible} iterates through all items in this dictionary and if the value is \textit{True} indicatind an active pruning, it runs the corresponding method.
The pruning methods are:
\begin{itemize}
  \item{\textit{prune\_check\_strands\_len} (SL): checks that the sum of the symbols in the upper and the lower strand is not graeter that the input length}
  \item{\textit{prune\_check\_total\_len} (TL): checks that the total length of the word (described in \ref{later}) is not greater then the doubled input length}
  \item{\textit{prune\_check\_word\_start} (WS): checks that the starting terminals of the word correspond to the input string}
  \item{\textit{prune\_check\_relation} (RL): checks that the starting terminals meet the coplementarity relation constraint}
  \item{\textit{prune\_check\_regex} (RE): checks that the input matches the regular expression based on the word}
\end{itemize}

All the pruning methods require a word and the input string as parameters. They return a boolean value indicating, whether the word is feasible or not.

\subsection{Node precedence heuristics}
Just like with pruning, the node precedence functionality needs to be flexible --- it must be possible to switch between various node precedence methods. The functionality is implemented by the method \textit{compute\_distance}. This methods simply uses \textit{nodePrecedenceList} and \textit{currentNodePrecedence} to call the right specific method.
There are 12 of these methods called \textit{compute\_distance\_[name]} where \textit{name} is the name of the specific heuristic (one of NTA, WNTA, TM1, TM2, TM3, NTA\_TM1, NTA\_TM2, NTA\_TM3, WNTA\_TM1, WNTA\_TM2, WNTA\_TM3, no\_heuristic).

\subsection{Transformation of grammar to WK Chomsky normal form}
The transformation of any grammar to the WK Chomsky normal form is described in \cite{WK_CYK}. It is quite similar to the transformation of standard context-free grammar to CNF. It is performed by the method \textit{to\_wk\_cnf} and it takes the following steps:

\begin{enumerate}
  \item{Removing $\lambda$-rules --- this is done by the method \textit{remove\_lambda\_rules}}

  \item{Removing unit rules --- unit rules are rules in the form of $A \rightarrow B$ where $A$ and $B$ are non-terminals. These is performaed by method \textit{remove\_unit\_rules}.}

  \item{Removing unterminatable symbols --- unterminatable symbols are non-terminals, which cannot be transformed into terminal strings by any sequence of rules. It is possible to remove them without affecting the grammar language, because if such a rule ever appeared in a word, the whole word would be automatically useless. Any rules containing such a symbol are useless, as well and also removed. This is performed by the method \textit{remove\_unterminatable\_symbols}.}

  \item{Removing unreachable symbols --- unreachable symbols are symbols that can never appear in the word, because there is no sequence of rules leading from the starting symbol, that generates these rules. Therefore they can removed without affecting the grammar language. Any rules containing such a symbol are useless, as well and also removed. This is done by the method \textit{remove\_unreachable\_symbols}.}

  \item{Dismanteling rules generating terminal letters --- WK-CNF requires all rules generating terminals to generate one symbol only, this means only rules in the form of $A \rightarrow \wkpair{a}{\lambda}$ and $A \rightarrow \wkpair{\lambda}{a}$ are allowed. This is achieved by method \textit{dismantle\_term\_letters}, which iterates through all the rules and replaces any terminal letters at the rule right-hand side with a newly generated non-terminal. Afterwards, new rules are and non-terminals are created which assure that the terminal letter is generated in steps, each rule having at maximum two items on the right-hand side.

  If the rule is $A \rightarrow A B \wkpair{ab}{c}$ ($A$, $B$ being non-terminals, $a$, $b$, $c$ being terminals), then the $A$ and $B$ are skipped and the letter $\wkpair{ab}{c}$ is replaced by a new non-terminal, say $N_1$. Then, another rule is created: $N_1 \rightarrow \wkpair{ab}{c}$, which needs to be broken down further. For each terminal in this letter, a new rule is created and the terminal replaced by a new non-terminal, until there remains only one terminal in that letter. The final set of rules is then going to be: $A \rightarrow A B N_1$, $N_1 \rightarrow \wkpair{a}{\lambda} N_2$, $N_2 \rightarrow \wkpair{\lambda}{c} N_3$, $N_3 \rightarrow \wkpair{b}{\lambda}$
  }

  \item{Dismanteling rules generating non-terminals --- in the final stage of transformation, the method transform\_to\_wk\_cnf\_form} iterates through all the rules, it keeps rule in the WK-CNF form ($A \rightarrow BC$, $A \rightarrow \wkpair{a}{\lambda}$ and $A \rightarrow \wkpair{\lambda}{a}$) and breaks down other rules in a process analogous to the actions of \textit{remove\_lambda\_rules}.

  \item{Recalculation of data --- runs the methods that precalculate data, similarly to the class init phase or after \textit{restore} method}
\end{enumerate}

The methods removing $\lambda$-rules, unit rules, unterminatable symbols and unreachable symbols are useful even outside of the transformation to the WK-CNF. Removing unterminatable symbols and unreachable symbols (and rules containing these symbols) are optional but useful steps in the transformation. The dismanteling of rules are two steps that make sense only in this context.


\subsection{Run of the WK-CYK algorithm}
The WK-CYK is implemented by method \textit{run\_wk\_cyk} which takes as a single parameter the input string and returns a boolean value indicating whether the input has been accepted or not. Similarly to Tree search, every iteration checks the elapsed time and if it exceed the time limit, it returns an empty value (\textit{None}). The implementation is taken from the section 6 of \cite{WK_CYK}. The \textit{run\_wk\_cyk} method corresponds to the \textit{sets construction} procedure of this paper. The \textit{compute set} procedure called from \textit{sets construction} then corresponds to the \textit{compute set} method of the cWK\_CNF class.


\section{Implementation of the rule and node classes}
A rule of a grammar is modelled by the class \textit{cRule}. It contains following data:
\begin{itemize}
  \item{\textit{lhs} --- a non-terminal, left-hand side of the rule}
  \item{\textit{rhs} --- a word, right-hand side of the rule}
  \item{\textit{ntsLen} --- length of all non-terminals of the right-hand side}
  \item{\textit{upperCnt} --- count of all terminals in the upper strand of the right-hand side}
  \item{\textit{lowerCnt} --- count of all terminals in the lower strand of the right-hand side}
\end{itemize}

The items \textit{ntsLen}, \textit{upperCnt} and \textit{lowerCnt} are there for optimization purposes. It is always possible to iterate over the word and count them, but it is more efficient to count it once for every rule and store this value.

The \textit{cRule} class then contains the following methods:
\begin{itemize}
  \item{\textit{compactize} --- method is called during the object initialization phase and ensures that the right-hand side does not contain any terminal letters next to each other, if it does, then these are merged together. For instance a rule $A \rightarrow \wkpair{a}{b} \wkpair{a}{b}$ is transformed to an equivalent rule $A \rightarrow \wkpair{aa}{bb}$.}
  \item{\textit{calculate\_cnts} --- method is called during the object initialization phase and counts values for \textit{upperCnt} and \textit{lowerCnt}}. The length of non-teminals is calculated during the initialization of the cWK\_CFG object, because it needs to know the length of non-terminals.
\end{itemize}

A constructor of the \textit{cRule} function requires a left-hand side of the rule, which is a non-terminal, and a right-hand side of the rule, which is a word, i.e. a list of letters. An example of a rule object creation for rule $A \rightarrow A \wkpair{ab}{\lambda}$ would then be:

\begin{lstlisting}[language=Python]
  cRule('A', ['A', (['a', 'b'], [])])
\end{lstlisting}

\bigskip

A tree node is modelled by the class \textit{cTreeNode}, which contains the following:
\begin{itemize}
  \item{\textit{word} --- the actual word of the grammar}
  \item{\textit{upperStrLen} --- number of upper strand terminals in the word}
  \item{\textit{lowerStrLen} --- number of lower strand terminals in the word}
  \item{\textit{ntLen} --- length of all non-terminals}
  \item{\textit{parent} --- node in the search tree, which is this node's  predecessor, this is not necessary for the search, but once a solution is found, it may be useful to know what path has been taken to reach it.}
  \item{\textit{hashNo} --- a unique has of the node calculated during the object initialization}
  \item{\textit{precedence} --- a value assigned by the active node precedence heuristic. This value is used to compare two objects of this class, which is needed by the prioroty queue used during the run of tree search.}
\end{itemize}

A \textit{cTreeNode} constructor requires a word, three integers specifing the \textit{upperStrLen}, \textit{lowerStrLen} and \textit{ntLen}, parent node, and precedence. An example of a creation of this class object (probably root node) could then be:

\begin{lstlisting}[language=Python]
  cTreeNode(['S'], 0, 0, 1, None, 1)
\end{lstlisting}

\bigskip

Both of these classes, \textit{cTreeNode} and \textit{cRule}, as well as the main class cWK\_CFG, are in the source file \textit{lib/ctf\_WK\_grammar.py}, as they are quite closely related.


\section{Implementation of the test runner class, test scripts and grammars}
The grammars that are used for testing both the Tree search and the WK-CYK are stored in the file \textit{lib/grammars.py}. Each grammar is characterized primarily by its rules, those are created first. Then the instance of the cWK\_CFG is created and then a generator of inputs is assigned to each grammar.

A generator of inputs is then a method of each grammar object called \textit{input\_gen\_func}. Its purpose is to generates inputs for the given grammar of increasing lengths. It takes 3 parameters, a starting number of characters, a step --- how many characters should be added in the next generated input, and a boolean indicating whether the generated inputs should be accepted by the grammar or not. The generator does not have to return the input string exactly of the length as it was specified. Sometimes it is not even possible. The generated string's length may be approximate to the stecified values.

Here is an example of input generator use for grammar 1:
\begin{lstlisting}[language=Python]
generator = g1.input_gen_func(5, 2, True)
input1 = next(generator)   # generates 'aaaaa'
g1.can_generate(input1)
input2 = next(generator)   # generates 'aaaaaaa'
g1.can_generate(input2)
\end{lstlisting}

I have implemented the following five test script, which are in the root directory:
\begin{itemize}
  \item{\textit{ts\_node\_precedence\_tests.py} --- runs a test for all of the 40 grammars (20 in basic form and 20 in the CNF) with inputs which are going to be accepted. Each test runs the Parse tree on this input one time for each of the available node precedence heuristics.}

  \item{\textit{ts\_pruning\_tests.py} --- runs two tests for each of the 40 grammars, one with an input that will be accepted and one that will be rejected by the Tree search. Each test runs the Parse tree with all pruning heuristics inactive, with all active, and then for each one it runs with activates all but the one heuristic. Thus comparing how each one heuristic contributes to the overall performance.}

  \item{\textit{ts\_speed\_tests.py --- runs two tests (one positive, one negative) for each of the 40 grammars, in each test a Tree search is run repeatedly (up to 30 times or it is stopped if it exceeds time limit)} with increasing input length. This is used to analyse the time (possible also memory) complexity of the Parse tree with respect to the input length.}

  \item{\textit{ts\_var\_inputs\_tests.py} --- runs tests for some hand-picked inputs of the same length in order to compare, how the different variants of the same length inputs affect the performance}

  \item{\textit{wk\_cyk\_tests.py} --- runs two tests (one positive, one negative) for 17 grammars, which are ready for WK-CYK run. Those must be in the CNF and grammars 5, 19 and 20 are excluded, since they use other complementarity relation then identity. In each test, the grammar runs the WK-CYK repeatedly with increasing input length}
\end{itemize}

Each of these scripts print its output into a table where all the results are compared. Ouputs, which I received by running these test scripts, are in the output directory.

All these scripts use the \textit{cPerfTester} class, which is a sort of middle layer between the grammar class and the test scripts. It helps with displaying the result table and gathering and parsing data returned by the algorithm runs.
It contains the following methods:

\begin{itemize}
  \item{\textit{run\_test\_ntimes} --- runs the Tree search several times, calculates and returns averages over results these runs}
  \item{\textit{run\_node\_precedence\_test} --- a wrapper used by the script \textit{ts\_node\_precedence\_tests.py}}
  \item{\textit{run\_prune\_test} --- a wrapper used by the script \textit{ts\_pruning\_tests.py}}
  \item{\textit{run\_speed\_test} ---a wrapper used by the script \textit{ts\_speed\_tests.py}}
  \item{\textit{var\_inputs\_test} ---a wrapper used by the script \textit{ts\_var\_inputs\_tests.py}}
  \item{\textit{run\_wk\_cyk\_test} ---a wrapper used by the script \textit{wk\_cyk\_tests.py}}
\end{itemize}


\section{How to use the cWK\_CFG class}
It is possible to directly run the one or more of the test scripts from the root folder. They do not have other requirements than to have Python3 installed. In the application root directory in Linux terminal it can be run simply by typing:

\begin{verbatim}
  python3 ts_node_precedence_tests.py
  python3 ts_pruning_tests.py
  ...
\end{verbatim}

In order to use the \textit{cWK\_CFG} class directly there is a demo script in the root directory \textit{demo.py} which shows the use of pre-defined grammars and creating a new grammar step by step and can be called in the same way:

\begin{verbatim}
  python3 demo.py
\end{verbatim}

In a nutshell, when using a pre-defined grammar, it can be used right after import. To test if a grammar 1 can genarate a string $aaaaa$, one could write:
\begin{lstlisting}[language=Python]
  from lib.grammars import g1
  o, a, p, result = g1.can_generate('aaaaa')
  print(result)
\end{lstlisting}

And to define a grammar from scratch and, for instance, run the WK-CYK algorithm, one can write:
\begin{lstlisting}[language=Python]
  rules = [
    cRule('S', ['S', 'S', 'S']),   # S -> S S S
    cRule('S', [(['a'], ['a'])])   # S -> a/a
  ]
  g1 = cWK_CFG(['S'], ['a'], 'S', rules, [('a', 'a')])
  g1.to_wk_cnf()
  result = g1.run_wk_cyk('aaaaa')
  print(result)
\end{lstlisting}


\chapter{Testing the state space search and comparison to WK-CYK} \label{chapter:testing}

\section{Watson-Crick grammars used for testing}

For the testing of the tree search algorithm and the WK-CYK algorithm, following Watson-Crick grammars have been used. Unless stated otherwise, the set of non-terminals and the set of terminals will simply be defined by the rules -- all the uppercase letters are non-terminals of the grammar and all the lowercase letters and digits are terminals. The starting non-terminal will be $S$ and the complementarity relation is going to be identity. With these specifications in mind the grammar can be defined by the rules only.

\begin{enumerate}
  \item{
    $$S \rightarrow \wkpair{a}{a} \:|\: S S S$$

    The accepted language is: $a(aa)^*$
  }

  \item{
    $$S \rightarrow \wkpair{a}{a} S \:|\: \wkpair{b}{b} S \:|\: \wkpair{c}{c} S \:|\: \wkpair{abc}{abc}$$

    The accepted language is: $(a+b+c)^*abc$

    The aim of this example is to test inputs with the decisive part on the very end. This could be problematic since the algorithm expands the non-terminals from left to right.
  }

  \item{
    $$S \rightarrow A \wkpair{abc}{abc}$$
    $$A \rightarrow A \wkpair{a}{a} \:|\: A \wkpair{b}{b} \:|\: A \wkpair{c}{c} \:|\: \wkpair{\lambda}{\lambda}$$

    The accepted language is: $(a+b+c)^*abc$

    The aim of this example is, again, to test inputs with the decisive part on the very end while, at the same time, the rules are left recursive.
  }

  \item{
    $$S \rightarrow Q \wkpair{a}{a} \:|\: A B C D E F G$$
    $$Q \rightarrow Q Q \:|\: A B C D E F G$$
    $$A \rightarrow \wkpair{a}{a} \:|\: \wkpair{\lambda}{\lambda}$$
    $$B \rightarrow \wkpair{b}{b} \:|\: \wkpair{\lambda}{\lambda}$$
    $$C \rightarrow \wkpair{c}{c} \:|\: \wkpair{\lambda}{\lambda}$$
    $$D \rightarrow \wkpair{d}{d} \:|\: \wkpair{\lambda}{\lambda}$$
    $$E \rightarrow \wkpair{e}{e} \:|\: \wkpair{\lambda}{\lambda}$$
    $$F \rightarrow \wkpair{f}{f} \:|\: \wkpair{\lambda}{\lambda}$$
    $$G \rightarrow \wkpair{g}{g} \:|\: \wkpair{\lambda}{\lambda}$$

    The accepted language is: $a?b?c?d?e?f?g? + (a?b?c?d?e?f?g?)^*a$ ($x?$ denotes that the symbol $x$ is optional, i.e. $(x + \epsilon)$ )

    During the transformation of this grammar to WK-CNF (more specifically, when removing the $\lambda$-rules) the number of rules increases rapidly.
  }


  \item{
    $$S \rightarrow \wkpair{a}{t} S \:|\: \wkpair{t}{a} S \:|\: \wkpair{g}{c} S \:|\: \wkpair{c}{g} A$$
    $$A \rightarrow \wkpair{c}{g} A \:|\: \wkpair{a}{t} S \:|\: \wkpair{g}{c} S \:|\: \wkpair{t}{a} B$$
    $$B \rightarrow \wkpair{c}{g} A \:|\: \wkpair{a}{t} S \:|\: \wkpair{t}{a} S \:|\: \wkpair{g}{c} C$$
    $$C \rightarrow \wkpair{a}{t} C \:|\: \wkpair{t}{a} C \:|\: \wkpair{g}{c} C \:|\: \wkpair{c}{g} C \:|\: \wkpair{\lambda}{\lambda}$$

    The complementarity relation of this grammar is: $\{(a, t), (t, a), (c, g), (g, c)\}$

    The accepted language is: $(\{a,t,c,g\}^*ctg\{a,t,c,g\}^*)^*$

    This grammar is a first step towards an actual analysis of the DNA. In this case it simply looks for the substring $ctg$
  }

  \item{
    $$S \rightarrow \wkpair{a}{\lambda} S \:|\: \wkpair{a}{\lambda} A$$
    $$A \rightarrow \wkpair{b}{a} A \:|\: \wkpair{b}{a} B$$
    $$B \rightarrow \wkpair{\lambda}{b} B \:|\: \wkpair{\lambda}{b}$$


    The accepted language is: $a^nb^n$ where $n \geq 1$
  }

  \item{
    $$S \rightarrow \wkpair{a}{a} S \wkpair{a}{a} \:|\: \wkpair{b}{b} S \wkpair{b}{b} \:|\: \wkpair{c}{c}$$

    The accepted language is: $wcw^R$ where $w \in \{a, b\}^*$($w^R$ is the reversal of string $w$)
  }

  \item{
    $$S \rightarrow \wkpair{a}{a} S \wkpair{a}{a} \:|\: \wkpair{b}{b} S \wkpair{b}{b} \:|\: \wkpair{\lambda}{\lambda}$$

    The accepted language is: $ww^R$ where $w \in \{a, b\}^*$
  }

  \item{
    $$S \rightarrow B L \:|\: R B$$
    $$L \rightarrow B L \:|\: A$$
    $$R \rightarrow R B \:|\: A$$
    $$A \rightarrow B A B \:|\: \wkpair{2}{2}$$
    $$B \rightarrow \wkpair{0}{0} \:|\: \wkpair{0}{0}$$

    The accepted language is: $x2y: x, y \in \{0,1\}^* \wedge \:|x| \neq |y|$
  }

  \item{
    $$S \rightarrow T \:|\: T \wkpair{p}{p} S$$
    $$T \rightarrow F \:|\: F T$$
    $$F \rightarrow \wkpair{e}{e} \:|\: W \:|\: \wkpair{o}{o} T \wkpair{p}{p} S \wkpair{c}{c} \:|\: X \wkpair{s}{s} \:|\: \wkpair{o}{o} Y \wkpair{c}{c} \wkpair{s}{s}$$
    $$X \rightarrow \wkpair{e}{e} \:|\: \wkpair{l}{l} \:|\: \wkpair{0}{0} \:|\: \wkpair{1}{1}$$
    $$Y \rightarrow T \wkpair{p}{p} S \:|\: F \wkpair{d}{d} T \:|\: X \wkpair{s}{s} \:|\: \wkpair{o}{o} Y \wkpair{c}{c} \wkpair{s}{s} \:|\: Z Z$$
    $$W \rightarrow \wkpair{l}{l} \:|\: Z$$
    $$Z \rightarrow \wkpair{0}{0} \:|\: \wkpair{1}{1} \:|\: Z Z$$

    The accepted language includes regular expressions over symbols 0 and 1 with parenthesis ($o$ for opening and $c$ for closing parenthesis) operators $+$ (p), $*$ (s), $\cdot$ (d) and symbols $\emptyset$ (e), $\varepsilon$ (l)
  }

  \item{
    $$S \rightarrow A \:|\: B \:|\: A B \:|\: B A$$
    $$A \rightarrow \wkpair{a}{a} \:|\: \wkpair{a}{a} A \wkpair{a}{a} \:|\: \wkpair{a}{a} A \wkpair{b}{b} \:|\: \wkpair{b}{b} A \wkpair{b}{b} \:|\: \wkpair{b}{b} A \wkpair{a}{a}$$
    $$B \rightarrow \wkpair{b}{b} \:|\: \wkpair{a}{a} B \wkpair{a}{a} \:|\: \wkpair{a}{a} B \wkpair{b}{b} \:|\: \wkpair{b}{b} B \wkpair{b}{b} \:|\: \wkpair{b}{b} B \wkpair{a}{a}$$

    The accepted language is: $\{a, b\}^* \setminus ww$ where $w \in \{a, b\}^*$ -- i.e. the complement of the copy language.
  }

  \item{
    $$S \rightarrow \wkpair{r}{\lambda} S \:|\: \wkpair{r}{\lambda} A$$
    $$A \rightarrow \wkpair{d}{r} A \:|\: \wkpair{d}{r} B$$
    $$B \rightarrow \wkpair{u}{d} B \:|\: \wkpair{u}{d} C$$
    $$C \rightarrow \wkpair{r}{u} C \:|\: \wkpair{r}{u} D$$
    $$D \rightarrow \wkpair{\lambda}{r} D \:|\: \wkpair{\lambda}{r}$$

    The accepted language is: $r^nd^nu^nr^n$ where $n \geq 1$
  }

  \item{
    $$S \rightarrow \wkpair{a}{\lambda} S \wkpair{b}{\lambda} \:|\: \wkpair{a}{\lambda} A \wkpair{b}{\lambda}$$
    $$A \rightarrow \wkpair{c}{a} A \:|\: \wkpair{\lambda}{c} B \wkpair{\lambda}{b}$$
    $$B \rightarrow \wkpair{\lambda}{c} B \wkpair{\lambda}{b} \:|\: \wkpair{\lambda}{\lambda}$$

    The accepted language is: $a^nc^nb^n$ where $n \geq 1$
  }

  \item{
    $$S \rightarrow \wkpair{a}{\lambda} S \:|\: \wkpair{a}{\lambda} A$$
    $$A \rightarrow \wkpair{b}{\lambda} A \:|\: \wkpair{b}{\lambda} B$$
    $$B \rightarrow \wkpair{c}{a} B \:|\: \wkpair{c}{a} C$$
    $$C \rightarrow \wkpair{d}{b} C \:|\: \wkpair{d}{b} D$$
    $$D \rightarrow \wkpair{\lambda}{c} D \:|\: \wkpair{\lambda}{d} D \:|\: \wkpair{\lambda}{\lambda}$$

    The accepted language is: $a^nb^mc^nd^m$ where $n, m \geq 1$
  }

  \item{
    $$S \rightarrow \wkpair{a}{\lambda} S \:|\: \wkpair{b}{\lambda} S \:|\: \wkpair{c}{\lambda} A$$
    $$A \rightarrow \wkpair{a}{a} A \:|\: \wkpair{b}{b} A \:|\: \wkpair{\lambda}{c} B$$
    $$B \rightarrow \wkpair{\lambda}{a} B \:|\: \wkpair{\lambda}{b} B \:|\: \wkpair{\lambda}{\lambda}$$

    The accepted language is: $wcw$ where $w \in \{a, b\}^*$
  }

  \item{
    $$S \rightarrow \wkpair{a}{\lambda} S \wkpair{a}{a} \:|\: \wkpair{a}{\lambda} A \wkpair{a}{a} $$
    $$A \rightarrow \wkpair{bb}{a} A \:|\: \wkpair{bbb}{a} A \:|\: \wkpair{\lambda}{b} B$$
    $$B \rightarrow \wkpair{\lambda}{b} B \:|\: \wkpair{\lambda}{\lambda}$$

    The accepted language is: $a^nb^ma^n$ where $2n \leq m \leq 3n$
  }

  \item{
    $$S \rightarrow S S \:|\: \wkpair{a}{a} S \wkpair{b}{b} \:|\: \wkpair{a}{\lambda} S \:|\: \wkpair{a}{\lambda} A$$
    $$A \rightarrow \wkpair{b}{a} A \:|\: \wkpair{b}{a} B \:|\: \wkpair{b}{a}$$
    $$B \rightarrow \wkpair{\lambda}{b} B \:|\: \wkpair{\lambda}{b} \:|\: B B \:|\: \wkpair{a}{a} S \wkpair{b}{b} \:|\: \wkpair{a}{\lambda} S \:|\: \wkpair{a}{\lambda} A$$

    The accepted language is: $w: \#_a(w) = \#_b(w)$ and for any prefix $v$ of $w: \#_a(v) \geq \#_b(v)$  where $\#_a(x)$ denotes the number of occurrences of symbol $a$ in string $x$
  }

  \item{
    $$S \rightarrow \wkpair{l}{\lambda} S \:|\: \wkpair{l}{\lambda} A$$
    $$A \rightarrow \wkpair{r}{l} A \:|\: \wkpair{r}{l} B$$
    $$B \rightarrow \wkpair{l}{r} B \:|\: \wkpair{\lambda}{r} B \:|\: \wkpair{\lambda}{\lambda} \:|\: A$$

    The accepted language is: $(l^n r^n)^k$ where $n$ does not increase for subsequent $k$. For instance: $lllrrrlrlr$ is within the language, $llrrlllrrr$ is not.
  }

  \item{
    The grammar is identical to the grammar 13 with a difference in the complementarity relation. Relations between symbols $a, b$ and symbols $a, c$ are added.

    This means that the relation is: $\rho = \{(a, a), (b, b), (c, c), (a, b), (b, a), (a, c), (c, a)\}$

    The accepted language is: $a^n b^m c^n$ where $n, m \geq 1$
  }

  \item{
	The grammar is identical to the grammar 14 with a difference in the complementarity relation. Relation between symbols $a, b$ is added making the relation $\rho = \{(a, a), (b, b), (c, c), (d, d), (a, b), (b, a)\}$

	The accepted language is: $a^m b^n c^o d^p$ where $m, n, o, p \geq 1 \wedge m+n = o+p$
  }


\end{enumerate}

There are twenty grammars altogether. Grammars 1--5 are regular, 6--11 are context-free and 12--18 are context-sensitive. Grammars 19 and 20 are context-free but they also have a non-bijective complementarity relation.

In reality, there are not 20 but 40 grammars, because all of them are going to be used in the basic form and after the transformation to the Chomsky normal form. That results in a different grammar (although accepting the same language) which is usually significantly more difficult to calculate with, as there are more rules and many rules generate more non-terminals.

\section{Testing the state space search}

There is a lot of parameters that could be tested and analysed. How efficient are the various heuristics (both pruning and node preference) for different grammars. What inputs lengths are answered withing a reasonable time frame? Or more generally -- what is the relation between input length and time to get the answer? What are the memory requirements? What is the difference in decition time between input strings which are in the given language and those which are not? Is there a difference between some inputs of the same lengths?

In order to analyse these questions, I have decided to test the state space search in the following stages.
\begin{enumerate}
  \item{Comparison of the node precedence heuristics and analysis of their efficiency.}
  \item{Comparison of the pruning heuristics and analysis of their efficiency.}
  \item{Analysis of the time and memory complexity based on the length of the input string for all grammars.}
  \item{Testing if there are any different inputs of the same length which would result in a significant difference in the computation complexity.}
  \item{Testing the WK-CYK algorithm with various grammars and inputs and comparision to the state space search.}
\end{enumerate}

\subsection{Comparison of the node precedence heuristics efficiency}
In section \ref{heur_node_pref}, 12 node precedence heuristics have been described and only one of them can be active at a time.

The space state search algorithm contains five different tests of pruning the state tree (which can and usually do work at the same time) and five rule precedence heuristics (of which only one is active). In order to compare their effectiveness I used the script \textit{node\_prec\_heuristic\_compare.py} which runs one test for each of the 40 grammars with an input that will be accepted. It is not useful to test node precedence heuristics with inputs that are not withing the given language as in such cases, the whole space state needs to be searched and node precedence cannot help in any way. The input strings have been chosen to have suitable lengths, so that the computation is finished (at least with some heuristics) in a reasonable time -- specifically within the time limit of ten seconds, but also to last some measurable amount of time.

Each of the 40 tests is run 12 times with a different node precedence heuristic. There are three metrics to observe:
\begin{itemize}
  \item{How many times the search timeouted?}
  \item{What the total time in which all 40 tests were completed is for each heuristic. There should be a kind of penalty involved if the test timeouts, because the time needed for the computation is in that case certainly greater then the time it actually ran, before it got stopped by the time limit. Therefore for the sake of the comparison, the time need is in this case doubled.}
  \item{The total time normalized for each test -- the time of the fastest heuristic for each test is normalized to one. This is probably the most telling metric as each test has roughly the same impact on the final number.}
\end{itemize}

Each test prints out the results in a table similar to \ref{tab:node_heuristics_table}. The upper part of the table displays the description of the accepted language, number of rules, non-terminals and terminals, string on the input, whether the input is expected to be accepted and time limit. In the lower part the table shows for each of the node precedence heuristic how much time it took, how many states were in the queue and how many were analysed, how many times each pruning heuristic was used (SL -- strands too long, TL -- strands is too long including the non-terminals, WS -- word does not match the input, RL -- complementarity relation failed, RE -- regular expression check failed) and the result of the search (True, False or Timeout). It would not be practical to present here the complete output, but it can be recreated simply by running the script again and is also attached in the file \textit{output/node\_prec\_heuristic.txt}.

\begin{figure}[h]
  \caption{An output of the node precedence heuristics test}
  \label{tab:node_heuristics_table}
\begin{tabular}{ |l|l|l|l|l|  }
  \hline
  \multicolumn{5}{|l|}{Test 1} \\
  \hline
  Grammar & \multicolumn{4}{|l|}{$a(aa)^*$} \\
  Rules / NTs / Ts & \multicolumn{4}{|l|}{2/1/1} \\
  Input string & \multicolumn{4}{|l|}{aaaaaaaaaaaaaaaaaaaaaaaaa... [len 801]} \\
  Should accept & \multicolumn{4}{|l|}{Yes} \\
  Timeout & \multicolumn{4}{|l|}{7 seconds} \\
  \hline
  Strategy & Time & States Q+C & Prunes (SL, TL, WS, RL, RE)& Accepted \\
  \hline
 NTA & 0.4886 & 994 + 3001  & 0, 5, 0, 0, 250 & TRUE \\
 WNTA & 0.4189 & 498 + 2503 & 0, 3, 0, 0, 250 & TRUE  \\
 TM1 & 0.739 & 1489 + 3994 & 0, 7, 0, 0, 249  & TRUE  \\
 TM2 & 0.7385 & 1489 + 3994 & 0, 7, 0, 0, 249 & TRUE  \\
 TM3 & 0.7366 & 1489 + 3994 & 0, 7, 0, 0, 249  & TRUE  \\
 NTA+TM1 & 0.5903 & 992 + 2999 & 0, 5, 0, 0, 250 & TRUE  \\
 NTA+TM2 & 0.5915 & 992 + 2999 & 0, 5, 0, 0, 250 & TRUE  \\
 NTA+TM3 & 0.5923 & 992 + 2999 & 0, 5, 0, 0, 250 & TRUE  \\
 WNTA+TM1 & 0.495 & 498 + 2503 & 0, 3, 0, 0, 250 & TRUE  \\
 WNTA+TM2 & 0.4949 & 498 + 2503 & 0, 3, 0, 0, 250 & TRUE  \\
 WNTA+TM3 & 0.4977 & 498 + 2503 & 0, 3, 0, 0, 250 & TRUE  \\
 no heuristic & 2.4445 & 4188 + 22147 & 0, 89, 0, 0, 239 & TRUE  \\
  \hline
  \hline
\end{tabular}
\end{figure}

It is interensting to note that different heuristics are better in different test cases. This is illustrated by selected test cases which are on figure \ref{fig:selected_tests}. There are some case when the best heuristic is the empty one, which assigned zero to each node, like in case of test 23. This is because this heuristic is the simpliest to compute and if no heuristic is effective in a particular test case, this one wins. But it does not win by a large margin, so these cases are not decisive.

In some cases, a certain heuristics don't work so well, but their combination does. This can be seen in test 22 -- NTA and WNTA have poor result, comparable to no heuristic. TM1, TM2, TM3 have somewhat better result, but by far the best result is achieved by combination of NTA with any version of TN.

\begin{figure}[h]
  \includegraphics[scale=0.42]{four_heur_tests.png}
  \caption{Selected comparisons of node precedence heuristics}
  \label{fig:selected_tests}
\end{figure}

The figure \ref{fig:node_heuristics_comp} shows the total result for all of the 40 tests. It is clear that the best results are achieved by the combination NTA, TM2. This heuristic did not timeout in any of the tests and overall is the fastest. Even if in some cases there are some faster heuristics, it's usually not by much.

\begin{figure}[h]
  \includegraphics[scale=0.44]{node_heuristics_comp.png}
  \caption{Comparison of the node precedence heuristic functions}
  \label{fig:node_heuristics_comp}
\end{figure}

\subsection{Comparison of the pruning heuristics efficiency}

Pruning has the advantage of being useful whether the input string is going to be accepted or rejected by the tree search. Also, all of the pruning can be on at the same time -- each node can be checked by all available check to see if it can be pruned or not.

The testing is performed over 80 tests -- each of the 40 grammars is used for a positive test (where the input will be accepted) and a negative test (input will be rejected).
Each test contains seven runs of the tree search algorithm -- one where all pruning heuristics are on, one where all are turned off, and one for each heuristic where all are turned on except the given one.

Similarly to the node precedence heuristics comparison, the metrics that are important are the total time needed to compute the 80 tests and a number of timeouts.

The tests can be run by the script \textit{pruning\_heuristic\_compare.py} and each test print out a table similar to the table \ref{tab:prune_heuristics_table}. Again, it would not be practical to include the entire output here. The complete set of results can be recreated by running the script again and it is included in file \textit{output/prune\_heuristic.txt}.

\begin{figure}[h]
  \caption{An output of the pruning heuristics test}
  \label{tab:prune_heuristics_table}
\begin{tabular}{ |l|l|l|l|l|  }
  \hline
  \multicolumn{5}{|l|}{Test 1} \\
  \hline
  Grammar & \multicolumn{4}{|l|}{$a(aa)^*$} \\
  Rules / NTs / Ts & \multicolumn{4}{|l|}{2/1/1} \\
  Input string & \multicolumn{4}{|l|}{aaaaaaaaaaaaaaaaaaaaaaaaa... [len 801]} \\
  Should accept & \multicolumn{4}{|l|}{Yes} \\
  Timeout & \multicolumn{4}{|l|}{7 seconds} \\
  \hline
  Strategy & Time & States Q+C & Prunes (SL, TL, WS, RL, RE)& Accepted \\
  \hline
  ALL ON & 0.7841 & 799 + 1200 & 0, 3, 0, 0, 400 & TRUE \\
  strands len OFF & 0.7781 & 799 + 1200 & 0, 3, 0, 0, 400 & TRUE \\
  total len OFF & 0.7799 & 801 + 1200 & 0, 0, 0, 0, 400 & TRUE \\
  terms match OFF & 0.7705 & 799 + 1200 & 0, 3, 0, 0, 400 & TRUE \\
  relation OFF & 0.655 & 799 + 1200 & 0, 3, 0, 0, 400 & TRUE \\
  regex OFF & 0.3594 & 800 + 1599 & 0, 3, 0, 0, 0 & TRUE \\
  ALL OFF & 0.2179 & 801 + 1600 & 0, 0, 0, 0, 0 & TRUE \\
  \hline
  \hline
\end{tabular}
\end{figure}

The summary is displayed on Figures \ref{fig:prune_timeouts_comp} -- the number of timeouts and \ref{fig:prune_times_comp} -- the amount of time for each of the seven cases across the 80 tests. The smaller the individual bars are, the better the result. But in case of the bars representing a specific pruning heuristic being turned off, the bigger the bar, the more important the given heuristic is, because the result is that much worse without it.

\begin{figure}[h]
  \includegraphics[scale=0.5]{prune_timeouts_comp.png}
  \caption{TIMEOUTS}
  \label{fig:prune_timeouts_comp}
\end{figure}

\begin{figure}[h]
  \includegraphics[scale=0.7]{prune_times_comp.png}
  \caption{TOTAL TIMES}
  \label{fig:prune_times_comp}
\end{figure}


From these results it is clear that the tree pruning is the key feature of the tree search algorithm. After turning off the pruning, the results are very poor -- 67 out of 80 tests timeouted. The total time is then not relvant at all. The middle bars representing the individual pruning heuristics turned off needs to be compared to the first one, where all heuristics are on, to see how important the given heuristic actually is. Thus the figure suggests, that the RL (complementarity relation) check si the most important one, because turning it off had the biggests impact on the result. This can be a bit misleading, as some heuristics can be sometimes backed up by another one. This is the reason why the WS (match of leftmost terminals and the input string) seem to have a rather small impact. If this heuristic is turned off, the dead branch can be idntified by the RE (regular expression) check and so the impact is not so big. Similarly, turning off the SL (strands length) heuristic is of smaller impact, because it is backed up by TL (total length) heuristic.

Nevertheless, all heuristic are useful according to this result, because no other bar is as small as the first one. This means that the best result is achieved when all heuristics are on at the same time. This is especially important in the case of the RE heuristic. This one is quite demanding with regards to computation power -- regular expression match is performed each time this check is executed. Therefore it is the last check that is used, if there is another heuristic able to prune the node, a lot of computational power is saved. However, the figure shows clearly, that the RE heuristic contributes significantly to the overall performance (still, some tests cannot take advantage of this heuristic and turning it off would improve the results).



\subsection{Analysis of the memory requirements}
The tree search algorithm needs to keep in memory the nodes, which have been generated but not yet analyzed. These are the states in the queue. Also, it keeps track of all nodes which have already been generated. Those are the states that had been in the queue before and states that are there at the moment, as in both cases, there is no reason to put them into the queue again. There is no need to keep in memory the states that have been generated, their hash is enough.

Then it is necessary to remember states in the queue, the number of the states there can go up or down as the search progresses, but it is more likely to go up, unless the search is coming to an end. In any case, the important figure is the maximum number of states, that were in the queue at one point.

Next parameter that needs to be considered is the size of one state in the memory. As mentioned in the section \ref{chapter:implementation}, there are six integers (storing the number of terminals in the two strands, number of non-terminals, hash number, the node parent and the node precedence evaluation) and the word itself. The word can contain up to twice the number of symbols then is the length of the input string. If it contains more, it is going to be pruned.

This is not necessarily true, if the grammar contains some $\lambda$-rules and non-terminals that can be erased. Then the theoretical length of the word in memory has no limit, but this is not a typical scenario and can be avoided altogether by applying the $\lambda$-rules removal algorithm.

The equation for getting the memory requirement based on the number of states working with is than the following:

$$S_{all} \times size(int) + S_{open} \times (6 \times size(int) + 2 \times size(symbol) \times |input|)$$

$S_{all}$ is the number of all states generated, $S_{open}$ is the maximum number of states in the queue, $size(int)$ is the size of an integer (for this calculation, I will assume it is 64 bytes), $size(symbol)$ is the size of one symbol of the grammar and $|input|$ is the length of the input string.

However, current implementation has more memory consumption. This is because when an input is accepted, it may be interesting to know what path from the starting non-terminal to the goal has been discovered. In order to do that, the program needs to retain all the states that have been analysed, storing their hashes is not enough.

I've chosen grammar 3 in CNF to test the results in practise, as this grammar is among the hardest ones with respect to computing complexity. I used inputs in the form of $a^nb$ with increasing $n$, which will always be rejected, beacuse the grammar accepts strings that end with $abc$.

The figure \ref{fig:mem_consumption} on the left show the amount of memory requirements in relation to the input length and in the middle in relation to the time needed. To the right is the experimental result where I measured real memory comsuption of the program in time.

\begin{figure}[h]
  \includegraphics[scale=0.5]{mem_consumption.png}
  \caption{Memory consumption of tree search}
  \label{fig:mem_consumption}
\end{figure}



\subsection{The time complexity of the state space search}
The previous sections showed, that the best overall performance is achieved when using all of the tree pruning heuristics and using the NTA+TM2 as the node precedence heuristic. This may not be the case for every grammar or every input, but it is the case most of the time. Therefore this will be the setting used in following sections --- for testing the tree search performance, analysing the practical complexity and comparing to WK-CYK algorithm.

I used script \textit{input\_compl\_compare.py} to test the time complexity in relation to the input length. It runs 80 test, two for each of the 40 grammars --- one with inputs that are in the language defined by the grammar and so the inputs are going to be accepted --- and one with inputs which will be refused. Each test runs the tree search several times and increases the input length. It stops when the computation takes longer than a limit of ten seconds or after 30 runs. For each of the 80 tests it prints out table similar to \ref{tab:input_compl_test}. As it would not be practical to present all of the results here, they cen be recreated by simply running the script again and they are included in the file \textit{output/input\_complexity\_test.txt}.

\begin{figure}[h]
  \caption{An output of the time complexity test}
  \label{tab:input_compl_test}
\begin{tabular}{ |l|l|l|l|l|  }
  \hline
  \multicolumn{5}{|l|}{Test 1} \\
  \hline
  Grammar & \multicolumn{4}{|l|}{$a(aa)^*$} \\
  Rules / NTs / Ts & \multicolumn{4}{|l|}{2/1/1} \\
  Should accept & \multicolumn{4}{|l|}{Yes} \\
  Timeout & \multicolumn{4}{|l|}{7 seconds} \\
  \hline
  Input length & Time & States Q+C & Prunes (SL, TL, WS, RL, RE)& Accepted \\
  \hline
 501 & 0.3362 & 499 + 750 & 0, 3, 0, 0, 250 & TRUE \\
 601 & 0.4482 & 599 + 900 & 0, 3, 0, 0, 300 & TRUE \\
 701 & 0.5978 & 699 + 1050 & 0, 3, 0, 0, 350 & TRUE \\
 801 & 0.7744 & 799 + 1200 & 0, 3, 0, 0, 400 & TRUE \\
 901 & 0.9728 & 899 + 1350 & 0, 3, 0, 0, 450 & TRUE \\
 1001 & 1.1931 & 999 + 1500 & 0, 3, 0, 0, 500 & TRUE \\
 1101 & 1.4367 & 1099 + 1650 & 0, 3, 0, 0, 550 & TRUE \\
 1201 & 1.6959 & 1199 + 1800 & 0, 3, 0, 0, 600 & TRUE \\
 1301 & 1.9928 & 1299 + 1950 & 0, 3, 0, 0, 650 & TRUE \\
 1401 & 2.3087 & 1399 + 2100 & 0, 3, 0, 0, 700 & TRUE \\
 1501 & 2.6799 & 1499 + 2250 & 0, 3, 0, 0, 750 & TRUE \\
 1601 & 3.0023 & 1599 + 2400 & 0, 3, 0, 0, 800 & TRUE \\
 1701 & 3.3801 & 1699 + 2550 & 0, 3, 0, 0, 850 & TRUE \\
 1801 & 3.776 & 1799 + 2700 & 0, 3, 0, 0, 900 & TRUE \\
 1901 & 4.2107 & 1899 + 2850 & 0, 3, 0, 0, 950 & TRUE \\
 2001 & 4.6528 & 1999 + 3000 & 0, 3, 0, 0, 1000 & TRUE \\
 2101 & 5.1277 & 2099 + 3150 & 0, 3, 0, 0, 1050 & TRUE \\
 2201 & 5.6251 &2199 + 3300 & 0, 3, 0, 0, 1100 & TRUE \\
 2301 & 6.134 & 2299 + 3450 & 0, 3, 0, 0, 1150 & TRUE \\
 2401 & 6.665 & 2399 + 3600 & 0, 3, 0, 0, 1200 & TRUE \\
 2501 & 7.3398 & 2499 + 3750 & 0, 3, 0, 0, 1250 & TRUE \\
 2601 & 8.057 & 2599 + 3900 & 0, 3, 0, 0, 1300 & TRUE \\
 2701 & 8.4195 & 2699 + 4050 & 0, 3, 0, 0, 1350 & TRUE \\
 2801 & 9.2167 & 2799 + 4200 & 0, 3, 0, 0, 1400 & TRUE \\
 2901 & 10.3625 & 2899 + 4350 & 0, 3, 0, 0, 1450 & TRUE \\
  \hline
  \hline
\end{tabular}
\end{figure}

Different grammars have very different complexity as the following results show. The following graphs display result for each of the 20 grammars.

% Figure \ref{fig:res_grammar_1} shows results for grammar 1. 

\begin{figure}[h]
  \includegraphics[scale=0.33]{compl_result_grammar_1.png}
  \caption{Grammar 1: $a(aa)^*$}
  \label{compl_result_grammar_1}
\end{figure}

\begin{figure}[h]
  \includegraphics[scale=0.33]{compl_result_grammar_2.png}
  \caption{Grammar 2: $(a+b+c)^*abc$ left recursive rules}
  \label{compl_result_grammar_2}
\end{figure}

\begin{figure}[h]
  \includegraphics[scale=0.33]{compl_result_grammar_3.png}
  \caption{Grammar 3: $(a+b+c)^*abc$ right recursive rules}
  \label{compl_result_grammar_3}
\end{figure}

\begin{figure}[h]
  \includegraphics[scale=0.33]{compl_result_grammar_4.png}
  \caption{Grammar 4: $a?b?c?d?e?f?g? + (a?b?c?d?e?f?g?)^*a$}
  \label{compl_result_grammar_4}
\end{figure}

\begin{figure}[h]
  \includegraphics[scale=0.33]{compl_result_grammar_5.png}
  \caption{Grammar 5: $(\{a,t,c,g\}^*ctg\{a,t,c,g\}^*)^*$}
  \label{compl_result_grammar_5}
\end{figure}

\begin{figure}[h]
  \includegraphics[scale=0.33]{compl_result_grammar_6.png}
  \caption{Grammar 6: $a^n b^n$}
  \label{compl_result_grammar_6}
\end{figure}

\begin{figure}[h]
  \includegraphics[scale=0.33]{compl_result_grammar_7.png}
  \caption{Grammar 7: $wcw^R$}
  \label{compl_result_grammar_7}
\end{figure}

\begin{figure}[h]
  \includegraphics[scale=0.33]{compl_result_grammar_8.png}
  \caption{Grammar 8: $ww^R$}
  \label{compl_result_grammar_8}
\end{figure}

\begin{figure}[h]
  \includegraphics[scale=0.33]{compl_result_grammar_9.png}
  \caption{Grammar 9: $x2y: |x| \neq |y|$}
  \label{compl_result_grammar_9}
\end{figure}

\begin{figure}[h]
  \includegraphics[scale=0.33]{compl_result_grammar_10.png}
  \caption{Grammar 10: regular expressions over 0 and 1}
  \label{compl_result_grammar_10}
\end{figure}

\begin{figure}[h]
  \includegraphics[scale=0.33]{compl_result_grammar_11.png}
  \caption{Grammar 11: $(ww)^C$}
  \label{compl_result_grammar_11}
\end{figure}

\begin{figure}[h]
  \includegraphics[scale=0.33]{compl_result_grammar_12.png}
  \caption{Grammar 12: $r^nd^nu^nr^n$}
  \label{compl_result_grammar_12}
\end{figure}

\begin{figure}[h]
  \includegraphics[scale=0.33]{compl_result_grammar_13.png}
  \caption{Grammar 13: $a^nc^nb^n$}
  \label{compl_result_grammar_13}
\end{figure}

\begin{figure}[h]
  \includegraphics[scale=0.33]{compl_result_grammar_14.png}
  \caption{Grammar 14: $a^nb^mc^nd^m$}
  \label{compl_result_grammar_14}
\end{figure}

\begin{figure}[h]
  \includegraphics[scale=0.33]{compl_result_grammar_15.png}
  \caption{Grammar 15: $wcw$}
  \label{compl_result_grammar_15}
\end{figure}

\begin{figure}[h]
  \includegraphics[scale=0.33]{compl_result_grammar_16.png}
  \caption{Grammar 16: $a^nb^ma^n$ where $2n \leq m \leq 3n$}
  \label{compl_result_grammar_16}
\end{figure}

\begin{figure}[h]
  \includegraphics[scale=0.33]{compl_result_grammar_17.png}
  \caption{Grammar 17: $w: \#_a(w) = \#_b(w)$ and for prefix $v$ of $w: w: \#_a(w) \geq \#_b(w)$}
  \label{compl_result_grammar_17}
\end{figure}

\begin{figure}[h]
  \includegraphics[scale=0.33]{compl_result_grammar_18.png}
  \caption{Grammar 18: $(l^nr^n)^k$ where $n$ does not increase for subsequent $k$s}
  \label{compl_result_grammar_18}
\end{figure}

\begin{figure}[h]
  \includegraphics[scale=0.33]{compl_result_grammar_19.png}
  \caption{Grammar 19: $a^n c^m b^n$}
  \label{compl_result_grammar_19}
\end{figure}

\begin{figure}[h]
  \includegraphics[scale=0.33]{compl_result_grammar_20.png}
  \caption{Grammar 20: $a^m b^n c^o d^p$ where $m+n = o+p$}
  \label{compl_result_grammar_20}
\end{figure}

These figures show that typically the tree search algorithm has a exponential growth but it starts to grow in quite high numbers. It it usually feasible to get the answer for inputs with lengths of hundres and often even thousands symbols.

Occasionally, thanks to the pruning heuristics the algorithm is able to tell practically immediately that there is no solution. This is the case of grammar 3 \ref{compl_result_grammar_3} and grammar 4 \ref{compl_result_grammar_4} in basic forms, making the complexity of this particular search constant. The grammar 3 has as the first rule, which it has to use to proceed further, $S \rightarrow A \wkpair{abc}{abc}$. If the input string does not end in $abc$ the regular expression check immediately detects that the input cannot be matched, prunes the only branch and the search is finished.

Similarly in the case of grammar 4, any inputs that are longer than seven symbols need to end with symbol $a$ and can be reached only by using $S \rightarrow Q \wkpair{a}{a}$ as the first rule. The regular expression check immediately prunes this branch. The rest of the tree is searched very quicky, because the only other possible starting rule is $S \rightarrow A B C D E F G$, there are not many states that can be reached from it, so this part of the tree is always small.

After conversion of the grammar to CNF, the complexity usually goes up. This is because the transformation adds a lot more rules and thus the state space expands more rapidly, there is also longer paths from the starting non-terminal to the final string (containing only terminals) making the tree deepr. Also, the node precedence heuristics and the pruning have harder time, because many rules contain non-terminals only and most of the heuristics work with terminals. The most extrem case is the grammar 3 where the tree search is very effective for grammar in basic form (as discussed, in case of rejecting inputs the result is immediate), but has very bad effectivness for in CNF. The maximum length of input it can answer within 10 second is about 13--14 symbols.

The worst results for grammar in the basic form are in the case of grammar 17 \ref{compl_result_grammar_17}. For all other grammars, the input length can be in hundrets and in most cases more that a thousand and more. With grammar 17 the tree search can handle only inputs with length of about 25 symbols within 10 seconds.

\section{Comparing different inputs of the same length}
There are some grammars which do not provide any possibility of an interesting or edge case input. Specifically, it is the case of grammars 1--4. Grammar 1 works only with a symbol $a$, grammars 2, 3 and 4 are only interested whether a string ends with specific symbols. But the rest of the grammars, grammars 5--20, all provide some space for trying to come up with a edge case input. Input, which has the key part on the very end or on the very beginning etc. For this test I tryed to think of these edge cases and tested what differences in performance there are. For this test I used a script \textit{var\_inputs\_test.py}. Its complete output can be again recreated by running the script and is attached in the file \textit{output/}

I have selected three of these tests to show here. The tables \ref{tab:input_compl_test_2}, \ref{tab:input_compl_test_4}, \ref{tab:input_compl_test_8}  show the input of the tests from the script \textit{var\_inputs\_test.py}. The first column called Input displays the string that has been used as an input in a compact format $(ns)^*$ where n is a number of occurrences of the symbol or string $s$. For instance, $3a\:2ab$ would translate to string $aaaabab$.

The advantage of State space search is, that sometimes it can recognise right away, that a certain input is not in a given language. It is usually when the string starts with a symbol that cannot be at the beginning. For instance the second input on table \ref{tab:input_compl_test_2} where the language is $a^nb^n$ and the input starts with $b$, which can never be accepted. Similarly case is the second input on table \ref{tab:input_compl_test_8} the language is $r^nd^nu^nr^n$ and it cannot start with anything other than $r$.
On the other hand, sometimes the search has harder time finding a key part of the string which is at the end. This can take longer as is the case in the last input of \ref{tab:input_compl_test_2} (search has to get to the end of the string to see that there are not enough of the $b$ symbols). Some inputs on the table \ref{tab:input_compl_test_8} also suffer for the same reason. This is usually not a problem that could break the practical use completely. The result is typically still reached within a reasonable time. A extreme example is, however, on the table \ref{tab:input_compl_test_4} where the last input is not decided in time, even though other inputs were decided very quickly.

\begin{figure}[h]
\centering
  \caption{Test of various inputs for grammar 6}
  \label{tab:input_compl_test_2}
\begin{tabular}{ |l|l|l|l|l|  }
  \hline
  \multicolumn{5}{|l|}{Test 2} \\
  \hline
  Grammar & \multicolumn{4}{|l|}{$a^n b^n$} \\
  Rules / NTs / Ts & \multicolumn{4}{|l|}{6/3/2} \\
  Timeout & \multicolumn{4}{|l|}{10 seconds} \\
  \hline
  Input & Time & States Q+C & Prunes(SL, TL, WS, RL, RE) & Accepted \\
  \hline
  500a & 0.1881 & 2 + 999 & 4, 0, 998, 0, 0 & FALSE \\
  500b & 0.0 & 1 + 0 & 0, 0, 2, 0, 0 & FALSE \\
  a 500b & 0.3448 & 3 + 502 & 2, 0, 2, 2, 500 & FALSE \\
  500a b & 1.0903 & 3 + 1996 & 2, 0, 1000, 998, 0  & FALSE \\
  \hline
\end{tabular}
\end{figure}


\begin{figure}[h]
\centering
  \caption{Test of various inputs for grammar 8}
  \label{tab:input_compl_test_4}
\begin{tabular}{ |l|l|l|l|l|  }
  \hline
  \multicolumn{5}{|l|}{Test 4} \\
  \hline
  Grammar & \multicolumn{4}{|l|}{$w w^r$} \\
  Rules / NTs / Ts & \multicolumn{4}{|l|}{3/1/2} \\
  Timeout & \multicolumn{4}{|l|}{10 seconds} \\
  \hline
  Input & Time & States Q+C & Prunes(SL, TL, WS, RL, RE) & Accepted \\
  \hline
  2000ab 2000ba a & 0.0003 & 1 + 1 & 0, 0, 3, 0, 2, 0 & FALSE \\
  a 2000ab 2000ba &  0.0003 & 1 + 1 & 0, 0, 2, 0, 3, 0 & FALSE \\
  2000ab a 2000ba & 10.0079 & 1 + 1461 & 0, 0, 2921, 0, 1, 0 & TIMEOUT \\
  \hline
\end{tabular}
\end{figure}

\begin{figure}[h]
\centering
  \caption{Test of various inputs for grammar 12}
  \label{tab:input_compl_test_8}
\begin{tabular}{ |l|l|l|l|l|  }
  \hline
  \multicolumn{5}{|l|}{Test 8} \\
  \hline
  Grammar & \multicolumn{4}{|l|}{$r^n d^n u^n r^n$} \\
  Rules / NTs / Ts & \multicolumn{4}{|l|}{10/5/3} \\
  Timeout & \multicolumn{4}{|l|}{10 seconds} \\
  \hline
  Input & Time & States Q+C & Prunes(SL, TL, WS, RL, RE) & Accepted \\
  \hline
 500r 500d 500u 500r d & 8.9568 & 1500 + 3001 & 0, 0, 3002, 998, 501, 1 & FALSE \\
 d 500r 500d 500u 500r & 0.0001 & 1 + 0 & 0, 0, 2, 0, 0, 0 & FALSE \\
 500r 500d 500u 501r & 8.9805 & 1500 + 3002 & 2, 0, 3000, 1000, 501, 0 & FALSE \\
 500r 500d 501u 500r & 3.0577 & 1000 + 2001 & 0, 0, 3000, 2, 0, 0 & FALSE \\
 500r 501d 500u 500r & 1.0785 & 1000 + 1001 & 0, 0, 2000, 2, 0, 0 & FALSE \\
 501r 500d 500u 500r & 1.0763 & 1001 + 1002 & 0, 0, 2002, 2, 0, 0 & FALSE \\
 r 500d 500u 500r & 0.0003 & 2 + 3 & 0, 0, 4, 2, 0, 0 & FALSE \\
 500r d 500u 500r & 0.191 & 501 + 502 & 0, 0, 1002, 2, 0, 0 & FALSE \\
 500r 500d u 500r & 0.9625 & 1000 + 1003 & 0, 0, 2002, 2, 0, 0 & FALSE \\
 500r 500d 500u r & 3.0452 & 1001 + 2002 & 2, 0, 3000, 2, 0, 0 & FALSE \\

  \hline
\end{tabular}
\end{figure}


\section{Testing the efficiency of WK-CYK}

I have tested the WK-CYK algorithm in a similar manner as the tree search. This time not all grammars can be used due to limitations of WK-CYK. Grammars must be in CNF form and grammars 5, 19 and 20 cannot be used at all, since WK-CYK requires the complementarity relation to be identity, which is not the case of these three grammas. Therefore there are 17 grammars that can be tested. The script \textit{wk\_cyk\_test} runs two tests for each of these grammrs. One with inputs that should be accepted and one with inputs that should be rejected. Again, each test increases the input string length until it the computation lasts more that a limit of 10 seconds. The output of each test is again a table similar to \ref{tab:wk_cyk_res}. The entire output can be remade by running the script again and is attached in file \textit{ouput/wk\_test.txt}.


\begin{figure}[h]
\centering
  \caption{An output of the WK-CYK time complexity test}
  \label{tab:input_compl_test}
\begin{tabular}{ |l|l|l|  }
  \hline
  \multicolumn{3}{|l|}{Test 1} \\
  \hline
  Grammar & \multicolumn{2}{|l|}{$a(aa)^*$} \\
  Rules / NTs / Ts & \multicolumn{2}{|l|}{2/1/1} \\
  Should accept & \multicolumn{2}{|l|}{Yes} \\
  Timeout & \multicolumn{2}{|l|}{7 seconds} \\
  \hline
  Input length & Time & Accepted \\
  \hline
  3 & 0.0 & TRUE \\
  5 & 0.0 & TRUE \\
  7 & 0.0 & TRUE \\
  9 & 0.01 & TRUE \\
  11 & 0.03 &  TRUE \\
  13 & 0.07 &  TRUE \\
  15 & 0.15 &  TRUE \\
  17 & 0.29 &  TRUE \\
  19 & 0.53 &  TRUE \\
  21 & 0.88 &  TRUE \\
  23 & 1.45 &  TRUE \\
  25 & 2.31 &  TRUE \\
  27 & 3.49 &  TRUE \\
  29 & 5.23 & TRUE \\
  31 & 7.55 & TRUE \\
  33 & 10.67 & TRUE \\

  \hline
\end{tabular}
\end{figure}

It turns out that the WK-CYK gives very similar performance in all tests --- for all the grammars and regardless whether the input is accepted or not. The figure \ref{fig:wk_cyk_grammar1} to the right shows a result for the first test which is very similar to all the others.

These results comfirm the claim made by authors of WK-CYK, that the complexity with regards to the input length is $O(n^6)$. The figure \ref{fig:wk_cyk_grammar1} shows the same test with the input lengths raised to the power of six, which can be considered to be a number of operations. The curve is then very close to linear.

When the results of the WK-CYK and state space search are compared, the advantage of state space search is the actual speed in most cases. The results in the previous sections showed that of all the grammars only one (grammar 17) was slower in the basic form when analysed by tree search, then when analysed by WK-CYK. After transformation to CNF two more grammars (grammar 3 and 11) were comparable or slower when analysed by tree search.

Another advantage of state space search is the flexibility regarding the grammar. It does not require to work with grammar in CNF. Also, it does not require the complementarity relation to be identity. Even though it is always possible to transform any WK grammar to the WK-CNF and it is always possible to further transform the grammar to use only the identity as the relation, this can significantly add to the grammar complexity.

The advantage of WK-CYK, on the other hand, is its universality. It has roughly the same speed every time, it does not significantly depend on the grammar (increasing number of rules adds a liitle bit) and it does not matter, if the input is going to be accepted or not. For very complicated grammars, especially with lots of rules or long derivations from the starting symbol to the final string, WK-CYK still might be more practical.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{wk_cyk_test_1.png}
  \caption{WK-CYK test result}
  \label{fig:wk_cyk_test1}
\end{figure}

The limit of ten seconds is reached by WK-CYK when the input has about 33 symbols.

\chapter{Conclusion}
In the work I have presented various existing models for representing Watson-Crick languages, most importantly Watson Crick automata and Watson-Crick grammars. Then I have analysed the WK-CYK algorithm for testing membership of strings in a context-free Watson-Crick grammars. Than I have come up with the algorithm I call state space search or tree search, which is the most important contribution of this thesis.

The state space search is based on standard Breadth-first search algorithm, where the starting non-terminal of the grammar is the root node and every applicable rule creates successors in the tree. State space search then introduces various optimizations, from which the most important are pruning and node precedence heuristics. Pruning uses five different methods of identifying, that a given node cannot produce the desired solution and removes the entire branch. Node precedence heuristics attempt to choose more promising nodes to be analysed first. I have tested twelve such heuristics and chosen the one, that had the best overall results (called NTA+TM2) as the default one.

I have collected and created twenty Watson-Crick context-free grammars to test the WK-CYK and state space search.
I have implemented both WK-CYK and state space search in the Python language and written scripts to test the algorithms with all these grammars and various inputs. The test results showed that for the majority of the grammars, state space search was very efficient and it can quickly decide membership problem of inputs that are hundrets or even thousands of symbols long. Another advantage of state space search is the fact that it can work with any WK-grammar --- it does not have to be in Chomsky normal form. Also the complementarity relation can be arbitrary, it does not have to be identity.

Testing the WK-CYK algorithm showed that its theoretical complexity $O(n^6)$ woth respect to the input length corresponds to the real performance. In practice, it is able to decide membership problem of inputs up to length of 40--50 symbols. That is much less than state space search, but on the other hand, its performance is almost identical for any grammar and for any inputs. It is more efficient in case of some specific or complex grammars, where the performance of state space search struggles.

The state space search is a suitable algorithm to be parallelized. More processes can take modes from the queue and analyse different branches of the tree independently. This would be a natural next step in the further development of state space search.
It is possible to come up with other heuristics for both the pruning and node preference. As for pruning, one option would be to combine the idea of comparing the lower strand to a regular expression in a similar way as it is currently done with the upper strand. Another idea is to calculate how many terminals can be generated at minimum to the lower strand and to the upper strand (currently, it is calculated how many terminals a non-termianal produces to both strands) thus making the constraint of the words stronger.
As for the node precedence heuristics, it may be worth while to use some of the grammars over which state space search is not efficient (in particular grammars 3 and 17) and design or improve node precedence heuristics with respect to these particular cases. Then it would be appropriate to test all these new heuristics and see if they contribute to the overall performance or not.
Another promising improvent could be analysing the input from both sides at the same time. This could help with the cases, when the key part of the input is at or near its end and the State space search may struggle to get there in reasonable time.
