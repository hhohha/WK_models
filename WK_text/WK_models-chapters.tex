\chapter{Introduction}

The ability to read DNA, to understand it or even to modify it, is certainly one of the ways that many people think will define the future. But in order to work with DNA there needs to be a mathematical model that can actually do calculations with such structures and that is prepared to be run on computers. Moreover, working with this model must be efficient enough because genetic code has a huge number of digits.

This works follows the work of M. Zulkufli et al. \cite{WK_GRAMMARS_1}, \cite{WK_GRAMMARS_2}, \cite{WK_CYK} who have studied models for working with Watson-Crick languages and introduced the WK-CYK algorithm, a modification of the CYK algorithm, which works with Watson-Crick context-free grammars and is able to decide the membership problem for these languages. The stated complexity of this algorithm is $\mathcal{O}(n^6)$ with respect to the input length. However, with this complexity the algorithm still does not seem to be useful for practical DNA computations considering how long DNA code is.

Therefore this work introduces the state space search algorithm. While its theoretical complexity is not as good as in case of WK-CYK, it takes a more practical approach. In practice, thanks to various heuristics, it is very often able to decide the membership in languages defined by Watson-Crick context-free grammars of inputs far longer then what WK-CYK can handle.

Chapter \ref{chapter:models} contains an overview of most common models for working with Watson-Crick languages. Chapter \ref{chapter:WK_CYK} discusses ways of deciding membership problem of those languages with the focus on the WK-CYK algorithm. Chapter \ref{chapter:parse_tree} introduces the state space search algorithm and the heuristics and optimizations that make it more efficient. Chapter \ref{chapter:implementation} focuses on the implementation of the state space search and is probably going to be useful to someone who wants to delve into the code and use it or further build on it. Finally, chapter \ref{chapter:testing} contains twenty grammars that were used for testing both state space search and WK-CYK algorithms in practice and presents results of these tests.

A integral component of this thesis is an implementation of the state space search algorithm, the WK-CYK algorithm and a number tests used to analyze the state and space complexities and to compare the algorithms.

\chapter{Watson-Crick models} \label{chapter:models}
A number of models working with double stranded sequences has been proposed. The purpose of this chapter is to present a motivation for using them and to summarize these models and some of their key attributes that will be important in later chapters.

\section{DNA as an inspiration for Watson-Crick languages}
The study of Watson-Crick models is motivated by DNA (deoxyribonucleic acid) computing. In order to study the DNA mathematically, i.e. to perform mathematical operations over it, it is necessary to work with a suitable abstraction --- a model which captures its key characteristics. Specifically, there are two characteristics that the Watson-Crick models capture --- the fact that the DNA is a double stranded chain and the Watson-Crick relation between DNA nucleotides.

The two fundamental models that are used to define a language in computer theory are grammars and automata. Several versions of both have been proposed but all of them work with these two characteristics in a very similar manner.

DNA consists of two chains of nucleotides, one of which is marked as $5'$ end and the other $3'$ end. The chains are connected by covalent bonds and together form a double helix (figure \ref{fig:dna}). These two chains are represented in the Watson-Crick automata by two reading heads which read two inputs independently but are controlled by the same states. Similarly, Watson-Crick grammars produce by their rules not just a chain of symbols, but two chains.

\begin{figure}[ht]
  \includegraphics[height=8cm]{DNA.png}
  \centering
  \label{fig:dna}
  \caption{The DNA double helix}
\end{figure}

Each nucleotide contains one of the four nucleobases - cytosine (C), guanine (G), adenine (A) and thymine (T). These bases are always connected with their counterpart: cytosine with guanine and adenine with thymine. That means that whenever one of the four appears in a chain, its counterpart appears in the other chain in the corresponding place being bound together by the covalent bond. The Watson-Crick models therefore introduce a complementarity relation --- a relation between symbols which must be kept in the whole input for it to be valid. Typically, this relation is symmetric ($a R b \Leftrightarrow b R a$) and covers the whole alphabet (every symbol must have at least one counterpart). Often every symbol has exactly one counterpart, just like in case of DNA. The relation is usually defined as an identity (i.e. each symbol is related to itself and only to itself) which is still somewhat similar to the DNA pairing.

\section{Watson-Crick automata} \label{section:WKA}
Watson-Crick automata have first been proposed in \cite{WK_FIN_AUT} as an enhancement of standard Finite Automata. Watson-Crick finite automaton is a 6-tuple $M = (V, \rho, Q, q_0, F, P)$ with the following meaning.
\begin{itemize}
  \item{$V$ -- finite input alphabet}
  \item{$\rho \subseteq V \times V$ -- complementarity relation}
  \item{$Q$ -- finite set of states}
  \item{$q_0 \in Q$ -- starting symbol}
  \item{$F \subseteq Q$ -- set of final states}
  \item{$P$ -- finite set of transition rules in a form $q \wkpair{w_1}{w_2} \rightarrow q'$ where $q, q' \in Q, w_1, w_2 \in V^*$}
\end{itemize}

Compared to Finite automata, Watson-Crick automata have different form of transition rules which read two strings at the same time. These represent the two independent reading heads --- one reading the upper strand ($w_1$) and the other reading the lower strand ($w_2$). They also add the complementarity relation which is usually required to be symmetric. The symbols in the upper and lower strands with the same indexes need to adhere to it.

$\wkpair{w_1}{w_2}$ denotes simply a pair $(w_1, w_2)$.
A Watson-Crick domain is a set $WK_{\rho}(V)$ which denotes all valid double strands associated with a given $V$ and $\rho$. Formally:
$$WK_{\rho}(V) = \wkdomain{V}{V}_{\rho}^{*} \:\:\: \textnormal{where} \:\:\: \wkdomain{V}{V}_{\rho} = \Big\{\wkdomain{a}{b} | a, b \in V, (a, b) \in \rho \Big\}$$
This implies that the both strands in a WK domain must have the same length.

A configuration of a Watson-Crick automaton is a pair $(q, \wkpair{w_1}{w_2})$ where $q \in Q$ is a current state and $w_1, w_2 \in V^*$ are the parts of the upper and lower strands yet to be read.

If $q\wkpair{u_1}{u_2} \rightarrow q' \in P$ and $\wkpair{u_1 v_1}{u_2 v_2} \in \wkpair{V^*}{V^*}$ then $q \wkpair{u_1 v_1}{u_2 v_2} \Rightarrow q' \wkpair{v_1}{v_2}$ is a transition of the Watson-Crick automaton. $\Rightarrow^*$ denotes the transitive and reflexive closure of the relation $\Rightarrow$.

A Watson-Crick automaton accepts the language $L(M)$:

$$L(M) = \Big\{w_1 \in V^* | q_0 \wkdomain{w_1}{w_2} \Rightarrow^* f \wkpair{\lambda}{\lambda} \textnormal{ where } f \in F, w_2 \in V^*, \wkdomain{w_1}{w_2} \in WK_{\rho}(V)\Big\}$$

$\lambda$ denotes a string of length zero (an empty string). This means that only the upper strand is accepted by this automaton to the language $L$. The lower strand has just an auxiliary purpose.

\section{Special versions of Watson-Crick automata}
Four special versions of Watson-Crick automata (WKA) are often used (\cite{DETERM_WKA}, \cite{STATE_COMPL}). These are:
\begin{itemize}
  \item{stateless WKA --- the WKA has only one state: $Q = F = {q_0}$}
  \item{all final WKA --- all the states are final: $Q = F$}
  \item{simple WKA --- each rule reads only one head: $(q \wkpair{w_1}{w_2} \rightarrow q' \in P) \Rightarrow (w_1 = \lambda \:\vee \:w_2 = \lambda)$}
  \item{1-limited WKA --- reads only one symbol at a time: $(q\wkpair{w_1}{w_2} \rightarrow q' \in P) \Rightarrow |w_1 w_2| = 1$}
\end{itemize}

Three of these four special types of WKAs have the same power as the actual WKA, namely all final WKA, simple WKA and 1-limited WKA (stateless WKA is weaker). Therefore one possible approach to decide membership would be to limit the decision algorithm to one of these three types without any loss in expressing power.

There are three different variants of deterministic WKA proposed by in \cite{DETERM_WKA}. These are:
\begin{itemize}
  \item{Weakly deterministic WKA: WKA where in each reachable configuration, there is at most one possible continuation.}
  \item{Deterministic WKA: For any two rules which lead from the same state, either their upper strands or their lower strands must not be prefix comparable (one is not the prefix of the other). Formally: $(q \wkpair{u}{v} \rightarrow q_1 \in P \wedge q \wkpair{u'}{v'} \rightarrow q_2 \in P) \Rightarrow u \nsim_p u' \vee v \nsim_p v'$ where $\sim_p$ is the relation of prefix comparability.}
  \item{Strongly deterministic WKA: this is a deterministic WKA whose complementarity relation is identity.}
\end{itemize}

It is not specified how to actually achieve weak determinism. In fact, \cite{DETERM_WKA} shows that this property is undecidable. Intuitively, for a WKA to be weakly deterministic but not deterministic, there must be at least two rules which could both be used in certain configuration (otherwise it would be deterministic). But such a configuration must not be reachable (otherwise it would not be weakly deterministic). The configuration may be unreachable trivially --- by such rules using an unreachable state or a symbols that have no related symbols in the complementarity relation. But a configuration may be unreachable non-trivially, if it is possible to tell how many symbols will be read from each strand before reaching certain state.

Both weakly deterministic and deterministic WKA are in reality not deterministic (in an intuitive sense). Their determinism relies on the fact that the configuration is known and for the configuration to be known, the entire input (meaning both strands of the input) needs to be specified. But that is often not a way how WKA are used, since WKA decides the membership in a language for the upper strand only. That means that a compatible strand has to be found in the process of running the WKA. Theoretically, it is possible to approach this problem by first generating all possible lower strands for the given upper strand based solely on the complementarity relation and afterwards use all these pairs as inputs for the WKA. In such a case, the weakly deterministic and deterministic automata would be truly deterministic, however this is clearly not feasible for non-trivial complementarity relations. Therefore, the strongly deterministic WKA is the only one witch is truly deterministic under all circumstances because the identity relation requirement leaves no space for these types of non-determinism.

\section{Watson-Crick grammars}
The first kind of Watson-Crick grammars that has been introduced were Watson-Crick regular grammars \cite{REG_GRAMMAR}. The key features are shared with Watson-Crick automata. Specifically, it it the complementarity relation $\rho$ and the double stranded strings that the grammar produces.
The WK regular grammars have been used as a basis for Watson-Crick linear grammars and Watson-Crick context-free grammars introduced in \cite{WK_GRAMMARS_1}. Since a WK linear grammar is a generalization of WK regular grammar and WK context-free grammar is a generalization of WK linear grammar, it makes sense to start with the definition of the context-free version and then specify the constraints of linear and regular versions.

A \textbf{Watson-Crick context-free grammar} is $G = (N, T, \rho, P, S)$ where $N$ is a finite set of non-terminals, $T$ is a finite set of terminals and $N \cup T = \emptyset$, $S \in N$ is a starting non-terminal, $\rho \subset T \times T$ is a symmetric complementarity relation, and $P$ is a finite set of rules that have the form $A \rightarrow \alpha$ where $A \in N \: \wedge \alpha \in (N \cup \wkpair{T^*}{T^*})^*$.

The derivation of the grammar $G$ starts with the starting symbol $S$. $x \in (N \cup  \wkpair{T^*}{T^*})^*$ directly derives $y \in (N \cup \wkpair{T^*}{T^*})^*$, denoted by $x \Rightarrow y$, if and only if:

$$x = \beta A \gamma \: \wedge \: y = \beta \alpha \gamma$$

where $A \in N \: \wedge \: \alpha, \beta, \gamma \in (N \cup \wkpair{T^*}{T^*})^* \: \wedge \: A \rightarrow \alpha \in P$.

The language generated by the grammar $G$ is:

$$L(G) = \big\{ w| S \Rightarrow^* \wkdomain{w_1}{w_2} \in \wkdomain{T^*}{T^*}_{\rho} \big\}$$

where $\Rightarrow^*$ is a reflexive and transitive closure of $\Rightarrow$.

A \textbf{Watson-Crick linear grammar} is a special version of a Watson-Crick context-free grammar where all the rules in the set of rules $P$ are in one of the following forms:

$$A \rightarrow \wkpair{T^*}{T^*} B \wkpair{T^*}{T^*}, \:\:\: A \rightarrow \wkpair{T^*}{T^*}$$

where $A, B \in N$

A \textbf{Watson-Crick regular grammar} is also a special version of a Watson-Crick context-free grammar (and of a Watson-Crick linear grammar) where all the rules in the set of rules $P$ are in one of the following forms:

$$A \rightarrow \wkpair{T^*}{T^*} B, \:\:\: A \rightarrow \wkpair{T^*}{T^*}$$

where $A, B \in N$

A further specialization of Watson-Crick regular grammar has been defined in \cite{REG_GRAMMAR} called \textbf{1-limited Watson-Crick regular grammar} (N1WK grammar). All rules of such a grammar must contain exactly one terminal symbol on the left-hand side and the starting non-terminal must be the only non-terminal in the grammar. In other words, the form of each rule must be one of the following:

$$S \rightarrow \wkpair{a}{\lambda} S, \:\:\: S \rightarrow \wkpair{\lambda}{a} S, \:\:\: S \rightarrow \wkpair{a}{\lambda}, \:\:\: S \rightarrow \wkpair{\lambda}{a}$$

\section{Some other models for Watson-Crick languages}
This section mentions some other, perhaps slightly less often used, models for Watson-Crick languages --- Watson-Crick pushdown automata, Watson-Crick context-free systems and parallel communicating Watson-Crick automata.

\subsection{Watson-Crick pushdown automata}
The Watson-Crick Pushdown automata (WCPDA) have been introduced in \cite{WK_PUSHDOWN_AUT}. It is basically a two-head pushdown automaton with the complementarity relation added on top. Formally a WCPDA $P$ is a 10-tuple $P = (Q, \#, \$, V, \Gamma, \delta, q_0, Z_0, F, \rho)$ with most symbols having the same standard meaning as in a conventional pushdown automaton --- $Q$ is a finite set of states, $V$ is an input alphabet, $\Gamma$ is a stack alphabet, $q_0 \in Q$ is a starting state, $Z_0 \in \Gamma$ is a starting stack symbol and $F \subseteq Q$ is the set of final states. Symbols $\#, \$ \notin V$ are left and right input markers on the two strands. $\rho$ is the complementarity relation similar to standard WKA.

$\delta$ is a set of rules in the following form: $(q, \wkpair{w_1}{w_2}, x) \rightarrow (q', \gamma) \textnormal{ where } q, q' \in Q, w_1, w_2 \in V^* \cup \#V^* \cup V^*\$ \cup \#V^*\$, x \in \Gamma, \gamma \in \Gamma^*$. It means that the automaton can transition from state $q$ reading the input $w_1$ with the first head and $w_2$ with the second head and go to state $q'$ while removing the top symbol from the stack and putting a string (i.e. 0-n symbols) of the stack symbols onto the stack. The two strands on the input are enclosed in the beginning symbol $\#$ and the closing symbol $\$$, therefore the symbol $\#$ may appear in the beginning of $w_1$ or $w_2$ and similarly the closing symbol $\$$ at the end.

A configuration of $P$ is a triple $(q, \wkpair{x}{y}, \gamma)$ where $q \in Q$ is a state, $\wkpair{x}{y}$ is the remaining input to be read where $x, y \in \#V^*\$ \cup V^*\$ \cup \lambda$ and $\gamma \in \Gamma^*$ is the content of the stack. The initial configuration of the automaton is $(q_0, \wkpair{\#w_1\$}{\#w_2\$}, Z_0)$ where $\wkdomain{w_1}{w_2} \in WK_{\rho}(V)$.

A transition $\vdash$ of $P$ is a relation between configurations defined as follows:

$$\big(q, \wkpair{a_1w_1}{a_2w_2}, X\beta\big) \vdash \big(p, \wkpair{w_1}{w_2}, \alpha\beta\big) \iff \big(q, \wkpair{a_1}{a_2}, X\big) \rightarrow (p, \alpha) \in \delta$$

$\vdash^*$ is a transitive relation of $\vdash$ denoting zero or more transitions.

The language accepted by $P$ is:
$$L(P) = \Big\{w_1 \in V^*| w_2 \in V^* \wedge \wkdomain{w_1}{w_2} \in WK_{\rho}(V)  \wedge \big(q_0, \wkpair{\#w_1\$}{\#w_2\$}, Z_0\big) \vdash^* \big(q, \wkpair{\lambda}{\lambda}, \alpha \big) \wedge q \in F \wedge \alpha \in \Gamma^* \Big\}$$

meaning that there exists a sequence of transitions ($\vdash^*$) from the initial configuration to a final configuration where the remaining input to be read is $\wkpair{\lambda}{\lambda}$, the content of the stack is arbitrary and the state is in the set of final states $F$. This means that WK pushdown automata accept by final states.

\cite {WK_PUSHDOWN_AUT} also defines two special versions of WK pushdown automata --- deterministic Watson-Crick pushdown automata (DWKPDA) and strongly deterministic Watson-Crick pushdown automata (SDWKPDA) which are inspired by the deterministic and strongly deterministic WKA.

Watson-Crick pushdown automaton is deterministic if any two rules, which start in the same state, read inputs which are prefix incomparable in the upper or lower part. Formally:

$$\big(q, \wkpair{u}{v}, X\big) \rightarrow (q', \gamma) \in \delta \wedge \big(q, \wkpair{u'}{v'}, X\big) \rightarrow (q'', \gamma') \in \delta \Rightarrow u \nsim_p u' \vee v \nsim_p v'$$

Watson-Crick pushdown automaton is strongly deterministic if it is deterministic and its complementarity relation is identity.


\subsection{Watson-Crick context-free systems}
A Watson-Crick context-free systems (WKCFS) have been defined in \cite{WKCF_SYSTEMS} A WKCFS is: $S = (V, \Sigma, \rho, A, P)$ where $V$ is a finite alphabet, $\Sigma \subset V$, $\rho \subseteq \Sigma \times (V \setminus \Sigma)$ is a complementarity relation where if $(a, \overline{a}) \in \rho$ then $\overline{a} \in V - \Sigma$ is unique for $a \in \Sigma$. $A$ is a finite set of axioms in form $\wkdomain{w}{s}$ where $w \in \Sigma^*$, $s \in (V - \Sigma)^*$ and $(w, s) \in \rho$. $P$ is a finite set of rules in one of the forms:

$$\wkpair{a}{\overline{a}} \rightarrow \wkpair{x}{y}, \:\: \wkpair{a}{\lambda} \rightarrow \wkpair{x}{y},\:\: \wkpair{\lambda}{a} \rightarrow \wkpair{x}{y}$$

where $a \in \Sigma$, $(a, \overline{a}) \in \rho$, $x \in \Sigma^*$ and $y \in (V - \Sigma)^*$

A derivation $\Rightarrow$ in $S$ is a relation between $\wkpair{u_1}{v_1}$ and $\wkpair{u_2}{v_2}$ defined as follows:

$$\wkpair{u_1}{v_1} \Rightarrow \wkpair{u_2}{v_2}$$ if one of the following conditions is met:

\begin{enumerate}
  \item{$\wkpair{u_1}{v_1} = \wkpair{x_1}{y_1} \wkpair{a}{\overline{a}} \wkpair{x_2}{y_2}$

  $\wkpair{u_2}{v_2} = \wkpair{x_1xx_2}{y_1yy_2}$ if $\wkpair{a}{\overline{a}} \rightarrow \wkpair{x}{y} \in P$
  }

  \item{$\wkpair{u_1}{v_1} = \wkpair{x_1}{y_1} \wkpair{a}{\lambda} \wkpair{x_2}{y_2}$

  $\wkpair{u_2}{v_2} = \wkpair{x_1xx_2}{y_1yy_2}$ if $\wkpair{a}{\lambda} \rightarrow \wkpair{x}{y} \in P$
  }

  \item{$\wkpair{u_1}{v_1} = \wkpair{x_1}{y_1} \wkpair{\lambda}{a} \wkpair{x_2}{y_2}$

  $\wkpair{u_2}{v_2} = \wkpair{x_1xx_2}{y_1yy_2}$ if $\wkpair{\lambda}{a} \rightarrow \wkpair{x}{y} \in P$
  }

\end{enumerate}

where $a \in \Sigma$, $(a, \overline{a}) \in \rho$, $x_1, x_2 \in \Sigma^*$, $y_1, y_2 \in (V - \Sigma)^*$

The language of $S$ is: $L(S) = \big\{x| \wkdomain{w}{s} \Rightarrow^* \wkdomain{x}{y} \big\}$ where $\wkdomain{w}{s} \in A$, $x \in \Sigma^*$ and $y \in (V - \Sigma)^*$

\subsection{Parallel communicating Watson-Crick automata systems}
Parallel communicating Watson-Crick automata systems (PCWK) have been defined in \cite{PARALLEL}. PCWK($n$) is a PCWK of degree $n$ which is a $(n + 3)$-tuple: $A = (V, \rho, A_1, A_2, ..., A_n, K)$

where $V$ is an input alphabet, $\rho$ is a complementarity relation, $A_i = (V, \rho, Q_i, q_i, F_i, \delta_i)$ for $1 \leq i \leq n$ is a Watson-Crick automaton. $K = {K_1, K_2, ..., K_n} \subseteq \bigcup_{i=1}^{n} Q_i$ is a set of query states.

The automata $A_{1-n}$ are the components of the system $A$. A configuration  of a PCWK is a $2n$-tuple $\big(s_1, \wkpair{u_1}{v_1}, s_2, \wkpair{u_2}{v_2}, ..., s_n, \wkpair{u_n}{v_n}\big)$
where $s_i$ is a state of component $A_i$ and $\wkpair{u_i}{v_i}$ is part of the input word that has not yet been read by $A_i$ for $1 \leq i \leq n$.

A transition is a relation $\vdash$ between two configurations and is defined as follows:

$$\big(s_1, \wkpair{u_1}{v_1}, s_2, \wkpair{u_2}{v_2}, ..., s_n, \wkpair{u_n}{v_n}\big) \vdash \big(r_1, \wkpair{u'_1}{v'_1}, r_2, \wkpair{u'_2}{v'_2}, ..., r_n, \wkpair{u'_n}{v'_n}\big)$$ if one of the following conditions is met:

\begin{enumerate}
  \item{$$K \cap \{s_1, s_2, ..., s_n\} = \emptyset \: \wedge \: \wkpair{u_i}{v_i} = \wkpair{x_i}{y_i}\wkpair{u'_i}{v'_i} \: \wedge \: r_i \in \delta_i\big(s_i, \wkpair{x_i}{y_i}\big)\:\: \textnormal{for} \:\: 1 \leq i \leq n$$}

  \item{for all $1 \leq i \leq n$ such that $s_i = K_{j_i} \wedge s_{j_i} \notin K$ there is $r_i = s_{j_i}$ and for all other $i \leq j \leq n$ there is $r_l = s_l$

  $\wkpair{u'_i}{v'_i} = \wkpair{u_i}{v_i}$ for all $1 \leq i \leq n$.}
\end{enumerate}

$\vdash^*$ denotes the reflexive and transitive closure of $\vdash$ and the language recognized by PCWK $A$ is:

$L(A) = \Big\{w_1 \in V^*| \big(q_1, \wkdomain{w_1}{w_2}, q_2, \wkdomain{w_1}{w_2}, ..., q_n, \wkdomain{w_1}{w_2}\big) \vdash^* \big(s_1, \wkdomain{\lambda}{\lambda}, s_2, \wkdomain{\lambda}{\lambda}, ..., s_n, \wkdomain{\lambda}{\lambda}\big), s_i \in F_i \:\:\: \textnormal{for} \:\:\: 1 \leq i \leq n\Big\}$

\section{Expressing power of Watson-Crick models} \label{section:exp_power}

The comparison of expressing power of WK language families in the context of Chomsky hierarchy has been studied in \cite{WK_GRAMMARS_1} and \cite{WK_GRAMMARS_2}. The main result is shown at the figure \ref{fig:expr-power}. The Chomsky hierarchy is represented on the right (REG --- regular languages, LIN --- linear languages, CF --- context-free languages, CS --- context sensitive languages, RE --- recursively enumerable languages) while the Watson-Crick languages are on the left. WKREG are languages defined by a non-deterministic Watson-Crick automata or a Watson-Crick regular grammars (\cite{REG_GRAMMAR} shows that these are equivalent). WKLIN are languages defined by WK linear grammars and WKCF are languages defined by WK context-free grammars (it has not been shown, yet, that WK pushdown automata have the same power). The full arrows denote proper inclusion, dotted arrows denote inclusion and dotted lines denote incomparability.

\begin{figure}[h!]
  \includegraphics[height=5.5cm]{exp_pow.png}
  \centering
  \label{fig:expr-power}
  \caption{Comparison of WK language families in the context of Chomsky hierarchy}
\end{figure}

It has been shown in \cite{COMPL_REL} that the type of complementary relation which is used does not increase the expressing power of the Watson-Crick automata and grammars. Also \cite{SURVEY} provides an algorithm how to transform any WK automaton to an equivalent WK automaton with the relation being identity. Therefore, many models and algorithms limit themselves to working with identity complementarity relation.

\chapter{Existing ways of testing membership in Watson-Crick languages} \label{chapter:WK_CYK}

This chapter focuses mainly on the WK-CYK algorithm which is practically the only algorithm explicitly designed to decide a membership in Watson-Crick languages defined by a WK context-free grammars. Another way could be using WK automata, which is discussed in the latter part.

\section{WK-CYK algorithm}
The WK-CYK algorithm has been introduced in \cite{WK_CYK} and it is an enhancement of the CYK algorithm modified for WK context-free languages. To understand it, it is good to be familiar with the way the original CYK algorithm works.

\subsection{The CYK algorithm}
The CYK algorithm is named after J. Cocke, D. Younger and T. Kasami \cite{CYK1}, \cite{CYK2} \cite{CYK3}. It is used to decide the membership in a language defined by a context-free grammar, which must be in the Chomsky normal form (CNF).

On the input there is a string and a grammar and the algorithm decides whether the string belongs to the language defined by the grammar (accepts or rejects the string). There are two kinds of rules in a grammar in the CNF (disregarding the $S \rightarrow \epsilon$ rule which is used only to include empty string in the language): $A \rightarrow a$ and $A \rightarrow BC$ where $A, B, C$ are non-terminals and $a$ is a terminal.

In the first stage, it analyzes the first kind of rules --- each of the symbols from the input string has to be generated by a rule or several rules of this form. Thus, it gets a set of candidate non-terminals for each symbol.

In the next stage it uses the second kind of rules. Every non-terminal (except the starting one) has to be generated by such a rule.
The algorithm is looking for rules which can generate the candidate non-terminals which have been found in the previous stage. All possible combinations need to be considered, for instance the sequence of non-terminals $ABC$ may be generated by rules $X \rightarrow AB$ and $Y \rightarrow XC$ or by rules $X \rightarrow BC$ and $Y \rightarrow AX$. In this way, the algorithm needs to find all possible ways to generate words of increasing length (all parse trees). Finally, it needs to find a non-terminal that can generate the whole word and, at the same time, it must be the starting non-terminal in the given grammar. If it succeeds, the word given on the input is in the language, otherwise it is not.

The complexity of the CYK algorithm is $O(n^3 \times R)$ where $n$ is the input string length and $R$ is the number of rules in the grammar.

\subsection{Watson-Crick Chomsky normal form}
Just like the CYK algorithm works with grammars in the Chomsky normal form, the WK-CYK algorithm requires grammars to be in the Watson-Crick Chomsky normal form. The Watson-Crick Chomsky normal form (WK-CNF) is a modification of CNF for Watson-Crick context-free grammars. A grammar in the WK-CNF has only rules of one of the following forms:

\begin{itemize}
  \item{$A \rightarrow \wkpair{a}{\lambda}$}
  \item{$A \rightarrow \wkpair{\lambda}{a}$}
  \item{$A \rightarrow B C$}
  \item{$S \rightarrow \wkpair{\lambda}{\lambda}$ (this rule is used only to include an empty word in the language)}
\end{itemize}

where $A$, $B$ and $C$ are non-terminals, $S$ is the starting non-terminal and $a$ is a terminal of the grammar. It is possible to transform any WK context-free grammar to the WK-CNF. The steps are mostly analogous to the transformation of the standard context-free grammar to the CNF. This process includes:

\begin{enumerate}
  \item{removing $\lambda$-rules (rules of the form $A \rightarrow \wkpair{\lambda}{\lambda}$)}
  \item{removing unit rules (rules of the form $A \rightarrow B$)}
  \item{removing useless rules and symbols (symbols that are unreachable from the starting symbol or cannot lead to a terminal string and rules which use such symbols)}
  \item{replacing every terminal on the left-hand side of each rule (except the rules already in the right form) with a new non-terminal and adding a new corresponding rule}
  \item{breaking down the rules producing non-terminals, so that they produce only two at a time}
\end{enumerate}

The procedure of the transformation is described formally in \cite{WK_CYK}.


\subsection{Order of generating terminals in WK grammar}

A complication compared to the CYK algorithm that WK-CYK has to deal with, is the ambiguity in the order of generating terminals. In case of a standard context-free grammar in the CNF, the order of non-terminals that generate a word, for instance $abcd$, is clear --- if the rules are $A \rightarrow a$, $B \rightarrow b$, $C \rightarrow c$ and $D \rightarrow d$, the non-terminal word that generates the terminal string $abcd$ must be $ABCD$. The order cannot change.

But in case of WK grammars, the order is not clear. For the terminal string $\wkpair{ab}{cd}$, the only given order of generation is $a$ before $b$ and $c$ before $d$, anything else is uncertain. If the rules are $A \rightarrow \wkpair{a}{\lambda}$, $B \rightarrow \wkpair{b}{\lambda}$, $C \rightarrow \wkpair{\lambda}{c}$ and $D \rightarrow \wkpair{\lambda}{d}$, that terminal word can be produced by six different orderings of the non-terminals:
$ABCD$, $ACBD$, $ACDB$, $CABD$, $CADB$ and $CDAB$.

\subsection{Description of the WK-CYK algorithm}
The WK-CYK algorithm taken from \cite{WK_CYK} is on the figures \ref{code:wk_cyk_main} and \ref{code:wk_cyk_compute_sets}.


\begin{lstlisting}[caption={Procedure SetsConstruction of WK-CYK}, label={code:wk_cyk_main}, escapeinside={(*}{*)},numbers=left,
numberstyle=\small, numbersep=8pt,frame = single, framexleftmargin=15pt]
procedure SetsConstruction:
Input: string [w/w] = [(*$x_{11}x_{12}...x_{1n} / x_{21}x_{22}...x_{2n}$*)]

for (*1 $\leq i \leq n$*) do
    (*$X_{i:i,0:0} = \{A: A \rightarrow (x_{1i}/\lambda)\}$*)
    (*$X_{0:0,i:i} = \{A: A \rightarrow (\lambda/x_{2i})\}$*)

for (*$2 \leq y \leq 2n $*) do
    for (*$0 \leq \beta \leq n$*) fo
        (*$\alpha = y - \beta$*)
        if (*$\alpha = 0$*) then
            i = j = 0
            for (*$1 \leq k \leq n - y + 1$*) do
                l = k + y - 1
                ComputeSet (*$X_{i:j,k:l}$*)
        else if (*$\beta = 0$*) then
            k = l = 0
            for (*$1 \leq i \leq n - y + 1$*) do
                j = i + y - 1
                ComputeSet (*$X_{i:j,k:l}$*)
        else
            for (*$1 \leq i \leq n - \alpha + 1$*) do
                for (*$1 \leq k \leq n - \beta + 1$*) do
                    j = i + (*$\alpha$*) - 1
                    l = k + (*$\beta$*) - 1
                    ComputeSet (*$X_{i:j,k:l}$*)
if (*$S \in X_{1:n,1:n}$*) then
    w (*$\in$*) L(G)
else
    w (*$\notin$*) L(G)
\end{lstlisting}

\begin{lstlisting}[caption={Procedure ComputeSet of WK-CYK}, label={code:wk_cyk_compute_sets}, escapeinside={(*}{*)},
numbers=left,numberstyle=\small, numbersep=8pt,frame = single, framexleftmargin=15pt]
procedure ComputeSet:

if i = j = 0 then
    (*$ X_{0:0,k:l} = \big\{\bigcup_{t \in [k, l-1]} X_{0:0,k:t} X_{0:0,t+1:l}\big\}$*)
else if k = l = 0 then
    (*$ X_{i:j,0:0} = \big\{\bigcup_{s \in [i, j-1]} X_{i:s,0:0} X_{s+1:j,0:0}\big\}$*)
else
    (*$ X_{i:j,k:l} = \big\{X_{i:j,0:0}X_{0:0,k:l} \cup X_{0:0,k:l}X_{i:j,0:0}\big\} \cup$*)
        (*$\bigcup_{s \in [i, j-1], t \in [k, l-1]} \big\{X_{i:s,k:t}X_{s+1:j,t+1:l}\big\} \cup$*)
        (*$\bigcup_{s \in [i, j-1]} \big\{X_{i:s,k:l}X_{s+1:j,0:0} \cup X_{i:s,0:0}X_{s+1:j,k:l}\big\} \cup$*)
        (*$\bigcup_{t \in [k, l-1]} \big\{X_{i:j,k:t}X_{0:0,t+1:l} \cup X_{0:0,k:t}X_{i:j,t+1:l}\big\}$*)
\end{lstlisting}


WK-CYK algorithm expects a grammar in the WK-CNF and a double stranded string on the input. Since one of the algorithm's requirements is that the complementarity relation must be identity, the upper and lower strands are always the same.

WK-CYK uses sets marked as $X_{a:b,c:d}$. These are sets of non-terminals that can generate a segment of the input double stranded string specified by the indexes $a$, $b$, $c$ and $d$. $a$ and $b$ are indexes of terminals in the upper strand and specify an interval (the indexing starts with the index 1 and the edge indexes are included). The lower strand interval is specified by indexes $c$ and $d$. If a pair of indexes is 0, no symbols from the corresponding strand are included. For instance, for the segment $\wkpair{abcd}{abcd}$, $X_{2:2,0:0}$ would contain a set of non-terminals that generate $\wkpair{b}{\lambda}$, $X_{2:4,1:3}$ non-terminals that generate $\wkpair{bcd}{abc}$.

\subsubsection{The main procedure of WK-CYK}
In the first step (lines 4--6 of figure \ref{code:wk_cyk_main}) WK-CYK finds sets $X_{i:i,0:0}$ and $X_{0:0,i:i}$ for $0 < i \leq |n|$ ($n$ is the length of the input). These are non-terminals that directly generate single terminals. Then, it searches for ways to generate segments of the input of increasing lengths, beginning with length of 2 and up to the length of $2n$. It is because in the input of length $n$ there are actually $2n$ of terminals --- $n$ in the upper and $n$ in the lower strand. For each length of the segment it takes all possible combinations of number of symbols from the upper and the lower strands. For instance, if the length of the current segment is 3, that can include 3 terminals from the upper strand and 0 from the lower or 2 and 1, 1 and 2, 0 and 3.

For each of these segments, it calls the procedure \textit{ComputeSet} which finds all non-terminals, that could generate the given segment. When WK-CYK uses this procedure to compute set $X$ of a segment of length $n$, it is necessary to have already computed sets $X$ for all segments of length $m < n$. Therefore it proceeds from the length 1 upward.

Let us consider an example with input $\wkpair{abcd}{abcd}$. The first step looks for way of generating the individual terminals, in other words non-terminals that generate $\wkpair{a}{\lambda}$, $\wkpair{\lambda}{a}$, $\wkpair{b}{\lambda}$ etc. Then it looks for non-terminals that could generate two terminals, meaning either two terminals from the upper strand, two terminals from the lower strand or one from each. In each of these cases, all possible combinations need to be considered. If the two terminals are from the upper strand, the combinations are either $\wkpair{ab}{\lambda}$, $\wkpair{bc}{\lambda}$ or $\wkpair{cd}{\lambda}$ (or using the $X$ sets: $X_{1:2,0:0}$, $X_{2:3,0:0}$ or $X_{3:4,0:0}$). It the two terminals are one from each strand, there are 16 combinations $\wkpair{x}{y}$ where $x, y \in \{a, b, c, d\}$ ($X_{1:1,1:1}$, $X_{2:2,1:1}$, $X_{1:1,2:2}$ etc.). And for terminals from the lower strand, the combinations are $\wkpair{\lambda}{ab}$, $\wkpair{\lambda}{bc}$ or $\wkpair{\lambda}{cd}$ ($X_{0:0,1:2}$, $X_{0:0,2:3}$ and $X_{0:0,3:4}$).

When the segment of length $2n$ has been computed, WK-CYK is finished. It has succeeded if the starting symbol $S$ can generate the whole input, in other words: if $S \in X_{1:n,1:n}$.

\subsubsection{The ComputeSet procedure}
The \textit{ComputeSet} procedure has as a parameter a segment of input specified by the four indexes. It searches all pairs of sets $X$ which could together produce the given segment. If the segment consists of symbols from one strand only ($X_{i:j,0:0}$ or $X_{0:0,i:j}$), the situation is simpler --- it needs to consider the pairs of sets $X$ that produce the segment split in any two parts. If the segment contains symbols from both strands, there are more ways to split it:

\begin{itemize}
  \item{The first set could produce the entire upper strand and the second set could produce the entire lower strand or the other way around (the order matters)}
  \item{The first set could produce the entire upper (or lower) strand and any part the lower (upper), the second one would produce the rest of the divided strand. Again, all possible divisions of the divided strand need to be considered.}
  \item{Both sets could produce parts of both strands. Again, all possible combinations of divisions of both strands need to be considered.}
\end{itemize}

When all combinations of two $X$ sets potentially producing the input segment have been found, the procedure then needs to check the grammar rules to find those rules which actually generate non-terminal from these sets. This step is not explicitly described in the \textit{ComputeSet} procedure. For each such a rule, the non-terminal on its left-hand side is going to be included in the procedure's result. The final result is then a set of all such non-terminals.

Lets us consider an example, where the procedure computes $X_{1:2,1:2}$, in other words, the segment $\wkpair{ab}{ab}$. All possible divisions of this segment are in the table \ref{tab:segment_divisions}:


\begin{figure}[H]
  \caption{All possible division of the segment $\wkpair{ab}{ab}$}
  \centering
  \label{tab:segment_divisions}
\begin{tabular}{ |l|l|l|l|  }
  \hline
   & sub-segments & corresponding sets  & example of sets contents \\
  \hline
  1. & $\wkpair{ab}{\lambda}$, $\wkpair{\lambda}{ab}$ & $X_{1:2,0:0}$, $X_{0:0,1:2}$ & $\{N_1\}$, $\{N_2\}$ \\ [1ex]
  2. & $\wkpair{\lambda}{ab}$, $\wkpair{ab}{\lambda}$ & $X_{0:0,1:2}$, $X_{1:2,0:0}$ & $\{N_3\}$, $\{N_4, N_5\}$ \\ [1ex]
  3. & $\wkpair{ab}{a}$, $\wkpair{\lambda}{b}$ & $X_{1:2,1:1}$, $X_{0:0,2:2}$ & $\{N_6\}$, $\emptyset$ \\ [1ex]
  4. & $\wkpair{a}{ab}$, $\wkpair{b}{\lambda}$ & $X_{1:1,1:2}$, $X_{2:2,0:0}$ & $\emptyset$, $\emptyset$ \\ [1ex]
  5. & $\wkpair{a}{\lambda}$, $\wkpair{b}{ab}$ & $X_{1:1,0:0}$, $X_{2:2,1:2}$ & $\emptyset$, $\emptyset$ \\ [1ex]
  6. & $\wkpair{\lambda}{a}$, $\wkpair{ab}{b}$ & $X_{0:0,1:1}$, $X_{1:2,2:2}$ & $\emptyset$, $\emptyset$ \\ [1ex]
  7. & $\wkpair{a}{a}$, $\wkpair{b}{b}$ & $X_{1:1,1:1}$, $X_{2:2,2:2}$ & $\emptyset$, $\emptyset$ \\ [1ex]
  \hline
\end{tabular}
\end{figure}


All the 14 $X$ sets from the middle column must already be computed, they are sets for segments of lengths 1, 2 and 3. Each of these sets contains zero or more non-terminals that can produce the given sub-segment. In last step, the procedure checks all rules of the grammar of type $A \rightarrow BC$ to find rules where $B$ is in the first $X$ set and $C$ is in the second $X$ set of one of the divisions of the segment. In the example, there are three combinations of non-terminals that could lead to a result: $N_1N_2$, $N_3N_4$ and $N_3N_5$ ($N_6$ is alone --- that is not not enough to produce the segment). Therefore, the result will be a set $\{X, Y, Z\}$ if there are rules $X \rightarrow N_1N_2$, $Y \rightarrow N_3N_4$ and $Z \rightarrow N_3N_5$. If only subset of these rules is found, the resulting set will contain only left-hand sides of those rules (or could even be an empty set).



\subsubsection{Two remarks regarding WK-CYK}
1. The loop on the line 9 of the procedure \textit{SetsConstruction} (figure \ref{code:wk_cyk_main}) iterates $\beta$ from 0 to $n$. In this context, $\beta$ represents the length of the lower strand segment while $\alpha$ represents the length of the upper strand segment and $\alpha = y - \beta$ where $y$ is the length of the whole segment. Part of the loop actually calculates with non-sensical values. When calculating with segment that is shorter then one strand (i.e. $y < n$), it includes the case when $\beta > y$ and so $\alpha < 0$. In other words, the algorithm splits the segment of length, for instance, 2, to two parts of lengths 3 and -1.

When calculating with segment that is longer than the input, it includes the case where $\beta$ is too short and so $\alpha$ is then longer then a strand length. In other words, if the input length is, for instance, 4 and the segment length is 8, it splits the segment to lengths 7 and 1 which is not possible with the input length of 4.

This does not affect the correctness of the computation because the non-sensical values find no result. However, more precise and efficient solution would be to iterate $\beta$ over the interval: $\langle max(y-n, 0), min(n, y)\rangle$ instead of interval $\langle 0, n\rangle$.

\medskip

2. The time complexity of WK-CYK is $\mathcal{O}(n^6)$. As described in \cite{WK_CYK} (section 6), the WK-CYK main procedure has complexity of $\mathcal{O}(n^4)$ and the nested procedure \textit{ComputeSet} has complexity of $\mathcal{O}(n^2)$. This is true with respect to the input length. Possibly, more precise description of the complexity would be $\mathcal{O}(n^6 \times R)$ where $n$ is the input length and $R$ is the number of rules in the grammar. The description of the procedure \textit{ComputeSet} uses the operation of set union ($\cup$), as if it has constant time complexity which, in reality, it does not --- it requires iterating over the rules of the grammar.

\section{Using automata to test membership in Watson-Crick languages}
In general, automata seem more suitable for deciding the membership in a language than grammars. In case of deterministic automata, the situation is quite straightforward --- every input deterministically leads to the next state and when the whole input is read, the automaton decides whether the input is accepted by finishing in a final state or not. The situation is not as clear in case of the weaker types of deterministic automata.

\subsection{Strongly deterministic Watson-Crick automata}
The strongly deterministic automata represent the simplest case as they have clearly linear complexity. A disadvantage may be the fact that strongly deterministic automata are weaker then deterministic automata.

\subsection{Weakly deterministic and deterministic Watson-Crick automata}
Weakly deterministic and deterministic automata are similar in the sense that their determinism depends on the configuration being known. But the membership of a string in a WK language is defined by the string being equal to upper strand only (see section \ref{section:WKA}). A corresponding lower strand simply needs to exist but is not automatically known in advance. If this is not the case and the entire input with both strands is available at the beginning, then there is practically no difference between the three types of deterministic WKA. The strongly deterministic WKA knows the lower strand thanks to the fact that the complementarity relation is identity --- the strands must be identical.

If the lower strand is not known, however, the deterministic WK automata are in practice the same as non-deterministic WK automata. It is, in general, not necessarily clear what the next state will be given the input. This is demonstrated by example on the left on the figure \ref{fig:deter_WKA}. The snippet of the WK automaton has two transition rules: $q_0 \wkpair{a}{b} \rightarrow q_1$ and $q_0 \wkpair{a}{c} \rightarrow q_2$. Let us suppose that both $(a, b) \in \rho$ and $(a, c) \in \rho$ and there is the string $aaa$ on the input. These rules fulfill the condition for deterministic WKA which, for this case, is: $a \nsim a \vee b \nsim c$. It is not possible to choose the next path based on the upper strand as $a$ is a prefix of $a$. However, it should be possible to decide based on the lower strand --- either a symbol $b$ is going to be read and the automaton will transition to $q_1$ or symbol $c$ and the automaton will transition to $q_2$. But if the process of accepting a string is at the same time looking for a suitable lower strand, it is up to the automaton to decide between these two possibilities and produce either the symbol $b$ to the lower strand or the symbol $c$.

The figure \ref{fig:deter_WKA} on the right shows a snippet of a completely non-deterministic WKA. The rules shown are: $q_0 \wkpair{a}{b} \rightarrow q_1$ and $q_0 \wkpair{aa}{bb} \rightarrow q_2$. Even if both input strands are known, in case the upper strand contains $aa$ and the lower strand contains $bb$, both paths are possible. Either one symbol from each strand will be read and transition will lead to $q_1$ or two symbols from each strand will be read and the transition will lead to $q_2$. So in practice, there is not much difference between a weakly deterministic WKA, deterministic WKA and non-deterministic WKA unless the lower strand is known in advance.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.43]{deter_wka.png}
  \caption{Example of an nondeterminism WKA}
  \label{fig:deter_WKA}
\end{figure}


\subsection{Accepting inputs by a non-deterministic Watson-Crick automaton}
It is possible to propose a similar algorithm for a run of WKA as for non-deterministic finite automata (FA). Such an algorithm for a finite automaton needs to consider all possible paths the FA may take --- in practice it must remember, not just one state where the FA is at the moment, but a subset of all its states. However, this is not enough in case of WKA. An algorithm which controls a run of a non-deterministic WKA needs to remember not only all the states where the automaton can be at a given moment, but also, what inputs have been read from the upper strand and what inputs have been generated to the lower strand. The figure \ref{fig:automata_run} demonstrates the difference between undeterminism in run of a FA and WKA.

The FA snippet on the left starts in state $q_0$. If the next symbol on the input is $a$, the automaton can transition to state $q_1$ or $q_2$. This means that the algorithm keeps in mind that the current state is one of the two (a subset of all FA states: $\{q_1, q_2\}$). If there is another $a$ which causes another non-deterministic choice, it turns out that the number of possibilities can even decrease or, as in this case, stay the same because some branches are merged. The new state is either $q_3$ or $q_4$.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.43]{automata_run.png}
  \caption{Non-determinism in WKA and FA}
  \label{fig:automata_run}
\end{figure}

The situation in case of WKA on the right is much more complicated. Even though it may seem like the paths merge in the state $q_4$, they do not. Let us assume that the input string is $aaa$ and that $(a, b) \in \rho$ and $(a, c) \in \rho$. The automaton has three choices:

\begin{enumerate}
  \item{Transition to $q_1$, read symbol $a$ from the input and generate symbol $b$ to the lower strand. This may be possible even if $(a, b) \notin \rho$ because a different number of symbols might have been read from the two strands (they are, in a way, out of sync).}

  \item{Transition to $q_2$, read two symbols $aa$ and generate two symbols $bb$ to the lower strand.}

  \item{Transition to $q_3$, read symbol $a$ and generate symbol $c$.}
\end{enumerate}

In these three cases, there is more differences between these states of the algorithm then just the different WKA state. Therefore, after the next step, the three paths will still differ in what has been read from the input (upper strand) and what has been generated to the lower strand.

The algorithm that would simulate the run of a WKA could therefore be designed as a state space searching algorithm. Each node would contain three items --- 1. what state is the WKA in, 2. what remains to be read from the input, 3. what has been generated to the lower strand. The solution would then be a node where WKA is in a final state, nothing remains to be read and the two strands (one read, one generated) adhere to the complementarity relation.

This is quite close to the solution that is proposed in the following chapter with the main difference that the space state search is based on WK context-free grammars instead of automata as they have more expressive power (see \ref{section:exp_power}).

\chapter{Testing membership by searching the state space} \label{chapter:parse_tree}
This chapter introduces the main algorithm of this thesis for testing membership in WK context-free languages. In this thesis it is referred to as the \textbf{state space search} or the \textbf{tree search}. Its core is a standard Breadth-first search algorithm (BFS) with various optimizations added on top.

Standard BFS starts with a root node. In case of grammars, that is the starting non-terminal symbol of the grammar. Then the tree is built by applying all possible rules to all possible non-terminals. Each rule application generates a new node. The node contains a word which consists of some non-terminals, some terminals in the upper strand and some terminals in the lower strand.

The BFS algorithm always finds a solution if there is one. It finds the optimal solution which, in this case, means the shortest sequence of rules that generate the input string from the starting non-terminal. However, whether the solution is optimal or not is irrelevant for the membership problem. If there is no solution, the algorithm will probably never stop, as the state tree is usually infinite. Also, such a tree would grow very rapidly and the solution would usually not be found in a reasonable time frame. Therefore, some optimizations need to be used. This work introduces two key kinds of optimizations. Firstly, identifying dead ends in the search tree and removing them from the computation --- this is referred to as \textbf{pruning}. Secondly, choosing such nodes for the subsequent computation which seem to be the most promising in leading to the solution. This is referred to as \textbf{node precedence}.

\section{Key characteristics of the state space search}
Besides pruning and node precedence heuristics, the algorithm keeps a set of states which have been generated (added to the tree), in order to avoid analyzing the same word repeatedly or even getting stuck in a loop. Also, it considers leftmost derivation only. This means that a node which contains several non-terminals can generate new nodes only by applying rules to the first non-terminal in the word.

The figure \ref{fig:search_tree} shows an example of a tree search progress. The rules of its grammar in this example are $S \rightarrow S S \:|\: A B C$, $A \rightarrow \wkpair{a}{a} \:|\: \wkpair{b}{b}$ and some rules $B \rightarrow ...$, $C \rightarrow ...$ which are not important. $S$ is the starting non-terminal, therefore, $S$ is the first node and there are two possible rules that can be applied to $S$, so this node has two successors. The node precedence heuristic will choose one of the successors to be analyzed next --- perhaps the left one with word $A B C$. This node, too, only has two successors, which are made by the two rules that can be applied to the first non-terminal --- $A$. Even though there are some rules for $B$ and $C$, these rules are not used to produce successors, yet. The nodes created by rules applied on $B$ would be successors of the words $\wkpair{a}{a} B C$ and $\wkpair{b}{b} B C$ which have the symbol $B$ as the first non-terminal from the left.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{search_tree.png}
  \caption{Example of a search tree}
  \label{fig:search_tree}
\end{figure}

In a word of a WK grammar, the terminals are clustered together into segments. If two segments appear next to each other in a word, they are merged. These segments, as well as non-terminals, are referred to as \textbf{letters} because together they constitute words. For instance, a three-letter word: $\wkpair{abc}{\lambda} A \wkpair{b}{b}$ after application of rule $A \rightarrow \wkpair{\lambda}{a}$, will result in a word with just one letter: $\wkpair{abcb}{ab}$.

The word in a node that is the solution needs to meet the following criteria:
\begin{enumerate}
  \item{It contains no non-terminals. Since neighboring terminal segments are always merged this implies that there is only one letter --- a segment of terminals.}

  \item{The upper and lower strands of this segment are of the same length.}

  \item{Each pair of symbols from the upper and lower strands with the same index must be related by the complementarity relation.}

  \item{The upper strand must be equal to the input string.}
\end{enumerate}
If the criteria are met, the algorithm has found the right node and that means the input string is a part of the language defined by the grammar. It has been accepted by the state space search algorithm. If the whole state space has been searched (in case it is not infinite), there is no solution and the input has been rejected by the state space search algorithm.


\section{Identifying a dead end in the state tree}
A blind BFS would stop searching a branch only when all non-terminals have been used to generate all possible terminal words (words with terminals only). But sometimes it is possible to tell in advance that a specific word cannot lead to the desired solution. If that is the case, the node can simply been removed and the whole branch which it would generate is skipped. The next section describes various ways (heuristics) of recognizing the dead branches. These are referred to as pruning heuristics, there are five of them and each one has an abbreviation which is used further on.

\begin{enumerate}
  \item{Detecting that one of the strands is already too long --- SL}
  \item{Detecting that the overall word is already too long --- TL}
  \item{Matching the starting terminals in the upper strand to the input --- WS}
  \item{Checking the complementarity relation --- RL}
  \item{Comparing the input to a regular expression generated from the word --- RE}
\end{enumerate}


\subsection{One of the strands is too long (SL)}
A terminal symbol which appears both in the upper or lower strand can never disappear further in the branch. That means that the count of all symbols in upper and in the lower strand must not be grater then the length of the input string. Otherwise the solution can never be reached from that branch.

\subsection{The word including non-terminals is too long (TL)}
Non-terminals present a more complex problem when dealing with the length of the word. First of all, the algorithm calculates in advance how many terminals each non-terminal produces at minimum. For instance, if the grammar contains rules: $A \rightarrow A A \:|\: \wkpair{ab}{cd} \:|\: B B$ and $B \rightarrow \wkpair{a}{\lambda}$, the non-terminal $B$ produces always one terminal, that means one terminal at minimum. The non-terminal $A$ can produce various number of terminals, but two at minimum --- thanks to the rule $A \rightarrow B B$ and the fact that $B$ has the minimum of one. This value is than considered to be the length of the given non-terminal.
This length can be applied both to the upper or to the lower strand because, in general, it is not known which strand will absorb the symbols generated from the non-terminal.
This then leads to the following constraint on the word:

$$|upper| + |lower| + |nts| \leq 2 \times |input|$$

where $|upper|$ and $|lower|$ are the counts of terminals in the upper and lower strands, $|nts|$ is the length of all non-terminals in the word and $|input|$ is the length of the input string. If this constraint is broken, the word cannot lead to the solution and the branch can be pruned.


If the grammar contains no $\lambda$-rule (rule of the form $N \rightarrow \wkpair{\lambda}{\lambda}$), This constraint guarantees that the algorithm will finish. Once all the words within the given length limit have been generated and a solution not found, the search will end.

If the grammar does contain $\lambda$-rules, the previous constraint can still be applied --- the non-terminals that can be erased are assigned the length of zero. In this case, it is not possible to guarantee that the search will end, because the non-terminals of length zero can be combined infinitely many times. However, it is possible to utilize the algorithm for removing $\lambda$-rules (which is described in \cite{WK_CYK} and which is implemented in the application described in the next chapter).

\subsection{The beginning of the word does not match the input (WS)}
If a word in a node begins with some terminal symbols in the upper strand, these symbols will always stay at the beginning further in the given branch. Unlike the other terminals, these starting terminals already have fixed indexes. If these symbols do not match the prefix of the input string of the same length, the input string can never be generated from this branch.
If on the other hand, the word starts with a non-terminal, there is nothing to be said about what can be at the beginning of the word further in the branch.

It is possible to check the end of the word in the same manner but the generation is performed from the left to the right and so there is little benefit in checking the end.

\subsection{Checking the complementarity relation (RL)}
As previously described, the symbols in the upper and lower strands with the same indexes must be related by the complementarity relation. Unfortunately, this can be checked only at the beginning of the word (Technically, it can be checked at the end as well, while indexes of these symbols are not yet known, the last terminal symbol will always stay the last. But just like in the case of previous heuristic, there is little benefit in checking the end when the generation is done from the left side.). Indexes of the symbols in the middle part (anywhere after the first non-terminal) are not known. Thus this check can be understood as an extension of the previous one --- if the word begins with some terminal symbols and there are some symbols in both the upper and lower strands, these symbols can be tested whether they adhere to the complementarity relation. But only to the length of the shorter of the two strands in this letter.

\subsection{The input matches a regular expression generated from the word (RE)}
It is possible to generate a regular expression that represents the current word. Each non-terminal serves as a wild card (\verb/.*/). Each terminal in the upper strand stands for itself. Lower strand is ignored. This expression must be matchable to the input string, otherwise it is not possible to generate it from the current branch. For instance, if the word is
$$\wkpair{abc}{f}N_1\wkpair{d}{gh}N_2\wkpair{e}{i}N_3$$

then the resulting regular expression will be: $\verb/^abc.*d.*e/$. The symbols $abc$ must be at the beginning (therefore the \verb/^/ denoting the beginning of the expression is placed at the start); then it is not known what will be generated by the non-terminal $N_1$ --- therefore the wildcard is there next; then there will have to be a symbol $d$; another wildcard for non-terminal $N_2$; symbol $e$; and then anything. The regular expression might end with a wildcard generated from the last non-terminal $N_3$ but that is not necessary. Wildcard before and after the expression is implicit. A starting non-terminal can be represented by omitting the symbol \verb/^/ which denotes the beginning of the string. An ending non-terminal can be represented by omitting the symbol \verb/$/ which denotes the end of the string.

\medskip

The order in which the pruning heuristics are applied matters. It is good to first apply the heuristics that are more likely to succeed and that require less computational power. If they are successful, the more complex heuristic can be skipped.

It is possible to come up with some more checks that could identify a dead end in the search tree. The disadvantage of any check is the computing power that has to be used for checking any node that is generated and analyzed. If some checks are unlikely to significantly prune the tree and/or are complicated to compute, it is not clear if they will improve the actual performance of the algorithm.

\subsection{Examples of pruning}
Let us consider a grammar with no $\lambda$-rules with the identity complementarity relation, the input string $abcd$ and the following words:

\begin{enumerate}
\item{
$$\wkpair{a}{ab} N_1 \wkpair{\lambda}{cd} N_2 \wkpair{\lambda}{e}$$
The input string can never be generated from this word because the fragments of the lower strand are too long already --- it has five symbols and the input string only has four. The SL pruning will remove this word.
}
\item{
$$\wkpair{ab}{ab} N_1 N_2 \wkpair{\lambda}{d} N_3 N_4$$
This word would be promising if there had been some $\lambda$-rules. Since there are not, the word contains too many non-terminals. Each of them is going to generate at least one terminal symbol and only three symbols are missing ($c$ and $d$ in the upper strand and $c$ in the lower strand). Inevitably, there will be at least one symbol too many. This word will be removed by the TL pruning.
}
\item{
$$\wkpair{abd}{\lambda} N_1 \wkpair{\lambda}{ab} N_2$$
Regardless of what can be generated from $N_1$ and $N_2$, the upper strand will always have to begin with symbols $abd$. There is no way how to insert $c$ between $b$ and $d$. Therefore the input string can never be generated from here and this word will be removed by the WS pruning.
}
\item{
$$\wkpair{abc}{ac} N_1 \wkpair{\lambda}{d} N_2$$
The upper strand looks promising as it starts with the same symbols $abc$ as the input string. But the lower strand starts with $ac$. The first symbol pair $(a/a)$ adheres to the complementarity relation, the second one $(b/c)$ does not. The third symbol in the upper strand --- $c$ cannot be related to any symbol --- it has no counterpart, yet. The check was always going to end with the second symbol pair. Anyway, this word will be removed by the pruning RL.
}
\item{
$$N_1 \wkpair{b}{\lambda} N_2 \wkpair{a}{\lambda} N_3$$
Whatever is generated from the non-terminals $N_1$, $N_2$ and $N_3$, the upper strand will always keep the order of symbols --- first, the symbol $b$, then the symbol $a$ (with potentially some symbols before, in between and after). That can never result in the string $abcd$. This word will be removed by the pruning RE.
}

\section{Heuristics for node precedence} \label{heur_node_pref}

The aim of the node precedence heuristics is to choose a path in the search tree, which is likely to lead to the solution, the more promising nodes are taken before the others and their successors are generated sooner. The individual heuristic functions attempts to answer the question --- which node is more promising than the rest? It assigns each node a number --- an evaluation of the node. The lower the node evaluation, the higher priority the node has.

Such heuristics can only be effective if the answer of the search is positive --- if there actually is a solution. Unfortunately, if it is negative, it does not help that the algorithm eliminates the more promising branches of the tree first. Eventually, it will have to search through all possible states anyway, in order to make sure that there is no solution.

The following node precedence heuristics have been implemented. Each heuristic also has an abbreviation that will refer to it further on.

\begin{itemize}
  \item{No heuristic --- the evaluation of the word is always 0. This is used for comparison to the other heuristics.}
  \item{Aversion to non-terminals (NTA) --- the evaluation is equal to the count of non-terminals in the word}
  \item{Weighted aversion to non-terminals (WNTA) --- each non-terminal has a pre-calculated weight, which is the minimum amount of rules that must be used in order to generate a terminal word from it. The evaluation is equal to the sum of the weights of all non-terminals in the word.}
  \item{The terminal matching --- there are three variants that differ slightly (TM1, TM2, TM3). Each of them increases the priority (i.e. decreases the evaluation) for each upper strand non-terminal (going from left to right) which matches the input string symbol on the same index.
  \begin{itemize}
    \item{TM1 examines terminals going from the left while ignoring non-terminals, decreases evaluation (i.e. increases priority) for each match and finishes when it discovers the first difference.}
    \item{TM2 is similar to TM1, but when it discovers a difference, it does not finish but increases the evaluation and moves on}
    \item{TM3 evaluates the first item in the word only. If it is a non-terminal, it returns zero.}
  \end{itemize}
  }
  \item{Combinations of NTA/WNTA and TM1/TM2/TM3 --- There are six combinations because it does not make sense to combine NTA and WNTA or TM1-3 together: NTA+TM1, NTA+TM2, NTA+TM3, WNTA+TM1, WNTA+TM2, WNTA+TM3.}
\end{itemize}

In summary, there are 12 node precedence heuristics considered in total (including the first, empty heuristic). Unlike in case of pruning, where all methods can be applied at the same time, there can be only one node precedence heuristic active at one time. Therefore chapter \ref{chapter:testing} contains the tests and comparison of the effectiveness of these heuristics.
\end{enumerate}

\section{Theoretical complexity of the state space search}
The state space search algorithm uses Breadth-first search (BFS) as its basis. Both the time and space complexity of BFS are $\mathcal{O}(b^d)$ where $b$ is the maximum number of successors of a node (branching factor) and $d$ is the depth of the tree. The branching factor is then equal to the maximum number of rules of the given grammar that have the same non-terminal on the left-hand side. This is because always only the first non-terminal in the word is used to generate successors in the tree. The depth of the tree is going to be different for different grammars and even for different inputs.

In general, the theoretical complexity of the state space search algorithm is not impressive, it is much worse then WK-CYK's $\mathcal{O}(n^6)$ or $\mathcal{O}(n^6 \times R)$. However, this is because it has been designed with a rather practical approach, it relies heavily on the heuristics and optimizations and so its performance is usually much better.

\section{Parallelization of the state space search}
The parallelization of the state space search should be very much possible and straightforward. The algorithm uses a priority queue to store all the nodes which are to be analyzed. Therefore, multiple processes could be taking nodes from the queue and analyze them independently. There are many variants how this could be done --- analyzing one node at the time and returning result immediately to the queue shared by all process or analyzing independently whole segments of the tree with less need for synchronization but, perhaps, more redundant work --- these are questions of the efficiency of the actual implementation which is out of the scope of this work.

\chapter{Implementation of the state space search} \label{chapter:implementation}

The implementation has been done in the language Python3. The main components are the following:
\begin{itemize}
  \item{the class representing a Watson-Crick context-free grammar, including the implementation of the WK-CYK algorithm and the state space search algorithm}
  \item{classes representing a rule of a grammar and a tree node}
  \item{a set of grammar definitions and generators of the input strings}
  \item{a set of test scripts that run the various tests comparing heuristics, performance etc.}
  \item{test runner class which is a middle layer between the test scripts and the main grammar class.}
\end{itemize}

In the code and its description I use the term \textbf{word} to refer to the right hand side of the rules and, in general, a list of letters. The term \textbf{letter} is a one element of the word, which is either a non-terminal, or a segment with terminals. Such segments with terminals are stored as a pair (tuple) of two lists --- upper and lower strand. For instance, a word
$A \wkpair{abc}{\lambda} B$
has three letters: non-terminals $A$ and $B$ and a segment of non-terminals $\wkpair{abc}{\lambda}$, which contains two lists, first (upper strand) with three items --- terminals $a$, $b$ and $c$ and the second one (lower strand) is an empty list.


\section{Implementation of the main class representing the grammar}
The class representing a grammar is called \textit{cWK\_CFG}, the source file is \textit{lib/ctf\_WK\_grammar.py}. It contains the following data:
\begin{itemize}
  \item{the items which define the grammar: \textit{nts} --- the set of non-terminals, which are represented by alpha-numeric characters; \textit{ts} --- set of terminals, also represented by alpha-numeric characters; \textit{startSymbol} --- starting non-terminal; \textit{rules} --- set of grammar rules, which are objects of the class \textit{cRule}; \textit{relation} --- list representing a complementarity relation, it contains tuples of two terminals. These five items are parameters which need to be passed to the class constructor. This corresponds to the way how a context-free WK grammar is formally defined.}

  \item{\textit{nodePrecedenceList} --- a list of all implemented node precedence heuristics, it is stored as a list of pairs (tuples) of heuristic name and function; \textit{currentNodePrecedence} is the index of the active one}

  \item{\textit{pruningOptions} --- a dictionary with a pruning function as a key and a boolean as a value indicating which pruning heuristics are active; \textit{pruneCnts} --- a dictionary of pruning functions as keys and integer values which counts, how many times the given pruning has been successfully used}

  \item{\textit{ruleDict} --- grammar rules stored in a dictionary with non-terminals as keys and list of rules as values. This is a more efficient way of accessing rules for a given non-terminal then iterating over all rules and filtering them based on left-hand side non-terminal.}

  \item{\textit{relDict} --- complementarity relation stored in a more convenient way in a dictionary with first symbol as a key and string of symbols (symbols that are related to the key symbol) as a value. This is more efficient way of finding all symbols related to a given symbol.}

  \item{\textit{ntDistances} --- a pre-calculated dictionary, with all non-terminals as keys and distance to terminals as values. This distance is a minimum number of rules which lead from the given non-terminal to a word with terminals only. This is used for the node precedence heuristic WNTA.}

  \item{\textit{erasableNts} --- a pre-calculated set of non-terminals which can be erased by applying certain sequence of rules. It is used for removing the $\lambda$-rules.}

  \item{\textit{termsFromNts} --- a pre-calculated dictionary which has all non-terminals as key and as a value minimum amount of terminals that can be generated from this non-terminal. This is used in the pruning heuristic TL.}

  \item{timeLimit --- after how long should the tree search or WK-CYK time out}
\end{itemize}

A constructor of the \textit{cWK\_CFG} class requires as parameters a list of non-terminals, a list of terminals, the starting non-terminal, the list of rules and the list of relations. An example of an object construction would then be (for rules definition, see \ref{section:cRule_cNode}):
\begin{verbatim}
  g = cWK_CFG(['S', 'A'], ['a', 'b'], 'S', rules, [('a', 'a'), ('b', 'b')])
\end{verbatim}

\bigskip
The class \textit{cWK\_CFG} has these key functionalities:
\begin{itemize}
  \item{initialization, backup and restore}
  \item{the run of the tree search algorithm}
  \item{pruning heuristics}
  \item{node precedence heuristics}
  \item{the transformation of grammar to the CNF}
  \item{the run of the WK-CYK algorithm}
\end{itemize}

\subsection{Initialization, backup and restore}
During an initialization of the class, several methods are run ensuring validity of the grammar and pre-calculating data.

\begin{itemize}
  \item{method \textit{is\_consistent} verifies that the constructor parameters that define the grammar are consistent: sets of terminals and non-terminals must be exclusive, the start symbols and all rules left-hand sides must be found among the non-terminals, rule right hand sides must contain only specified terminals and non-terminals and the complementarity relation list must refer only to the defined terminals. If the method fails, an exception is thrown and the class is not created.}

  \item{method \textit{generate\_rule\_dict} parses the set of rules and creates \textit{ruleDict}}

  \item{method \textit{generate\_relation\_dict} parses the complementarity relation and creates \textit{relDict}}

  \item{method \textit{find\_erasable\_nts} creates the set \textit{erasableNts}, a set of non-terminals that can be erased by applying a sequence of rules. The set is empty if the grammar contains no $\lambda$-rule.}

  \item{method calc\_nt\_distances creates the dictionary \textit{ntDistances} mapping non-terminals to the minimum number of rules needed to reach a terminal word}

  \item{method calc\_min\_terms\_from\_nt creates the dictionary \textit{termsFromNts} mapping non-terminals to the minimum number of terminals that they can generate}

  \item{method calc\_rules\_nt\_lens calculates the rule length for all rules}
\end{itemize}

A rule length is a value which indicates how the minimal length of a word will be changed after the application of the rule. It is used by the TL (total length) pruning heuristic. The rule length is equal to the negative left-hand side non-terminal length plus lengths of all elements on the right-hand side.

For instance, let us have a word $A \wkpair{ab}{c} B$, the rule being applied $A \rightarrow B \wkpair{d}{d}$ and the minimum number of terminals generated from $A$ is 2 and from $B$ is 3. The resulting word will be $B \wkpair{dab}{dc} B$. The word at the start had total length of 8 (1 for each terminal symbol, 2 for $A$, 3 for $B$). The word afterwards has a total length of 11 (3 for each $B$ and 1 for each terminal). Therefore, the rule $A \rightarrow B \wkpair{d}{d}$ must have the length of 3. And it does: $-2$ for the non-terminal $A$ on the left-hand side, 1 for each of the two terminals and 3 for the non-terminal $B$.

Since the grammar is able to apply certain transformations (like to the CNF or removing lambda rules), it is convenient to be able to save the state of the grammar and later restore it. This is what the methods \textit{backup} and \textit{restore} are for. The \textit{backup} method simply saves a copy of the sets of rules, non-terminals and terminals as \textit{rulesBackup}, \textit{ntsBackup} and \textit{tsBackup}. The \textit{restore} method then restores these backup sets and runs again the pre-calculating methods similarly to the class initialization phase.

\subsection{Run of the state space search algorithm}
The state space search algorithm is run by the \textit{run\_tree\_search} method which has one parameter: the string to be tested for the language membership. It uses a priority queue (python \textit{PriorityQueue} class from the \textit{queue} module) to store the nodes of the search tree. When getting items from this queue, it returns the smallest value item that it contains. The items in the queue are objects of the class \textit{cTreeNode} (described in \ref{section:cRule_cNode}). The method also uses a set of hash codes (acquired by the standard Python \textit{hash} function) of all the nodes that have been put in the queue (whether they are still there or have been taken out), so that duplicate nodes are skipped.

At the beginning, the queue contains one node --- the starting non-terminal. If the queue is empty (and there is no node being parsed), it indicates that the whole state space has been searched and the method ends with a negative response. Otherwise, it gets the next node from the queue and generates all possible successors of this node (method \textit{get\_all\_successors}). Each of the successor nodes is tested whether it is the desired solution (method \textit{is\_result}). If so, the search optionally prints the path to the solution and ends with the positive result. Otherwise, it checks whether the node is new (has not been in the queue before) by computing its hash and looking into the set of hashes of all generated states. If it is new, it is added to the queue. Also, the main loop checks during every iteration whether the time limit has been exceeded. If so, it returns an empty response (\textit{None} value).

The return value of the method is a tuple containing following items: a maximum number of items in the queue, number of all generated nodes, list of pruning statistics, the actual result (\textit{True}, \textit{False}, or \textit{None}).

The \textit{get\_all\_successors} method requires two parameters --- a node and the input string (which is then passed to the pruning methods). First, it finds the first non-terminal in the given word. It applies all the rules (method \textit{apply\_rule}) that the grammar has for this non-terminal, each time creating a new node. The node is then checked by all the pruning algorithms (method \textit{is\_word\_feasible} described in next section) to see, if the node can lead to the solution. If so, the priority of the node is calculated (method \textit{calculate\_distance} described in the next section) and the node is yielded as a result.

The \textit{is\_result} method needs to check all the conditions that the node has to meet in order to be a solution (word contains only one letter, the letter is a segment of terminals, its upper strand and lower strand have the same length, all the symbols from the two strands correspond to the complementarity relation and the upper strand is equal to the input string). It takes a word and the input string as parameters and returns a boolean value indicating whether the word is the solution.

The \textit{apply\_rule} method takes three parameters: a word, an index of the non-terminal to be replaced and a rule right-hand side. It removes the non-terminal specified by its index (because there can be more than one occurrence of this non-terminal in the word) from the word, and replaces it with the word snippet specified by the rule left-hand side. It contains logic for merging letters containing terminals if they appear in the word next to each other. It returns the final word.

\subsection{Pruning heuristics}
In order to be able to work flexibly with the pruning methods, the class contains a dictionary (called \textit{pruningOptions}) of the implemented pruning functions and indication whether they are active or not. The method \textit{is\_word\_feasible} iterates through all items in this dictionary and if the value is \textit{True}, indicating the active pruning, it runs the corresponding method.
The pruning methods are:
\begin{itemize}
  \item{\textit{prune\_check\_strands\_len} (SL): checks that the sum of the symbols in the upper and the lower strand is not greater than the input length}

  \item{\textit{prune\_check\_total\_len} (TL): checks that the total length of the word (count of all terminal symbols plus lengths of all non-terminals) is not greater then the doubled input length}

  \item{\textit{prune\_check\_word\_start} (WS): checks that the starting terminals of the word correspond to the input string}

  \item{\textit{prune\_check\_relation} (RL): checks that the starting terminals meet the complementarity relation constraint}

  \item{\textit{prune\_check\_regex} (RE): checks that the input matches the regular expression based on the word}
\end{itemize}

All the pruning methods require a word and the input string as parameters. They return a boolean value indicating whether the word is feasible or not.

\subsection{Node precedence heuristics}
Just like with pruning, the node precedence functionality needs to be flexible --- it must be possible to switch between various node precedence methods. The functionality is implemented by the method \textit{compute\_precedence}. This methods simply uses \textit{nodePrecedenceList} and \textit{currentNodePrecedence} to call the right specific method.
There are 12 of these methods called \textit{compute\_precedence\_[name]} where \textit{name} is the name of the specific heuristic (one of NTA, WNTA, TM1, TM2, TM3, NTA\_TM1, NTA\_TM2, NTA\_TM3, WNTA\_TM1, WNTA\_TM2, WNTA\_TM3, no\_heuristic). All of these methods take a word and an input string as parameters, they return an integer evaluation of the node.

\subsection{Transformation of grammar to WK Chomsky normal form}
The transformation of a WK grammar to WK-CNF is quite similar to the transformation of a standard context-free grammar to the CNF. It is performed by the method \textit{to\_wk\_cnf} and it takes the following steps:

\begin{enumerate}
  \item{Removing $\lambda$-rules --- this is done by the method \textit{remove\_lambda\_rules}}

  \item{Removing unit rules --- unit rules are rules in the form of $A \rightarrow B$ where $A$ and $B$ are non-terminals. These is performed by the method \textit{remove\_unit\_rules}.}

  \item{Removing unterminatable symbols --- unterminatable symbols are non-terminals, which cannot be transformed into terminal strings by any sequence of rules. It is possible to remove them without affecting the grammar language because if such a rule ever appeared in a word, the whole word would be automatically useless. Any rules containing such a symbol are useless, as well, and are also removed. This is performed by the method \textit{remove\_unterminatable\_symbols}.}

  \item{Removing unreachable symbols --- unreachable symbols are symbols that can never appear in a word because there is no sequence of rules leading from the starting symbol that would be able to generate them. Therefore they can be removed without affecting the grammar language. Any rules containing such a symbol are useless, as well, and are also removed. This is done by the method \textit{remove\_unreachable\_symbols}.}

  \item{Dismantling rules generating terminal letters --- WK-CNF requires all rules generating terminals to generate one symbol only, this means only rules in the form of $A \rightarrow \wkpair{a}{\lambda}$ and $A \rightarrow \wkpair{\lambda}{a}$ are allowed. This is achieved by the method \textit{dismantle\_term\_letters} which iterates through all the rules and replaces any terminal letters at the rule right-hand side with a newly generated non-terminal. Afterwards, new rules are added and non-terminals are created which assure that the terminal letter is generated in steps, each rule having at maximum two items on the right-hand side.

  If the rule is $A \rightarrow A B \wkpair{ab}{c}$ ($A$, $B$ being non-terminals, $a$, $b$, $c$ being terminals), then the $A$ and $B$ are skipped and the letter $\wkpair{ab}{c}$ is replaced by a new non-terminal $N_1$. Then, another rule is created: $N_1 \rightarrow \wkpair{ab}{c}$, which needs to be broken down further. For each terminal in this letter, a new rule is created and the terminal replaced by a new non-terminal, until there remains only one terminal in that letter. The final set of rules is then going to be: $A \rightarrow A B N_1$, $N_1 \rightarrow \wkpair{a}{\lambda} N_2$, $N_2 \rightarrow \wkpair{\lambda}{c} N_3$, $N_3 \rightarrow \wkpair{b}{\lambda}$}

  \item{Dismantling rules generating non-terminals --- in the final stage of the transformation, the method \textit{transform\_to\_wk\_cnf\_form} iterates through all the rules, it keeps the rules in the WK-CNF form ($A \rightarrow BC$, $A \rightarrow \wkpair{a}{\lambda}$ and $A \rightarrow \wkpair{\lambda}{a}$) and breaks down other rules in a process analogous to the actions of \textit{dismantle\_term\_letters}}.

  \item{Recalculation of data --- runs the methods that pre-calculate data, similarly to the class initialization phase or after \textit{restore} method}
\end{enumerate}

The methods which remove $\lambda$-rules, unit rules, unterminatable symbols and unreachable symbols are useful even outside of the transformation to the WK-CNF. Removing unterminatable symbols and unreachable symbols (and rules containing these symbols) are optional but useful steps in the transformation. The dismantling of rules are two steps that make sense only in this context.


\subsection{Run of the WK-CYK algorithm}
The WK-CYK algorithm is implemented by the method \textit{run\_wk\_cyk} which takes as a single parameter an input string and returns a boolean value indicating whether the input has been accepted or not. Similarly to tree search, every iteration of the algorithm's main loop checks the elapsed time and if it exceeds the time limit, it returns an empty value (\textit{None}). The implementation follows closely the description in \cite{WK_CYK} (in section 6) and \ref{code:wk_cyk_main}. The \textit{run\_wk\_cyk} method corresponds to the \textit{sets construction} procedure. The \textit{compute set} procedure called from \textit{sets construction} then corresponds to the \textit{computeSet} method of the cWK\_CNF class.


\section{Implementation of the rule and node classes} \label{section:cRule_cNode}
A rule of a grammar is modeled by the class \textit{cRule}. It contains the following data:
\begin{itemize}
  \item{\textit{lhs} --- a non-terminal, left-hand side of the rule}
  \item{\textit{rhs} --- a word, right-hand side of the rule}
  \item{\textit{ntsLen} --- length of all non-terminals of the right-hand side}
  \item{\textit{upperCnt} --- count of all terminals in the upper strand of the right-hand side}
  \item{\textit{lowerCnt} --- count of all terminals in the lower strand of the right-hand side}
\end{itemize}

The items \textit{ntsLen}, \textit{upperCnt} and \textit{lowerCnt} are there for optimization purposes. It is always possible to iterate over the word and count them, but it is more efficient to count it once for every rule and store this value.

The \textit{cRule} class then contains the following methods:
\begin{itemize}
  \item{\textit{compactize} --- this method is called during the object initialization phase and ensures that the right-hand side does not contain any terminal letters next to each other, if it does, then these are merged together. For instance a rule $A \rightarrow \wkpair{a}{b} \wkpair{a}{b}$ is transformed to an equivalent rule $A \rightarrow \wkpair{aa}{bb}$.}
  \item{\textit{calculate\_cnts} --- method is called during the object initialization phase and counts values for \textit{upperCnt} and \textit{lowerCnt}}. The length of non-terminals is calculated during the initialization of the cWK\_CFG object because it needs to know the length of non-terminals.
\end{itemize}

A constructor of the \textit{cRule} class requires a left-hand side of the rule, which is a non-terminal, and a right-hand side of the rule, which is a word, i.e. a list of letters. A letter is either a non-terminal or a tuple of two lists. An example of a rule object creation for rule $A \rightarrow A \wkpair{ab}{\lambda}$ would then be:

\begin{lstlisting}[language=Python]
  cRule('A', ['A', (['a', 'b'], [])])
\end{lstlisting}

\bigskip

A tree node is modeled by the class \textit{cTreeNode} which contains the following:
\begin{itemize}
  \item{\textit{word} --- the actual word of the grammar}
  \item{\textit{upperStrLen} --- number of upper strand terminals in the word}
  \item{\textit{lowerStrLen} --- number of lower strand terminals in the word}
  \item{\textit{ntLen} --- length of all non-terminals}
  \item{\textit{parent} --- node in the search tree, which is this node's  predecessor, this is not necessary for the search but once a solution is found, it may be useful to know what path has been taken to reach it}
  \item{\textit{hashNo} --- a unique hash code of the node calculated during the object initialization}
  \item{\textit{precedence} --- a value assigned by the active node precedence heuristic, it is used to compare two objects of this class which is needed by the priority queue to order the nodes}
\end{itemize}

A \textit{cTreeNode} constructor requires a word, three integers specifying the \textit{upperStrLen}, \textit{lowerStrLen} and \textit{ntLen}, the parent node, and the precedence. Of course, the lengths of terminals could be counted by the tree node itself during the initialization or when needed. These values are passed to the constructor for optimization purposes. Counting them would require iterating over the whole word which can be quite long. When creating a new node, it is more efficient to take these counts from the parent node and add or subtract differences which are stored in the rule object which is being applied.

An example of a creation of this class object (in this case the root node) could then be:

\begin{lstlisting}[language=Python]
  cTreeNode(['S'], 0, 0, 1, None, 1)
\end{lstlisting}

\bigskip

Both of these classes, \textit{cTreeNode} and \textit{cRule}, as well as the main class cWK\_CFG, are in the source file \textit{lib/ctf\_WK\_grammar.py}, as they are quite closely related.


\section{Implementation of the test runner class, test scripts and grammars}
The grammars that are used for testing both the tree search and the WK-CYK are stored in the file \textit{lib/grammars.py}. Each grammar is characterized primarily by its rules, those are created first. Then the instance of the cWK\_CFG is created and then a generator of inputs is assigned to each grammar.

A generator of inputs is a method of each grammar object called \textit{input\_gen\_func}. Its purpose is to generate inputs for the given grammar of increasing lengths. It takes 3 parameters: a starting number of characters, a step --- how many characters should be added in the next generated input, and a boolean indicating whether the generated inputs should be accepted by the grammar or not. The generator does not have to return the input string exactly of the length which it was specified. Sometimes it is not even possible. The generated string's length may be approximate to the specified values.

Here is an example of input generator use for grammar 1:
\begin{lstlisting}[language=Python]
generator = g1.input_gen_func(5, 2, True)
input1 = next(generator)   # generates 'aaaaa'
g1.can_generate(input1)
input2 = next(generator)   # generates 'aaaaaaa'
g1.can_generate(input2)
\end{lstlisting}

I have implemented the following five test scripts, which are in the root directory:
\begin{itemize}
  \item{\textit{ts\_node\_precedence\_tests.py} --- runs a test for all of the 40 grammars (20 in the basic form and 20 in the CNF) with inputs which are going to be accepted (node precedence is irrelevant if inputs are eventually rejected). Each test runs the tree on search this input one time for each of the available node precedence heuristics.}

  \item{\textit{ts\_pruning\_tests.py} --- runs two tests for each of the 40 grammars, one with an input that will be accepted and one that will be rejected by the search. Each test runs the search with all pruning heuristics inactive, then with all active, and then for each one it runs with activated all but the one heuristic. Thus comparing how each one heuristic contributes to the overall performance.}

  \item{\textit{ts\_speed\_tests.py} --- runs two tests (one positive, one negative) for each of the 40 grammars, in each test a Tree search is run repeatedly (up to 30 times or it is stopped if a search exceeds time limit) with increasing input length. This is used to analyze the time and memory complexity of the tree search with respect to the input length.}

  \item{\textit{ts\_var\_inputs\_tests.py} --- runs tests for some hand-picked inputs of the same length in order to compare, how the different variants of the same length inputs affect the performance}

  \item{\textit{wk\_cyk\_tests.py} --- runs two tests (one positive, one negative) for 17 grammars, which are ready for WK-CYK run. Those must be in the CNF and grammars 5, 19 and 20 are excluded, since they use other complementarity relation then identity. In each test, the grammar runs the WK-CYK repeatedly with increasing input lengths.}
\end{itemize}

Each of these scripts prints its output into a table where all the results are compared. Outputs, which I received by running these test scripts, are attached in the output directory.

All these scripts use the \textit{cPerfTester} class, which is a sort of middle layer between the grammar class and the test scripts. It helps with displaying the result table and gathering and parsing data returned by the algorithm runs.
It contains the following methods:

\begin{itemize}
  \item{\textit{run\_test\_ntimes} --- runs the tree search several times, calculates and returns averages over results of these runs}
  \item{\textit{run\_node\_precedence\_test} --- a wrapper used by the script \textit{ts\_node\_precedence\_tests.py}}
  \item{\textit{run\_prune\_test} --- a wrapper used by the script \textit{ts\_pruning\_tests.py}}
  \item{\textit{run\_speed\_test} ---a wrapper used by the script \textit{ts\_speed\_tests.py}}
  \item{\textit{var\_inputs\_test} ---a wrapper used by the script \textit{ts\_var\_inputs\_tests.py}}
  \item{\textit{run\_wk\_cyk\_test} ---a wrapper used by the script \textit{wk\_cyk\_tests.py}}
\end{itemize}


\section{How to use the application}
It is possible to directly run the one or more of the test scripts from the root folder. They do not have other requirements than having Python3 installed. In the application root directory in Linux terminal it can be run simply by typing:

\begin{verbatim}
  python3 ts_node_precedence_tests.py
  python3 ts_pruning_tests.py
  ...
\end{verbatim}

In order to use the \textit{cWK\_CFG} class directly there is a demo script in the root directory \textit{demo.py} which shows the use of predefined grammars and creating a new grammar step by step and can be called in the same way:

\begin{verbatim}
  python3 demo.py
\end{verbatim}

In a nutshell, when using a predefined grammar, it can be used right after import. To test if a grammar 1 can generate a string $aaaaa$, one could write:
\begin{lstlisting}[language=Python]
  from lib.grammars import g1
  o, a, p, result = g1.can_generate('aaaaa')
  print(result)
\end{lstlisting}

And to define a grammar from scratch and, for instance, run the WK-CYK algorithm, one can write:
\begin{lstlisting}[language=Python]
  rules = [
    cRule('S', ['S', 'S', 'S']),   # S -> S S S
    cRule('S', [(['a'], ['a'])])   # S -> a/a
  ]
  g1 = cWK_CFG(['S'], ['a'], 'S', rules, [('a', 'a')])
  g1.to_wk_cnf()
  result = g1.run_wk_cyk('aaaaa')
  print(result)
\end{lstlisting}


\chapter{Testing the state space search and the WK-CYK algorithm} \label{chapter:testing}

In this chapter I present the set of grammars that have been used to test the algorithms and then the results of the tests. First, there is the test comparing the node precedence heuristics. Since only one can be active at a time, it is appropriate to start with choosing the one which provides the best overall performance. The tests that follow after that will all use this winning node precedence heuristic. Next, I test pruning heuristics to see if all of them contribute to the overall performance or if it is better to turn some off. This way I get the best configuration of the state space search that is available. In some cases, a different configuration would be more efficient but the goal here is to get the best overall performance.

When the best configuration is known, I test the performance of both the state space search algorithm and WK-CYK with various inputs, especially inputs of increasing lengths and compare the results.

\section{Watson-Crick grammars used for testing}

For the testing of the tree search algorithm and the WK-CYK algorithm, I have used the following Watson-Crick grammars. Unless stated otherwise, the set of non-terminals and the set of terminals is defined simply symbols that appear in the rules --- all the uppercase letters are non-terminals of the grammar and all the lowercase letters and digits are terminals. The starting non-terminal is $S$ and the complementarity relation is identity. With these specifications in mind the grammar can be defined by the rules only.

\begin{enumerate}
  \item{
    $$S \rightarrow \wkpair{a}{a} \:|\: S S S$$

    The accepted language is: $a(aa)^*$
  }

  \item{
    $$S \rightarrow \wkpair{a}{a} S \:|\: \wkpair{b}{b} S \:|\: \wkpair{c}{c} S \:|\: \wkpair{abc}{abc}$$

    The accepted language is: $(a+b+c)^*abc$

    The aim of this example is to test inputs with the decisive part on the very end. This could prove difficult  since the tree search expands the non-terminals from left to right.
  }

  \item{
    $$S \rightarrow A \wkpair{abc}{abc}$$
    $$A \rightarrow A \wkpair{a}{a} \:|\: A \wkpair{b}{b} \:|\: A \wkpair{c}{c} \:|\: \wkpair{\lambda}{\lambda}$$

    The accepted language is: $(a+b+c)^*abc$

    The aim of this example is, again, to test inputs with the decisive part on the very end while, at the same time, the rules are left recursive.
  }

  \item{
    $$S \rightarrow Q \wkpair{a}{a} \:|\: A B C D E F G$$
    $$Q \rightarrow Q Q \:|\: A B C D E F G$$
    $$A \rightarrow \wkpair{a}{a} \:|\: \wkpair{\lambda}{\lambda}$$
    $$B \rightarrow \wkpair{b}{b} \:|\: \wkpair{\lambda}{\lambda}$$
    $$C \rightarrow \wkpair{c}{c} \:|\: \wkpair{\lambda}{\lambda}$$
    $$D \rightarrow \wkpair{d}{d} \:|\: \wkpair{\lambda}{\lambda}$$
    $$E \rightarrow \wkpair{e}{e} \:|\: \wkpair{\lambda}{\lambda}$$
    $$F \rightarrow \wkpair{f}{f} \:|\: \wkpair{\lambda}{\lambda}$$
    $$G \rightarrow \wkpair{g}{g} \:|\: \wkpair{\lambda}{\lambda}$$

    The accepted language is: $a?b?c?d?e?f?g? + (a?b?c?d?e?f?g?)^*a$ ($x?$ denotes that the symbol $x$ is optional, i.e. $(x + \lambda)$ )

    The problematic feature of this grammar may be the fact, that during the transformation of this grammar to WK-CNF (more specifically, when removing the $\lambda$-rules) the number of rules increases rapidly.
  }

  \item{
    $$S \rightarrow \wkpair{a}{t} S \:|\: \wkpair{t}{a} S \:|\: \wkpair{g}{c} S \:|\: \wkpair{c}{g} A$$
    $$A \rightarrow \wkpair{c}{g} A \:|\: \wkpair{a}{t} S \:|\: \wkpair{g}{c} S \:|\: \wkpair{t}{a} B$$
    $$B \rightarrow \wkpair{c}{g} A \:|\: \wkpair{a}{t} S \:|\: \wkpair{t}{a} S \:|\: \wkpair{g}{c} C$$
    $$C \rightarrow \wkpair{a}{t} C \:|\: \wkpair{t}{a} C \:|\: \wkpair{g}{c} C \:|\: \wkpair{c}{g} C \:|\: \wkpair{\lambda}{\lambda}$$

     The terminals in this grammar refer to the actual nucleobases in the DNA and the complementarity relation mirrors the relations among them: $\rho = \{(a, t), (t, a), (c, g), (g, c)\}$

    The accepted language is: $(\{a,t,c,g\}^*ctg\{a,t,c,g\}^*)^*$

    This grammar is taken from \cite{WK_GRAMMARS_1} and is a first step towards an actual analysis of the DNA. In this case, it simply looks for the substring $ctg$.
  }

  \item{
    $$S \rightarrow \wkpair{a}{\lambda} S \:|\: \wkpair{a}{\lambda} A$$
    $$A \rightarrow \wkpair{b}{a} A \:|\: \wkpair{b}{a} B$$
    $$B \rightarrow \wkpair{\lambda}{b} B \:|\: \wkpair{\lambda}{b}$$


    The accepted language is: $a^nb^n$ where $n \geq 1$ (symbol $x^n$ denotes $n$ occurrences of the symbol $x$)

    The grammar is taken from \cite{REG_GRAMMAR}.
  }

  \item{
    $$S \rightarrow \wkpair{a}{a} S \wkpair{a}{a} \:|\: \wkpair{b}{b} S \wkpair{b}{b} \:|\: \wkpair{c}{c}$$

    The accepted language is: $wcw^R$ where $w \in \{a, b\}^*$($w^R$ is the reversal of string $w$)
  }

  \item{
    $$S \rightarrow \wkpair{a}{a} S \wkpair{a}{a} \:|\: \wkpair{b}{b} S \wkpair{b}{b} \:|\: \wkpair{\lambda}{\lambda}$$

    The accepted language is: $ww^R$ where $w \in \{a, b\}^*$
  }

  \item{
    $$S \rightarrow B L \:|\: R B$$
    $$L \rightarrow B L \:|\: A$$
    $$R \rightarrow R B \:|\: A$$
    $$A \rightarrow B A B \:|\: \wkpair{2}{2}$$
    $$B \rightarrow \wkpair{0}{0} \:|\: \wkpair{1}{1}$$

    The accepted language is: $x2y: x, y \in \{0,1\}^* \wedge \:|x| \neq |y|$

    The grammar is taken from \cite{GRAMMAR_9}.
  }

  \item{
    $$S \rightarrow T \:|\: T \wkpair{p}{p} S$$
    $$T \rightarrow F \:|\: F T$$
    $$F \rightarrow \wkpair{e}{e} \:|\: W \:|\: \wkpair{o}{o} T \wkpair{p}{p} S \wkpair{c}{c} \:|\: X \wkpair{s}{s} \:|\: \wkpair{o}{o} Y \wkpair{c}{c} \wkpair{s}{s}$$
    $$X \rightarrow \wkpair{e}{e} \:|\: \wkpair{l}{l} \:|\: \wkpair{0}{0} \:|\: \wkpair{1}{1}$$
    $$Y \rightarrow T \wkpair{p}{p} S \:|\: F \wkpair{d}{d} T \:|\: X \wkpair{s}{s} \:|\: \wkpair{o}{o} Y \wkpair{c}{c} \wkpair{s}{s} \:|\: Z Z$$
    $$W \rightarrow \wkpair{l}{l} \:|\: Z$$
    $$Z \rightarrow \wkpair{0}{0} \:|\: \wkpair{1}{1} \:|\: Z Z$$

    The accepted language includes regular expressions over symbols 0 and 1 with parenthesis ($o$ for opening and $c$ for closing parenthesis), operators $+$ (p), $*$ (s), $\cdot$ (d) and symbols $\emptyset$ (e), $\varepsilon$ (l)

    The grammar is taken from \cite{GRAMMAR_10}.
  }

  \item{
    $$S \rightarrow A \:|\: B \:|\: A B \:|\: B A$$
    $$A \rightarrow \wkpair{a}{a} \:|\: \wkpair{a}{a} A \wkpair{a}{a} \:|\: \wkpair{a}{a} A \wkpair{b}{b} \:|\: \wkpair{b}{b} A \wkpair{b}{b} \:|\: \wkpair{b}{b} A \wkpair{a}{a}$$
    $$B \rightarrow \wkpair{b}{b} \:|\: \wkpair{a}{a} B \wkpair{a}{a} \:|\: \wkpair{a}{a} B \wkpair{b}{b} \:|\: \wkpair{b}{b} B \wkpair{b}{b} \:|\: \wkpair{b}{b} B \wkpair{a}{a}$$

    The accepted language is: $\{a, b\}^* \setminus ww$ where $w \in \{a, b\}^*$ --- i.e. the complement of the copy language.
  }

  \item{
    $$S \rightarrow \wkpair{r}{\lambda} S \:|\: \wkpair{r}{\lambda} A$$
    $$A \rightarrow \wkpair{d}{r} A \:|\: \wkpair{d}{r} B$$
    $$B \rightarrow \wkpair{u}{d} B \:|\: \wkpair{u}{d} C$$
    $$C \rightarrow \wkpair{r}{u} C \:|\: \wkpair{r}{u} D$$
    $$D \rightarrow \wkpair{\lambda}{r} D \:|\: \wkpair{\lambda}{r}$$

    The accepted language is: $r^nd^nu^nr^n$ where $n \geq 1$

    The grammar is taken from \cite{REG_GRAMMAR}.
  }

  \item{
    $$S \rightarrow \wkpair{a}{\lambda} S \wkpair{b}{\lambda} \:|\: \wkpair{a}{\lambda} A \wkpair{b}{\lambda}$$
    $$A \rightarrow \wkpair{c}{a} A \:|\: \wkpair{\lambda}{c} B \wkpair{\lambda}{b}$$
    $$B \rightarrow \wkpair{\lambda}{c} B \wkpair{\lambda}{b} \:|\: \wkpair{\lambda}{\lambda}$$

    The accepted language is: $a^nc^nb^n$ where $n \geq 1$

    The grammar is taken from \cite{WK_GRAMMARS_1}.

  }

  \item{
    $$S \rightarrow \wkpair{a}{\lambda} S \:|\: \wkpair{a}{\lambda} A$$
    $$A \rightarrow \wkpair{b}{\lambda} A \:|\: \wkpair{b}{\lambda} B$$
    $$B \rightarrow \wkpair{c}{a} B \:|\: \wkpair{c}{a} C$$
    $$C \rightarrow \wkpair{d}{b} C \:|\: \wkpair{d}{b} D$$
    $$D \rightarrow \wkpair{\lambda}{c} D \:|\: \wkpair{\lambda}{d} D \:|\: \wkpair{\lambda}{\lambda}$$

    The accepted language is: $a^nb^mc^nd^m$ where $n, m \geq 1$

    The grammar is taken from \cite{WK_GRAMMARS_1}.
  }

  \item{
    $$S \rightarrow \wkpair{a}{\lambda} S \:|\: \wkpair{b}{\lambda} S \:|\: \wkpair{c}{\lambda} A$$
    $$A \rightarrow \wkpair{a}{a} A \:|\: \wkpair{b}{b} A \:|\: \wkpair{\lambda}{c} B$$
    $$B \rightarrow \wkpair{\lambda}{a} B \:|\: \wkpair{\lambda}{b} B \:|\: \wkpair{\lambda}{\lambda}$$

    The accepted language is: $wcw$ where $w \in \{a, b\}^*$

    The grammar is taken from \cite{WK_GRAMMARS_1}.
  }

  \item{
    $$S \rightarrow \wkpair{a}{\lambda} S \wkpair{a}{a} \:|\: \wkpair{a}{\lambda} A \wkpair{a}{a} $$
    $$A \rightarrow \wkpair{bb}{a} A \:|\: \wkpair{bbb}{a} A \:|\: \wkpair{\lambda}{b} B$$
    $$B \rightarrow \wkpair{\lambda}{b} B \:|\: \wkpair{\lambda}{\lambda}$$

    The accepted language is: $a^nb^ma^n$ where $2n \leq m \leq 3n$

    The grammar is taken from \cite{WK_GRAMMARS_1}.
  }

  \item{
    $$S \rightarrow S S \:|\: \wkpair{a}{a} S \wkpair{b}{b} \:|\: \wkpair{a}{\lambda} S \:|\: \wkpair{a}{\lambda} A$$
    $$A \rightarrow \wkpair{b}{a} A \:|\: \wkpair{b}{a} B \:|\: \wkpair{b}{a}$$
    $$B \rightarrow \wkpair{\lambda}{b} B \:|\: \wkpair{\lambda}{b} \:|\: B B \:|\: \wkpair{a}{a} S \wkpair{b}{b} \:|\: \wkpair{a}{\lambda} S \:|\: \wkpair{a}{\lambda} A$$

    The accepted language is: $w: \#_a(w) = \#_b(w)$ and for any prefix $v$ of $w: \#_a(v) \geq \#_b(v)$  where $\#_a(x)$ denotes the number of occurrences of symbol $a$ in string $x$

    The grammar is taken from \cite{WK_CYK}.
  }

  \item{
    $$S \rightarrow \wkpair{l}{\lambda} S \:|\: \wkpair{l}{\lambda} A$$
    $$A \rightarrow \wkpair{r}{l} A \:|\: \wkpair{r}{l} B$$
    $$B \rightarrow \wkpair{l}{r} B \:|\: \wkpair{\lambda}{r} B \:|\: \wkpair{\lambda}{\lambda} \:|\: A$$

    The accepted language is: $(l^n r^n)^k$ where $n$ does not increase for subsequent $k$. For instance: $lllrrrlrlr$ is within the language, $llrrlllrrr$ is not.

    The grammar is taken from \cite{WK_GRAMMARS_1} (where it is stated that the language of this grammar is $(l^nr^n)^k$ for $n, k \geq 1$ which is not correct). The original symbols for opening and closing parenthesis have been replaced by letters $l$ (left parenthesis) and $r$ (right parenthesis)
  }

  \item{
    The grammar is identical to the grammar 13 with a difference in the complementarity relation. The relations between symbols $a, b$ and symbols $a, c$ are added.

    This means that the relation is: $\rho = \{(a, a), (b, b), (c, c), (a, b), (b, a), (a, c), (c, a)\}$

    The accepted language is: $a^n b^m c^n$ where $n, m \geq 1$
  }

  \item{
	The grammar is identical to the grammar 14 with a difference in the complementarity relation. The relation between symbols $a, b$ is added making the relation $\rho = \{(a, a), (b, b), (c, c), (d, d), (a, b), (b, a)\}$

	The accepted language is: $a^m b^n c^o d^p$ where $m, n, o, p \geq 1 \wedge m+n = o+p$
  }


\end{enumerate}

There are twenty grammars altogether. Grammars 1--5 are regular, 6--11 are context-free and 12--18 are context-sensitive. Grammars 19 and 20 are context-free but they also have a non-bijective complementarity relation.

In reality, there are not 20 but 40 grammars, because all of them can be used in the basic form and after the transformation to the Chomsky normal form. That results in a different grammar (although accepting the same language) which is usually significantly more difficult to calculate with, as there are more rules and many rules generate more non-terminals.

\section{Testing the state space search}

There is a lot of parameters that could be tested and analyzed. How efficient are the various heuristics (both pruning and node preference) for different grammars. What input lengths are answered within a reasonable time? Or more generally, what is the relation between input length and time to get the answer? What are the memory requirements? What is the difference in decision time between input strings which are in the given language and those which are not? Are there significant differences between some inputs of the same lengths?

In order to analyze these questions, I have decided to test the state space search in the following stages.
\begin{enumerate}
  \item{Comparison of the node precedence heuristics and analysis of their efficiency.}
  \item{Comparison of the pruning heuristics and analysis of their efficiency.}
  \item{Analysis of the time and memory complexity based on the length of the input string.}
  \item{Testing if there are any different inputs of the same length which would result in a significant difference in the computation complexity.}
  \item{Testing the WK-CYK algorithm with various grammars and inputs and comparison to the state space search.}
\end{enumerate}

\subsection{Comparison of the node precedence heuristics efficiency} \label{section:node_prec_test}
In section \ref{heur_node_pref}, 12 node precedence heuristics have been described and only one of them can be active at a time.
In order to compare their effectiveness I used the script \textit{ts\_node\_prece\-dence\_tests.py} which runs one test for each of the 40 grammars with an input that will be accepted. It is not useful to test the node precedence heuristics with inputs that are not within the given language as in such cases, the whole space state needs to be searched and node precedence cannot help in any way. The input strings have been chosen to have suitable lengths, so that the computation is finished (at least with some heuristics) in a reasonable time, specifically within the time limit of ten seconds, but also to last some measurable amount of time.

Each of the 40 tests consists of 12 runs, each with a different node precedence heuristic active. There are three metrics to observe:
\begin{itemize}
  \item{How many times the search timed out?}

  \item{What is the total time in which all 40 tests were completed for each of the heuristics. There should be a kind of penalty involved if the test times out because the time needed for the computation is in that case certainly greater then the time it actually ran, before it was stopped by the time limit. Therefore, for the sake of the comparison, the time of the search is in this case doubled.}

  \item{The total time normalized for each test --- the time of the fastest heuristic for each test is normalized to one. This is probably the most telling metric as each test has roughly the same impact on the final number.}
\end{itemize}

Each test prints out the results in a table similar to \ref{tab:node_heuristics_table}. The upper part of the table displays the description of the accepted language, number of rules, non-terminals and terminals in the grammar, string on the input (if it is too long to be printed out, its beginning part and total length is printed out instead), whether the input is expected to be accepted and time limit. In the lower part the table shows for each of the node precedence heuristic how much time it took, how many states at most were in the queue at a time and how many states were analyzed, how many times each pruning heuristic was successful and the result of the search (True, False or Timeout). It would not be practical to present here the complete output, but it can be recreated simply by running the script again and is also attached in the file \textit{output/ts\_node\_precedence\_output.txt}.

\begin{figure}[h!]
  \caption{An output of a node precedence heuristics test}
  \label{tab:node_heuristics_table}
\begin{tabular}{ |l|l|l|l|l|  }
  \hline
  \multicolumn{5}{|l|}{Test 1} \\
  \hline
  Grammar & \multicolumn{4}{|l|}{$a(aa)^*$} \\
  Rules / NTs / Ts & \multicolumn{4}{|l|}{2/1/1} \\
  Input string & \multicolumn{4}{|l|}{aaaaaaaaaaaaaaaaaaaaaaaaa... [len 801]} \\
  Should accept & \multicolumn{4}{|l|}{Yes} \\
  Timeout & \multicolumn{4}{|l|}{7 seconds} \\
  \hline
  Strategy & Time & States Q+C & Prunes (SL, TL, WS, RL, RE)& Accepted \\
  \hline
 NTA & 0.4886 & 994 + 3001  & 0, 5, 0, 0, 250 & TRUE \\
 WNTA & 0.4189 & 498 + 2503 & 0, 3, 0, 0, 250 & TRUE  \\
 TM1 & 0.739 & 1489 + 3994 & 0, 7, 0, 0, 249  & TRUE  \\
 TM2 & 0.7385 & 1489 + 3994 & 0, 7, 0, 0, 249 & TRUE  \\
 TM3 & 0.7366 & 1489 + 3994 & 0, 7, 0, 0, 249  & TRUE  \\
 NTA+TM1 & 0.5903 & 992 + 2999 & 0, 5, 0, 0, 250 & TRUE  \\
 NTA+TM2 & 0.5915 & 992 + 2999 & 0, 5, 0, 0, 250 & TRUE  \\
 NTA+TM3 & 0.5923 & 992 + 2999 & 0, 5, 0, 0, 250 & TRUE  \\
 WNTA+TM1 & 0.495 & 498 + 2503 & 0, 3, 0, 0, 250 & TRUE  \\
 WNTA+TM2 & 0.4949 & 498 + 2503 & 0, 3, 0, 0, 250 & TRUE  \\
 WNTA+TM3 & 0.4977 & 498 + 2503 & 0, 3, 0, 0, 250 & TRUE  \\
 no heuristic & 2.4445 & 4188 + 22147 & 0, 89, 0, 0, 239 & TRUE  \\
  \hline
  \hline
\end{tabular}
\end{figure}

It is interesting to notice how different heuristics are better in different test cases. This is illustrated by selected test cases which are on figure \ref{fig:selected_tests}. There are some cases where the best heuristic is the empty one which assigns zero to each node, like in the case of test 23. This is because this heuristic is the simplest one to compute and if no heuristic is effective in a particular test case, this one wins. But since it does not win by a large margin, these cases do not have a decisive impact on the overall result. There was only one timeout of heuristic TM2 and the empty heuristics.

In some cases, a certain heuristics do not work so well, but their combination does. This can be seen in test 22 --- NTA and WNTA have poor result, comparable to no heuristic. TM1, TM2, TM3 have somewhat better result, but by far the best result is achieved by combination of NTA with any version of TN.

\begin{figure}[h!]
  \includegraphics[scale=0.45]{four_heur_tests.png}
  \caption{Selected comparisons of node precedence heuristics}
  \label{fig:selected_tests}
\end{figure}

The figure \ref{fig:node_heuristics_comp} shows the total result for all of the 40 tests. The right bar of each heuristic shows the total time for the 40 tests and the left bar shows the normalized time. It turns out that the best results are achieved by the combination NTA+TM1. The best time overall was achieved by TM3 by very narrow margin. The normalized times show that all NTA+TM1, NTA+TM2 and NTA+TM3 heuristics are very close but the narrow winner is NTA+TM1. Even if in some cases there are some faster heuristics, it's usually not by much. Interestingly, even though TM2 turns out to have the worst results of them all, often worse than no heuristic, with the combination of NTA the results are among the best. Therefore, for all of the following tests, the winning node precedence heuristic NTA+TM1 will be used.

\begin{figure}[h!]
  \includegraphics[scale=0.45]{node_heuristics_comp.png}
  \caption{Comparison of the node precedence heuristic functions}
  \label{fig:node_heuristics_comp}
\end{figure}

\subsection{Comparison of the pruning heuristics efficiency} \label{section:pruning_test}

Pruning has the advantage of being useful whether the input string is going to be accepted or rejected by the tree search. Also, all of the pruning can be active at the same time. Each node can be tested by all available checks to see if it can be pruned or not.

The testing is performed over 80 tests --- each of the 40 grammars is used for a positive test (where the input will be accepted) and a negative test (the input will be rejected).
Each test contains seven runs of the tree search algorithm --- one where all pruning heuristics are active, one where all are inactive, and one for each heuristic where all are active except the given one.

Similarly to the node precedence heuristics comparison, the metrics that are important are the total time needed to compute the 80 tests and a number of timeouts for each of the seven cases.

The tests are run by the script \textit{ts\_pruning\_tests.py} and each test prints out a table similar to the table \ref{tab:prune_heuristics_table}. Again, it would not be practical to include the entire output here. The complete set of results can be recreated by running the script again and it is included in the file \textit{output/ts\_pruning.txt}.

\begin{figure}[h!]
  \caption{An output of the pruning heuristics test}
  \label{tab:prune_heuristics_table}
\begin{tabular}{ |l|l|l|l|l|  }
  \hline
  \multicolumn{5}{|l|}{Test 1} \\
  \hline
  Grammar & \multicolumn{4}{|l|}{$a(aa)^*$} \\
  Rules / NTs / Ts & \multicolumn{4}{|l|}{2/1/1} \\
  Input string & \multicolumn{4}{|l|}{aaaaaaaaaaaaaaaaaaaaaaaaa... [len 801]} \\
  Should accept & \multicolumn{4}{|l|}{Yes} \\
  Timeout & \multicolumn{4}{|l|}{7 seconds} \\
  \hline
  Strategy & Time & States Q+C & Prunes (SL, TL, WS, RL, RE)& Accepted \\
  \hline
  ALL ON & 0.7841 & 799 + 1200 & 0, 3, 0, 0, 400 & TRUE \\
  strands len OFF & 0.7781 & 799 + 1200 & 0, 3, 0, 0, 400 & TRUE \\
  total len OFF & 0.7799 & 801 + 1200 & 0, 0, 0, 0, 400 & TRUE \\
  terms match OFF & 0.7705 & 799 + 1200 & 0, 3, 0, 0, 400 & TRUE \\
  relation OFF & 0.655 & 799 + 1200 & 0, 3, 0, 0, 400 & TRUE \\
  regex OFF & 0.3594 & 800 + 1599 & 0, 3, 0, 0, 0 & TRUE \\
  ALL OFF & 0.2179 & 801 + 1600 & 0, 0, 0, 0, 0 & TRUE \\
  \hline
  \hline
\end{tabular}
\end{figure}

The summary of results is displayed on figure \ref{fig:prune_timeouts_comp} --- the number of timeouts and \ref{fig:prune_times_comp} --- the amount of time for each of the seven cases across the 80 tests. The smaller the individual bars are, the better the result. But in case of the bars representing a specific pruning heuristic being turned off, the bigger the bar, the more important the given heuristic is, because the result is that much worse without it.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.44]{prune_timeouts_comp.png}
  \caption{Number of timeouts of all pruning tests}
  \label{fig:prune_timeouts_comp}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.47]{prune_times_comp.png}
  \caption{Total time of all pruning tests}
  \label{fig:prune_times_comp}
\end{figure}


From these results it is clear that the tree pruning is the key feature of the tree search algorithm. After turning off the pruning, the results are quite poor --- 67 out of 80 tests timed out. The total time is then not relevant at all. The middle bars, which represent the individual pruning heuristics being turned off, needs to be compared to the first one, where all heuristics are active, to see how important the given heuristic actually is. Thus, the figure suggests, that the RL (complementarity relation) check is the most important one because turning it off had the biggest impact on the result. This can be a bit misleading, as some heuristics can be sometimes backed up by another one. This is the reason why the WS (match of leftmost terminals to the input string) seem to have a rather small impact. If this heuristic is turned off, the dead branch can be identified by the RE (regular expression) check and so the impact is not so big. Similarly, turning off the SL (strands length) heuristic has smaller impact, because it is backed up by TL (total length) heuristic.

Nevertheless, all heuristic are useful according to this result, because no other bar is as small as the first one. This means that the best result is achieved when all heuristics are active the same time. This is especially important in the case of the RE heuristic. This one is quite demanding with regards to computation power --- regular expression match is performed each time this check is executed. It is the reason why it is the last check that is used, if there is another heuristic able to prune the node, a lot of computational power is saved. However, the figure shows clearly, that the RE heuristic contributes significantly to the overall performance (still, some tests cannot take advantage of this heuristic and turning it off would improve the results).


\subsection{Analysis of the memory requirements}
The tree search algorithm needs to keep in memory the nodes, which have been generated but not yet analyzed. These are in the priority queue waiting to be used. Also, it keeps track of all nodes which have already been generated. These are the nodes that had been in the queue before and nodes that are there at the moment, as in both cases, there is no reason to put them into the queue again. But it is not necessary to keep all these nodes in the memory in order to identify them, their hash is enough.

If the algorithm should not only find a solution if there is one, but also find the path from the initial node to the solution, it is not enough just to remember the hashes of nodes that have been analyzed. It is necessary to remember all of the nodes. This is actually the case in the current implementation because when testing the tree search algorithm, it turns out that the memory consumption is not a real issue and it is sometimes convenient to learn the path to the solution. A very simple modification would change this --- simply removing the item \textit{parent} from the tree node (\textit{cTreeNode} class) and the method printPath from the grammar (\textit{cWK\_CFG} class).

The number of nodes in the queue can go up or down as the search progresses but it is more likely to go up, unless the search is coming to its end. In any case, the important figure is the maximum number of nodes that were in the queue at one point.

Next parameter that needs to be considered is the size of one node in the memory. As mentioned in the section \ref{chapter:implementation}, one node contains six integers (storing the number of terminals in the two strands, number of non-terminals, hash number, the node parent and the node precedence evaluation) and the word itself. The word can contain up to twice the number of symbols then is the length of the input string. If it contains more, it is going to be pruned.

This is not necessarily true, if the grammar contains some $\lambda$-rules and non-terminals that can be erased. Then the theoretical length of the word in memory has no limit, but this is not a typical scenario and it can be avoided altogether by applying the $\lambda$-rules removal algorithm.

The equation for getting the memory requirement, based on the number of nodes working with, is than the following:

$$S_{all} \times size(int) + S_{all} \times (6 \times size(int) + 2 \times size(symbol) \times |input|)$$

After the removal of the parent information from the nodes, the equation would be as follows:

$$S_{all} \times size(int) + S_{open} \times (6 \times size(int) + 2 \times size(symbol) \times |input|)$$

$S_{all}$ is the number of all nodes generated, $S_{open}$ is the maximum number of nodes in the queue, $size(int)$ is the size of an integer (for this calculation, I will assume it is 64 bytes), $size(symbol)$ is the size of one symbol of the grammar and $|input|$ is the length of the input string.

I've chosen grammar 3 in CNF to test the results in practice, as this grammar is among the hardest ones with respect to computing complexity. I used inputs in the form of $a^nb$ with increasing $n$, which will always be rejected, because the grammar accepts strings that end with $abc$.

The figure \ref{fig:mem_consumption} on the left shows the amount of memory requirements in relation to the input length. The middle graph shows the memory consumption in relation to the time needed for computation. To the right is the experimental result where I measured real memory consumption of the program in time.

\begin{figure}[h!]
  \includegraphics[scale=0.47]{mem_consumption.png}
  \caption{Memory consumption of tree search}
  \label{fig:mem_consumption}
\end{figure}

The measurement of the real memory consumption shows that there is a possibility to be limited by the memory available. As the figure \ref{fig:mem_consumption} on the right shows, the tree search used 1 GB of memory after approximately 80 seconds of running and the consumption increases linearly (the number depends on the hardware and environment so it is very crude). On the other hand, it does not seem practical to have the time limit of the computation too high. If the result is not found in the order of tens of seconds or, at most, minutes, it is probably not going to be found (within a reasonable time frame) at all.

\subsection{The time complexity of the state space search}
The previous sections showed that the best overall performance is achieved when using all of the tree pruning heuristics and using the NTA+TM1 as the node precedence heuristic. This may not be the case for every grammar or every input, but it is the case overall. Therefore, this will be the setting used in the sections that follow --- testing the tree search performance, analyzing the practical complexity and comparing to WK-CYK algorithm.

I used the script \textit{ts\_speed\_tests.py} to test the time complexity with respect to the input length. It runs 80 test, two for each of the 40 grammars --- one with inputs that are going to be accepted by the tree search and one with inputs which will be refused. Each test runs the tree search several times and increases the input length. It stops when the computation takes longer than a limit of ten seconds or after 30 runs. For each of the 80 tests, it prints out a table similar to \ref{tab:input_compl_test}. As it would not be practical to present all of the results here, they can be recreated by simply running the script again and they are included in the file \textit{output/ts\_speed.txt}.

\begin{figure}[h!]
  \caption{An output of the time complexity test}
  \label{tab:input_compl_test}
\begin{tabular}{ |l|l|l|l|l|  }
  \hline
  \multicolumn{5}{|l|}{Test 1} \\
  \hline
  Grammar & \multicolumn{4}{|l|}{$a(aa)^*$} \\
  Rules / NTs / Ts & \multicolumn{4}{|l|}{2/1/1} \\
  Should accept & \multicolumn{4}{|l|}{Yes} \\
  Timeout & \multicolumn{4}{|l|}{10 seconds} \\
  \hline
  Input length & Time & States Q+C & Prunes (SL, TL, WS, RL, RE)& Accepted \\
  \hline
 2001 & 4.6528 & 1999 + 3000 & 0, 3, 0, 0, 1000 & TRUE \\
 2101 & 5.1277 & 2099 + 3150 & 0, 3, 0, 0, 1050 & TRUE \\
 2201 & 5.6251 &2199 + 3300 & 0, 3, 0, 0, 1100 & TRUE \\
 2301 & 6.134 & 2299 + 3450 & 0, 3, 0, 0, 1150 & TRUE \\
 2401 & 6.665 & 2399 + 3600 & 0, 3, 0, 0, 1200 & TRUE \\
 2501 & 7.3398 & 2499 + 3750 & 0, 3, 0, 0, 1250 & TRUE \\
 2601 & 8.057 & 2599 + 3900 & 0, 3, 0, 0, 1300 & TRUE \\
 2701 & 8.4195 & 2699 + 4050 & 0, 3, 0, 0, 1350 & TRUE \\
 2801 & 9.2167 & 2799 + 4200 & 0, 3, 0, 0, 1400 & TRUE \\
  \hline
  \hline
\end{tabular}
\end{figure}

The performance for different grammars is quite different. In case of eleven of the twenty grammars (specifically, grammars 2, 5--10, 12-14, 19), the resulting graph is a very regular parabola. Often a bit steeper when transformed to the CNF and often steeper when the inputs are going to be rejected. An example is on figure \ref{fig:speed_test_grammar_12}. The performance with these grammars allows at least hundreds, in most cases thousands, of symbols on the input.

\begin{figure}[h!]
  \includegraphics[scale=0.33]{speed_test_grammar_12.png}
  \caption{Grammar 12: $r^n d^n u^n r^n$}
  \label{fig:speed_test_grammar_12}
\end{figure}


Occasionally, thanks to the pruning heuristics, the algorithm is able to tell practically immediately that there is no solution. This is the case of grammar 3 \ref{compl_result_grammar_3} and grammar 4 \ref{compl_result_grammar_4} in basic forms, making the complexity of this particular search constant. The grammar 3 has as the first rule, which it has to use to proceed further, $S \rightarrow A \wkpair{abc}{abc}$. If the input string does not end in $abc$ the regular expression check immediately detects that the input cannot be matched, prunes the only branch and the search is finished.

Similarly, in the case of grammar 4, any inputs that are longer than seven symbols need to end with symbol $a$ and can be reached only by using $S \rightarrow Q \wkpair{a}{a}$ as the first rule. The regular expression check immediately prunes this branch. The rest of the tree is searched very quickly because the only other possible starting rule is $S \rightarrow A B C D E F G$, there are not many states that can be reached from it, so this part of the tree is always small.

\begin{figure}[h!]
  \includegraphics[scale=0.33]{speed_test_grammar_3.png}
  \caption{Grammar 3: $(a+b+c)^*abc$ with left recursive rules}
  \label{fig:speed_test_grammar_3}
\end{figure}

\begin{figure}[h!]
  \includegraphics[scale=0.33]{speed_test_grammar_4.png}
  \caption{Grammar 4: $(a+b+c)^*abc$ with right recursive rules}
  \label{fig:speed_test_grammar_4}
\end{figure}

After conversion of the grammar to CNF, the complexity usually goes up. The transformation adds a lot more rules and thus the state space expands more rapidly, there are also longer paths from the starting non-terminal to the final string (containing only terminals) making the tree deeper. Also, the node precedence heuristics and the pruning have harder time because many rules contain non-terminals only and most of the heuristics work with terminals. The most extreme case is the grammar 3 \ref{fig:speed_test_grammar_4} where the tree search is very effective for grammar in basic form (as discussed, in case of rejecting inputs the result is immediate) but has very bad effectiveness for this grammar in the CNF. The maximum length of input it can answer within 10 second is about 13--14 symbols.

The worst results for grammar in the basic form are in the case of grammar 17 \ref{fig:speed_test_grammar_17}. Here the tree search can handle only inputs with length of about 20 symbols within 10 seconds.

\begin{figure}[h!]
  \includegraphics[scale=0.33]{speed_test_grammar_17.png}
  \caption{Grammar 17: $w: \#_a(w) = \#_b(w)$ and for any prefix $v$ of $w$: $v: \#_a(v) \geq \#_b(v)$}
  \label{fig:speed_test_grammar_17}
\end{figure}

Some grammars manifest interesting behavior --- for some longer inputs the performance is actually better. This is the case of grammars 11 (\ref{fig:speed_test_grammar_11} on the left), 16 (\ref{fig:speed_test_grammar_16} on the middle right) and 18 (\ref{fig:speed_test_grammar_18} on the middle left and middle right). Some of these results may appear to be partly random, simply the input generator might sometimes generate an input which is more complex and sometimes less complex to compute. This is the case of the grammar 18, in fact the shape of the randomly generated input has a big impact on the performance. However, repeated tests of the other two grammars confirmed that this happens always and the figure of grammar 16 has a clear pattern, it is certainly not random.

\begin{figure}[h!]
  \includegraphics[scale=0.33]{speed_test_grammar_11.png}
  \caption{Grammar 11: $(ww)^C$}
  \label{fig:speed_test_grammar_11}
\end{figure}

\begin{figure}[h!]
  \includegraphics[scale=0.33]{speed_test_grammar_16.png}
  \caption{Grammar 16: $a^n b^m a^n$ where $2n \leq m \leq 3n$}
  \label{fig:speed_test_grammar_16}
\end{figure}

\begin{figure}[h!]
  \includegraphics[scale=0.33]{speed_test_grammar_18.png}
  \caption{Grammar 18: $(l^n r^n)^k$ where $n$ does not increase for subsequent $k$s}
  \label{fig:speed_test_grammar_18}
\end{figure}

The results of the three grammars which have not been yet mentioned, grammars 1, 15 and 20, are mostly similar to the standard parabola of the majority of tests with some irregularities.

\subsection{Comparing different inputs of the same length}
There are some grammars which do not provide any possibility of an interesting or edge case input. Specifically, it is the case of grammars 1--4. Grammar 1 works only with the terminal symbol $a$, grammars 2, 3 and 4 are only interested whether a string ends with specific symbols. But the rest of the grammars, grammars 5--20, all provide some space for trying to come up with an edge case input --- an input, which has the key part on the very end or on the very beginning etc. I have manually created some of these edge cases and tested what differences in performance there are. For this test I used a script \textit{ts\_var\_inputs\_tests.py}. Its complete output can be again recreated by running the script and is attached in the file \textit{output/ts\_var\_inputs.txt}.

I have selected three of these tests to show here. The tables \ref{tab:input_compl_test_2}, \ref{tab:input_compl_test_4}, \ref{tab:input_compl_test_8}  show the input of the tests from the script \textit{ts\_var\_inputs\_tests.py}. The first column called Input displays the string that has been used as an input in a compact format $(ns)^*$ where $n$ is a number of occurrences of the symbol or string $s$. For instance, $3a\:2ab$ would translate to string $aaaabab$.

As mentioned, one advantage of state space search is, that sometimes it can recognize right away that a certain input is not in a given language. It is usually when the string starts with a symbol that cannot be at the beginning. For instance the second input on table \ref{tab:input_compl_test_2} where the language is $a^nb^n$ and the input starts with $b$ can never be accepted. A similar case is the second input on table \ref{tab:input_compl_test_8} where the language is $r^nd^nu^nr^n$ and it cannot start with anything other than $r$.

On the other hand, sometimes the search has harder time finding a key part of the string which is at the end. This can take longer as is the case in the last input of \ref{tab:input_compl_test_2} (search has to get to the end of the string to see that there are not enough of the $b$ symbols). Some inputs on the table \ref{tab:input_compl_test_8} also suffer for the same reason. This is usually not a problem that could break the practical use completely. The result is typically still reached within a reasonable time. A extreme example is, however, on the table \ref{tab:input_compl_test_4} where the last input is not decided in time, even though other inputs were decided very quickly.

\begin{figure}[h]
\centering
  \caption{Test of various inputs for grammar 6}
  \label{tab:input_compl_test_2}
\begin{tabular}{ |l|l|l|l|l|  }
  \hline
  \multicolumn{5}{|l|}{Test 2} \\
  \hline
  Grammar & \multicolumn{4}{|l|}{$a^n b^n$} \\
  Rules / NTs / Ts & \multicolumn{4}{|l|}{6/3/2} \\
  Timeout & \multicolumn{4}{|l|}{10 seconds} \\
  \hline
  Input & Time & States Q+C & Prunes(SL, TL, WS, RL, RE) & Accepted \\
  \hline
  500a & 0.1881 & 2 + 999 & 4, 0, 998, 0, 0 & FALSE \\
  500b & 0.0 & 1 + 0 & 0, 0, 2, 0, 0 & FALSE \\
  a 500b & 0.3448 & 3 + 502 & 2, 0, 2, 2, 500 & FALSE \\
  500a b & 1.0903 & 3 + 1996 & 2, 0, 1000, 998, 0  & FALSE \\
  \hline
\end{tabular}
\end{figure}


\begin{figure}[h]
\centering
  \caption{Test of various inputs for grammar 8}
  \label{tab:input_compl_test_4}
\begin{tabular}{ |l|l|l|l|l|  }
  \hline
  \multicolumn{5}{|l|}{Test 4} \\
  \hline
  Grammar & \multicolumn{4}{|l|}{$w w^r$} \\
  Rules / NTs / Ts & \multicolumn{4}{|l|}{3/1/2} \\
  Timeout & \multicolumn{4}{|l|}{10 seconds} \\
  \hline
  Input & Time & States Q+C & Prunes(SL, TL, WS, RL, RE) & Accepted \\
  \hline
  2000ab 2000ba a & 0.0003 & 1 + 1 & 0, 0, 3, 0, 2, 0 & FALSE \\
  a 2000ab 2000ba &  0.0003 & 1 + 1 & 0, 0, 2, 0, 3, 0 & FALSE \\
  2000ab a 2000ba & 10.0079 & 1 + 1461 & 0, 0, 2921, 0, 1, 0 & TIMEOUT \\
  \hline
\end{tabular}
\end{figure}

\begin{figure}[h]
\centering
  \caption{Test of various inputs for grammar 12}
  \label{tab:input_compl_test_8}
\begin{tabular}{ |l|l|l|l|l|  }
  \hline
  \multicolumn{5}{|l|}{Test 8} \\
  \hline
  Grammar & \multicolumn{4}{|l|}{$r^n d^n u^n r^n$} \\
  Rules / NTs / Ts & \multicolumn{4}{|l|}{10/5/3} \\
  Timeout & \multicolumn{4}{|l|}{10 seconds} \\
  \hline
  Input & Time & States Q+C & Prunes(SL, TL, WS, RL, RE) & Accepted \\
  \hline
 500r 500d 500u 500r d & 8.9568 & 1500 + 3001 & 0, 0, 3002, 998, 501, 1 & FALSE \\
 d 500r 500d 500u 500r & 0.0001 & 1 + 0 & 0, 0, 2, 0, 0, 0 & FALSE \\
 500r 500d 500u 501r & 8.9805 & 1500 + 3002 & 2, 0, 3000, 1000, 501, 0 & FALSE \\
 500r 500d 501u 500r & 3.0577 & 1000 + 2001 & 0, 0, 3000, 2, 0, 0 & FALSE \\
 500r 501d 500u 500r & 1.0785 & 1000 + 1001 & 0, 0, 2000, 2, 0, 0 & FALSE \\
 501r 500d 500u 500r & 1.0763 & 1001 + 1002 & 0, 0, 2002, 2, 0, 0 & FALSE \\
 r 500d 500u 500r & 0.0003 & 2 + 3 & 0, 0, 4, 2, 0, 0 & FALSE \\
 500r d 500u 500r & 0.191 & 501 + 502 & 0, 0, 1002, 2, 0, 0 & FALSE \\
 500r 500d u 500r & 0.9625 & 1000 + 1003 & 0, 0, 2002, 2, 0, 0 & FALSE \\
 500r 500d 500u r & 3.0452 & 1001 + 2002 & 2, 0, 3000, 2, 0, 0 & FALSE \\

  \hline
\end{tabular}
\end{figure}


\section{Testing the efficiency of WK-CYK}

I have tested the WK-CYK algorithm in a similar manner as the tree search. This time not all grammars can be used due to limitations of WK-CYK. The grammars must be in CNF form and grammars 5, 19 and 20 cannot be used at all, since WK-CYK requires the complementarity relation to be identity, which is not the case of these three grammars. Therefore there are 17 grammars that can be tested. The script \textit{wk\_cyk\_tests.py} runs two tests for each of these grammars. One with inputs that should be accepted and one with inputs that should be rejected. Again, each test increases the input string length until the computation lasts more that a limit of 10 seconds. The output of each test is, again, a table similar to \ref{tab:wk_cyk_res}. The entire output can be recreated by running the script again and is attached in file \textit{output/wk\_test.txt}.


\begin{figure}[h]
\centering
  \caption{An output of the WK-CYK time complexity test}
  \label{tab:wk_cyk_res}
\begin{tabular}{ |l|l|l|  }
  \hline
  \multicolumn{3}{|l|}{Test 1} \\
  \hline
  Grammar & \multicolumn{2}{|l|}{$a(aa)^*$} \\
  Rules / NTs / Ts & \multicolumn{2}{|l|}{2/1/1} \\
  Should accept & \multicolumn{2}{|l|}{Yes} \\
  Timeout & \multicolumn{2}{|l|}{7 seconds} \\
  \hline
  Input length & Time & Accepted \\
  \hline
  3 & 0.0 & TRUE \\
  5 & 0.0 & TRUE \\
  7 & 0.0 & TRUE \\
  9 & 0.01 & TRUE \\
  11 & 0.03 &  TRUE \\
  13 & 0.07 &  TRUE \\
  15 & 0.15 &  TRUE \\
  17 & 0.29 &  TRUE \\
  19 & 0.53 &  TRUE \\
  21 & 0.88 &  TRUE \\
  23 & 1.45 &  TRUE \\
  25 & 2.31 &  TRUE \\
  27 & 3.49 &  TRUE \\
  29 & 5.23 & TRUE \\
  31 & 7.55 & TRUE \\
  33 & 10.67 & TRUE \\

  \hline
\end{tabular}
\end{figure}

It turns out that the WK-CYK gives very similar performance in all tests --- for all the grammars and regardless whether the input is accepted or not. The figure \ref{fig:wk_cyk_test1} to the right shows a result for the first test which is very similar to all the others. The limit of ten seconds is reached by WK-CYK when the input has about 33 symbols.

These results confirm the claim made by authors of WK-CYK that the complexity with regards to the input length is $O(n^6)$. The figure \ref{fig:wk_cyk_test1} shows the same test with the input lengths raised to the power of six which can be considered to be a number of numeric operations needed for a computation. The curve is then very close to linear.

When the results of the WK-CYK and state space search are compared, the advantage of state space search is the actual speed in most cases. The results in the previous sections showed that of all the grammars only one (grammar 17) was slower in the basic form when analyzed by tree search then when analyzed by WK-CYK. After transformation to CNF two more grammars (grammar 3 and 11) were comparable or slower when analyzed by the tree search. Grammar 1 was slower for the negative inputs.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.5]{wk_cyk_test_1.png}
  \caption{WK-CYK test result}
  \label{fig:wk_cyk_test1}
\end{figure}

\section{Comparison of state space search and WK-CYK}

It has been concluded in the previous section that the WK-CYK algorithm is able to compute within the time limit of ten seconds results for inputs of length of approximately 33 symbols. I will assume that will always be the case even for grammars that would have to be modified in order to be suitable for WK-CYK (grammars 5, 19 and 20). If these results are compared with the results of state space search over grammars in basic forms, only one of them is more efficient with WK-CYK. Other 19 are more efficient with the state space search allowing hundreds of input symbols at minimum. That means that state space search was more efficient in 38 out of 40 test cases (each grammar is tested with accepted and rejected inputs) i.e. in 97.5 \% of cases. In this comparison the state space search benefits from being able to work with any WK grammar --- there is no need to transform it to the WK-CNF.

If the state space search is compared to WK-CYK over all 40 grammars, WK-CYK has better efficiency in case of grammars 3 and 11 (in the CNF) and with the rejected inputs for grammar 1. That is 9 test cases out of 80 (four test cases of grammar 17, two of grammars 3 and eleven, one of grammar 1) i.e. in 88,75 \% of cases. However, this comparison assumes that there is a need to use the grammars in the CNF, as well.

These results show some advantages and disadvantages if the two algorithm. An advantage of state space search is the flexibility regarding the grammars. It does not require to work with grammars in the CNF. Also, it does not require the complementarity relation to be identity. Even though it is always possible to transform any WK grammar to the WK-CNF and it is always possible to further transform the grammar in order to use only the identity as the relation, this can significantly add to the grammar's complexity.

A useful feature of the state space search is the fact that it can be configured for the need of a specific grammar. If the membership test will be performed repeatedly on a grammar, it is possible to find out what node precedence heuristic works best and what pruning heuristics are useful in that particular scenario by running tests analogous to those presented in section \ref{section:node_prec_test}. Thus the performance may be further enhanced.

The advantage of WK-CYK, on the other hand, is its universality. It has roughly the same speed every time, it does not significantly depend on the grammar (increasing number of rules adds a little bit) and it does not matter, if the input is going to be accepted or not. For very complicated grammars, especially with lots of rules or long derivations from the starting symbol to the final string, WK-CYK still might be more practical.

\chapter{Conclusion}
In this work I have presented various existing models for representing Watson-Crick languages, most importantly Watson Crick automata and Watson-Crick grammars and I have analyzed the algorithm WK-CYK which is used for testing a membership of strings in languages defined by Watson-Crick context-free grammars. Than I have come up with another algorithm for testing membership in WK languages which I call state space search or tree search which is the most important contribution of this thesis.

The state space search is based on a standard Breadth-first search algorithm where the starting non-terminal of the grammar is the root node and every applicable rule creates successors in the tree. The state space search then introduces various optimizations, from which the most important are pruning and node precedence heuristics. Pruning uses five different methods of identifying that a given node cannot produce the desired solution and removes the entire branch. Node precedence heuristics attempt to choose more promising nodes to be analyzed first. I have implemented and tested 12 such heuristics and chosen the one which had the best overall results (called NTA+TM1) as the default one.

I have collected or created twenty Watson-Crick context-free grammars to test the WK-CYK and state space search algorithms.
I have implemented both WK-CYK and state space search in the Python language and written scripts to test the algorithms with all these grammars and various inputs. The test results showed that for the majority of the grammars, state space search was very efficient and it can quickly decide membership problem of inputs that are hundreds or even thousands of symbols long.

Among the advantages of the state space search is the fact that it can work over any forms of WK context-free grammars (not only grammars in the CNF or grammars with the identity complementarity relation). Also, it can be optimized for a particular grammar. Tests comparing performance of node precedence heuristics and tests of pruning heuristics can be run in order to find which combination is best for a particular grammar thus further improving the performance.

Testing the WK-CYK algorithm showed that its theoretical complexity $O(n^6)$ with respect to the input length corresponds to the real performance. In practice, it is able to decide membership problem of inputs up to the length of approximately 30--50 symbols. That is much less than state space search but, on the other hand, its performance is almost identical for any grammar and for any input. It is more efficient in case of some specific or complex grammars where the performance of state space search struggles.

The state space search is a suitable algorithm for parallization. Several processes can take nodes from the queue of open nodes and analyze different branches of the tree independently. This would be a natural next step in the further development of the state space search.
It is possible to come up with other heuristics for both the pruning and node preference. As for pruning, one possibility would be to expand the regular expression matching (RE) heuristic also to the lower strand.
Another pruning idea is to calculate how many terminals can be generated at minimum to the lower strand and to the upper strand individually (currently, it is calculated how many terminals a non-terminal produces to both strands) thus making the constraint of the words stronger.
As for the node precedence heuristics, it may be worthwhile to use some of the grammars with which state space search is not efficient (in particular grammars 3 in the CNF and grammar 17) and design or improve node precedence heuristics with respect to these particular cases. Then it would be necessary to test all these new heuristics and see if they contribute to the overall performance or not.
Another promising improvement could be analyzing the input from both sides at the same time. This could help with the cases, when the key part of the input is at or near its end and the state space search may struggle to get there in a reasonable time frame.
